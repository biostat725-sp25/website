---
title: "Regularization"
author: "Prof. Sam Berchuck"
date: "2025-02-13"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    html-math-method: mathjax
filters:
  - parse-latex
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(knitr)
library(mvtnorm)
library(coda)
library(lars)
library(LaplacesDemon)
library(nimble)
```

## Review of last lecture

-   On Tuesday, we learned about robust regression.

    -   Heteroskedasticity

    -   Heavy-tailed distributions

    -   Quantile regression

-   These were all models for the observed data $Y_i$.

-   Today, we will focus on prior specifications for $\boldsymbol{\beta}$.

## Sparsity in regression problems

-   Supervised learning can be cast as the problem of estimating a set of coefficients $\boldsymbol{\beta} = \{\beta_j\}_{j=1}^{p}$ that determines some functional relationship between a set of $\{x_j\}_{j = 1}^p$ and a target variable $y$.

-   This is a central focus of statistics and machine learning.

-   Challenges arise in "large-$p$" problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.

-   Finding a sparse solution, where some $\beta_j$ are zero, is desirable.

## Bayesian sparse estimation

-   From a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.

-   Discrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.

    -   Easy to force $\beta_j$ to exactly zero, but require discrete parameter specification.

-   Shrinkage priors force $\beta_j$ to zero using regularization, but struggle to get exact zeros.

    -   In recent years, shrinkage priors have become dominant in Bayesian sparsity priors.

## Horseshoe prior

-   Let's assume $\mathbf{Y} \stackrel{}{\sim}N\left(\mathbf{X}\boldsymbol{\beta},\sigma^2\mathbf{I}_n\right)$, where $\boldsymbol{\beta}$ is assumed to be sparse.

-   The horseshoe prior is specified as,

\begin{align*}
\beta_j | \lambda_j, \tau &\sim N(0, \lambda_j^2 \tau^2)\\
\lambda_j &\sim \mathcal C^+(0, 1),
\end{align*} where $\mathcal C^+(0, 1)$ is a half-Cauchy distribution for the standard deviation $\lambda_j$.

## Half-Cauchy distribution

A random variable $X \sim \mathcal C^+(\mu,\sigma)$ follows a half-Cauchy distribution with location $\mu$ and scale $\sigma > 0$ and has the following density,

$$f(X | \mu, \sigma) = \frac{2}{\pi \sigma}\frac{1}{1 + (X - \mu)^2 / \sigma^2},\quad X \geq \mu$$

-   The Half-Cauchy distribution with $\mu = 0$ is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution.

## Half-Cauchy distribution in Stan

In Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.

```{stan output.var = "half-cauchy", eval = FALSE}
parameters {
  real<lower = 0> lambda;
}
model {
  target += cauchy_lpdf(lambda | 0, 1);
}
```

## Horseshoe prior

-   The horseshoe prior is specified as,

\begin{align*}
\beta_j | \lambda_j, \tau &\sim N(0, \lambda_j^2 \tau^2)\\
\lambda_j &\sim \mathcal C^+(0, 1),
\end{align*} where $\mathcal C^+(0, 1)$ is a half-Cauchy distribution for the standard deviation $\lambda_j$.

-   $\lambda_j$'s are the *local* shrinkage parameters.

-   $\tau$ is the *global* shrinkage parameter.

## Horseshoe prior

The horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.

1.  It has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.

2.  It has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of $\boldsymbol{\beta}$.

As we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors.

## Relation to other shrinkage priors

\begin{align*}
\beta_j | \lambda_j, \tau &\sim N(0, \lambda_j^2 \tau^2)\\
\lambda_j^2 &\sim f(\lambda_j)
\end{align*}

1.  $\lambda_j = \lambda$, implies a Gaussian prior for $\beta_j$ (Ridge regression).

2.  $f(\lambda_j) = \text{Exponential}(2)$, implies independent Laplacian priors for $\beta_j$ (LASSO).

3.  $f(\lambda_j) = \text{Inverse-Gamma}(a,b)$, implies independent Student-t priors for $\beta_j$ (relevance vector machine).

## Horsehoe density

![](images/10/carvalho1.png){fig-alt="workflow" fig-align="center" height="4.5in"}

[From Carvalho 2009](https://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf)

## Shrinkage of each prior

-   Define the posterior mean of $\beta_j$ as $\bar{\beta}_j$ and the maximum likelihood estimator for $\beta_j$ as $\hat{\beta}_j$.

-   The following relationship holds: $\bar{\beta}_j = (1 - \kappa_j) \hat{\beta}_j$,

$$\kappa_j = \frac{1}{1 + n\sigma^{-2}\tau^{2}s_j^2\lambda_j^2}.$$

-   $\kappa_j$ is called the shrinkage factor for $\beta_j$.

-   $s_j^2 = \mathbb{V}(x_j)$ is the variance for each predictor.

## Standardization of predictors

-   In regularization problems, predictors are standardized (to mean zero and standard deviation one).

-   This means that so that $s_j = 1$.

-   Shrinkage parameter:

$$\kappa_j = \frac{1}{1 + n\sigma^{-2}\tau^{2}\lambda_j^2}.$$

-   $\kappa_j = 1$, implies complete shrinkage.

-   $\kappa_j = 0$, implies no shrinkage.

## Shrinkage parameter

![](images/10/carvalho2.png){fig-alt="workflow" fig-align="center" height="5in"}

[From Carvalho 2009](https://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf)

## Horseshoe shrinkage parameter

-   Choosing $\lambda_j âˆ¼ \mathcal C^+(0, 1)$ implies $\kappa_j âˆ¼ \text{Beta}(0.5, 0.5)$, a density that is symmetric and unbounded at both 0 and 1.

-   This horseshoe-shaped shrinkage profile expects to see two things a priori:

    1.  Strong signals ($\kappa \approx 0$, no shrinkage), and

    2.  Zeros ($\kappa \approx 1$, total shrinkage).

## Spike-and-slab prior

-   The prior is often written as a two-component mixture of Gaussians,

\begin{align*}
\beta_j | \lambda_j, c^2, \epsilon &\sim \lambda_j N(0, c^2) + (1-\lambda_j) N(0,\omega^2)\\
\lambda_j &\sim \text{Bernoulli}(\pi).
\end{align*}

-   $\omega \ll c$ and the indicator variable $\lambda_j \in \{0, 1\}$ denotes whether $\beta_j$ is close to zero (comes from the "spike", $\lambda_j = 0$) or nonzero (comes from the "slab", $\lambda_j = 1$).

## Spike-and-slab prior

-   Often $\omega = 0$ (the spike is a true spike), and the prior can be written as,

\begin{align*}
\beta_j | \lambda_j, c^2 &\sim N(0, \lambda_j^2 c^2)\\
\lambda_j &\sim \text{Bernoulli}(\pi).
\end{align*}

-   Instead of giving continuous priors for $\lambda_j$'s as in the horseshoe, here only two values are allowed (0,1).

-   The shrinkage factor $\kappa_j$ only has mass at $\kappa_j = \frac{1}{1 + n\sigma^{-2}\tau^{2}}$ and $\kappa_j = 1$ with probabilities $\pi$ and $1-\pi$,

## Similarity to horseshoe {.midi}

-   Letting $c \rightarrow \infty$, all the mass is concentrated at the extremes $\kappa_j = 0$ and $\kappa_j = 1$ (this resembles the horseshoe).

![](images/10/aki1.png){fig-alt="workflow" fig-align="center" height="3in"}

[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)

-   The horseshoe can be seen as a continuous approximation to the spike-and-slab prior as $c \rightarrow \infty$.

## Regularized horseshoe prior

\begin{align*}
\beta_j | \lambda_j, \tau, c &\sim N\left(0, \tau^2 \tilde{\lambda}_j^2\right),\quad \tilde{\lambda}_j^2 = \frac{c^2 \lambda_j^2}{c^2 + \tau^2 \lambda_j^2},\\
\lambda_j &\sim \mathcal C^+(0,1).
\end{align*}

-   When $\tau^2 \lambda_j^2 \ll c^2$ (i.e., $\beta_j$ close to zero), $\beta_j \sim  N\left(0, \tau^2\lambda_j^2\right)$

-   When $\tau^2 \lambda_j^2 \gg c^2$, (i.e., $\beta_j$ far from zero), $\beta_j \sim  N\left(0, c^2\right)$

-   $c \rightarrow \infty$ recovers the original horseshoe.

**Why is this an appealing extension?**

## Regularized horseshoe compared to spike-and-slab

-   The regularized horseshoe prior is comparable to the spike-and-slab with finite $c$.

![](images/10/aki2.png){fig-alt="workflow" fig-align="center" height="3in"}

[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)

## Choosing a prior for $c^2$

-   Unless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for $c$ instead of fixing it.

-   Often a reasonable choice is, $$c^2 \sim \text{Inv-Gamma}(\alpha, \beta), \quad \alpha = \nu/2, \beta = \nu s^2 / 2,$$

-   This translates to a $t_{\nu}(0,s^2)$ slab for the coefficients far from 0.

-   Another motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero.

## Choosing a prior for $\tau$

-   Carvalho et al. 2009 suggest $\tau \sim \mathcal C^+(0,1)$.

-   Polson and Scott 2011 recommend $\tau | \sigma \sim \mathcal C^+(0, \sigma^2)$.

-   Another prior comes from a quantity called the effective number of nonzero coefficients,\
    $$m_{eff} = \sum_{j=1}^p (1 - \kappa_j).$$

## Global shrinkage parameter $\tau$

-   The prior mean can be shown to be,

$$\mathbb{E}\left[m_{eff} | \tau, \sigma\right] = \frac{\tau \sigma^{-1} \sqrt{n}}{1 + \tau \sigma^{-1} \sqrt{n}}p.$$

-   Setting $\mathbb{E}\left[m_{eff} | \tau, \sigma\right] = p_0$ (prior guess for the number of non-zero coefficients) yields for $\tau$,

$$\tau_0 = \frac{p_0}{p - p_0} \frac{\sigma}{\sqrt{n}}$$

## Global shrinkage parameter $\tau$

![](images/10/aki3.png){fig-alt="workflow" fig-align="center" height="5in"}

[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)

## Prepare for next class

-   We are going to jump into an AE on body fat, but first some reminders.

-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03), which was just assigned.

-   Complete reading to prepare for next Tuesday's lecture

-   Tuesday's lecture: Classification
