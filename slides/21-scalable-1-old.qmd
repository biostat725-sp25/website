---
title: "Scalable Gaussian Processes #1"
author: "Christine Shen"
date: "2025-04-01"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    html-math-method: mathjax
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
editor_options: 
  chunk_output_type: console
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(sf)
library(rnaturalearth)
```

## Review of previous lectures {.midi}

Two weeks ago, we learned about:

1.  Gaussian processes, and

2.  How to use Gaussian processes for

    -   longitudinal data
    -   geospatial data

## Motivating dataset {.midi}

Recall we worked with a dataset on women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. Variables are:

-   `loc_id`: location id (i.e. survey cluster).

-   `hemoglobin`: hemoglobin level (g/dL).

-   `anemia`: anemia classifications.

-   `age`: age in years.

-   `urban`: urban vs. rural.

-   `LATNUM`: latitude.

-   `LONGNUM`: longitude.

## Motivating dataset {.midi}

```{r, echo = FALSE}
data <- read.csv(file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/data/drc/hemoglobin_anemia.csv")
data <- data[complete.cases(data),-1]

# Convert to spatial dataset and merge DRC data
data_sf <- st_as_sf(data, coords = c("LONGNUM", "LATNUM"), crs = 4326)
congo_states_map <- ne_states(country = "Democratic Republic of the Congo", returnclass = "sf") %>%
  select(name,geometry)
# Ensure that the CRS for the country and grid points match
congo_states_map <- st_transform(congo_states_map, crs = 4326) 
data_sf_drc <- st_intersection(data_sf, congo_states_map)

head(data)
```

::: callout-important
## Modeling goals:

-   Learn the associations between age and urbanicity and hemoglobin, accounting for unmeasured spatial confounders.

-   Create a predicted map of hemoglobin across the spatial surface controlling for age and urbanicity, with uncertainty quantification.
:::

## Map of the Sud-Kivu state {.midi}

Last time, we focused on one state with \~500 observations at \~30 locations.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 6

states_of_interest <- c("Sud-Kivu")
data_sf_states_of_interest <- data_sf_drc %>% 
  filter(name %in% states_of_interest) %>%
  group_by(loc_id, urban) %>%
  summarise(hemoglobin=mean(hemoglobin))

map_state_of_interest <- subset(congo_states_map, name %in% states_of_interest)

ggplot() +
  geom_sf(data = map_state_of_interest, fill = "lightblue", color = "black", size = 0.2) +
  # Plot the points from your data
  geom_sf(data = data_sf_states_of_interest, aes(color = hemoglobin), shape = 16, size = 2) +
  scale_color_viridis_b() + 
  theme_minimal() +
  labs(title = "Average hemoglobin at each community",
       subtitle = paste("State:", paste(states_of_interest, collapse = ", ")),
       caption = "Data Source: rnaturalearth",
       color = "Hemoglobin (g/dL)") 
```

## Prediction for the Sud-Kivu state {.midi}

And we created a $20 \times 20$ grid for prediction of the spatial intercept surface over the Sud-Kivu state.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: "center"
#| fig-width: 5
#| fig-height: 5
#| layout-ncol: 2
# Extract bounding box coordinates for our particular states
bbox <- st_bbox(map_state_of_interest)

# Create a grid of points within the bounding box
# Adjust the step size for the grid resolution (e.g., 0.1 degree)
latitudes <- seq(bbox["ymin"], bbox["ymax"], length.out = 20)
longitudes <- seq(bbox["xmin"], bbox["xmax"], length.out = 20)
# latitudes <- seq(bbox[, "ymin"], bbox[, "ymax"], length.out = 20)
# longitudes <- seq(bbox[, "xmin"], bbox[, "xmax"], length.out = 20)

# Create a data frame with all combinations of latitudes and longitudes
grid_points <- expand.grid(lat = latitudes, long = longitudes)

# Convert the grid into an sf object
grid_sf <- st_as_sf(grid_points, coords = c("long", "lat"), crs = 4326)

# Ensure both the grid and the country map are in the same CRS
grid_sf <- st_transform(grid_sf, crs = 4326)

# Keep only points within the DRC boundary using st_within (points must be within the polygon)
grid_inside_drc <- grid_sf[st_within(grid_sf, map_state_of_interest, sparse = FALSE), ]

# Plot the grid points within the states
ggplot() +
  geom_sf(data = map_state_of_interest, fill = "lightblue", color = "black") +
  geom_sf(data = grid_sf, color = "red", shape = 16, size = 1) +
  theme_minimal() +
  labs(title = "Grid of Points across Sud-Kivu",
       caption = "",
       # subtitle = "Red points represent valid grid points inside DRC boundaries",
       x = "Longitude", y = "Latitude")
ggplot() +
  geom_sf(data = map_state_of_interest, fill = "lightblue", color = "black") +
  geom_sf(data = grid_inside_drc, color = "red", shape = 16, size = 1) +
  theme_minimal() +
  labs(title = "Grid of Points WITHIN Sud-Kivu",
       caption = "Data Source: rnaturalearth",
       # subtitle = "Red points represent valid grid points inside DRC boundaries",
       x = "Longitude", y = "Latitude")
```

## Map of the DRC {.midi}

Today we will extend the analysis to the full dataset with \~8,600 observations at \~500 locations.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: "center"
#| fig-width: 8
#| fig-height: 6

data_mean <- data %>%
  group_by(loc_id, urban, LATNUM, LONGNUM) %>%
  summarise(hemoglobin=mean(hemoglobin))
data_sf_whole <- st_as_sf(data_mean, coords = c("LONGNUM", "LATNUM"), crs = 4326)

ggplot() +
  # Plot the map of DRC
  geom_sf(data = congo_states_map, fill = "lightblue", color = "black") +
  geom_sf(data = map_state_of_interest, 
          fill = NA, color = "#a50f15", linewidth = 1.5) +
  geom_sf(data = data_sf_whole, aes(color = hemoglobin), shape = 16, size = 2) +
  scale_color_viridis_b() + 
  # Customize the plot appearance
  theme_minimal() +
  labs(title = "Average Hemoglobin Across Communities in the Democratic Republic of Congo",
       subtitle = "Points represent community locations with hemoglobin averages",
       caption = "Data Source: rnaturalearth",
       x = "Longitude", y = "Latitude", color = "Hemoglobin (g/dL)") 

```

## Prediction for the DRC {.midi}

And we will make predictions on a $30 \times 30$ grid over the DRC.

```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: "center"
#| fig-width: 5
#| fig-height: 5
#| layout-ncol: 2

# Extract bounding box coordinates
bbox <- st_bbox(congo_states_map)

latitudes <- seq(bbox["ymin"], bbox["ymax"], length.out = 30)
longitudes <- seq(bbox["xmin"], bbox["xmax"], length.out = 30)
grid_points <- expand.grid(lat = latitudes, long = longitudes)

# Convert the grid into an sf object
grid_sf <- st_as_sf(grid_points, coords = c("long", "lat"), crs = 4326)
in_drc <- st_within(grid_sf, congo_states_map, sparse = FALSE)
in_drc <- apply(in_drc,1,any)

grid_inside_drc <- grid_sf[in_drc, ]

# Plot the grid points within the states
ggplot() +
  geom_sf(data = congo_states_map, fill = "lightblue", color = "black") +
  geom_sf(data = grid_sf, color = "red", shape = 16, size = 1) +
  theme_minimal() +
  labs(title = "Grid of Points across DRC",
       caption = "",
       # subtitle = "Red points represent valid grid points inside DRC boundaries",
       x = "Longitude", y = "Latitude")
ggplot() +
  geom_sf(data = congo_states_map, fill = "lightblue", color = "black") +
  geom_sf(data = grid_inside_drc, color = "red", shape = 16, size = 1) +
  theme_minimal() +
  labs(title = "Grid of Points WITHIN DRC",
       caption = "Data Source: rnaturalearth",
       # subtitle = "Red points represent valid grid points inside DRC boundaries",
       x = "Longitude", y = "Latitude")

```

## Modeling {.midi}

\begin{align*}
  Y_j(\mathbf{u}_i) &= \alpha + \mathbf{x}_j(\mathbf{u}_i) \boldsymbol{\beta} + \theta(\mathbf{u}_i) + \epsilon_j(\mathbf{u}_i), \quad \epsilon_j(\mathbf{u}_i) \stackrel{iid}{\sim} N(0,\sigma^2)
\end{align*}

**Data objects:**

-   $i \in \{1,\dots,n\}$ indexes unique locations.

-   $j \in \{1,\dots,n_i\}$ indexes individuals at each location.

-   $Y_j(\mathbf{u}_i)$ denotes the hemoglobin level of individual $j$ at location $\mathbf{u}_i$.

-   $\mathbf{x}_j(\mathbf{u}_i) = (\text{age}_{ij}/10,\text{urban}_i) \in \mathbb{R}^{1 \times p}$, where $p=2$ is the number of predictors (excluding intercept).

## Modeling {.midi}

\begin{align*}
  Y_j(\mathbf{u}_i) &= \alpha + \mathbf{x}_j(\mathbf{u}_i) \boldsymbol{\beta} + \theta(\mathbf{u}_i) + \epsilon_j(\mathbf{u}_i), \quad \epsilon_j(\mathbf{u}_i) \stackrel{iid}{\sim} N(0,\sigma^2)
\end{align*}

**Population parameters:**

-   $\alpha \in \mathbb{R}$ is the intercept.

-   $\boldsymbol{\beta} \in \mathbb{R}^p$ is the regression coefficients.

-   $\sigma^2 \in \mathbb{R}^+$ is the overall residual error (nugget).

**Location-specific parameters:**

-   $\mathbf{u}_i = (\text{longitude}_i,\text{latitude}_i) \in \mathbb{R}^2$ denotes coordinates of location $i$.

-   $\theta(\mathbf{u}_i)$ denotes the spatial intercept at location $\mathbf{u}_i$.

## Location-specific notation {.midi}

$$\mathbf{Y}(\mathbf{u}_i) = \alpha \mathbf{1}_{n_i} + \mathbf{X}(\mathbf{u}_i) \boldsymbol{\beta} + \theta(\mathbf{u}_i)\mathbf{1}_{n_i} + \boldsymbol{\epsilon}(\mathbf{u}_i), \quad \boldsymbol{\epsilon}(\mathbf{u}_i) \sim N_{n_i}(\mathbf{0},\sigma^2\mathbf{I})$$

-   $\mathbf{Y}(\mathbf{u}_i) = (Y_1(\mathbf{u}_i),\ldots,Y_{n_i}(\mathbf{u}_i))^\top$.

-   $\mathbf{X}(\mathbf{u}_i)$ is an $n_i \times p$ dimensional matrix with rows $\mathbf{x}_j(\mathbf{u}_i)$.

-   $\boldsymbol{\epsilon}(\mathbf{u}_i) = (\epsilon_i(\mathbf{u}_i),\ldots,\epsilon_{n_i}(\mathbf{u}_i))^\top$.

## Full data notation {.midi}

$$\mathbf{Y} = \alpha \mathbf{1}_{N} + \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\theta} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim N_N(\mathbf{0},\sigma^2\mathbf{I})$$

-   $\mathbf{Y} = (\mathbf{Y}(\mathbf{u}_1)^\top,\ldots,\mathbf{Y}(\mathbf{u}_{n})^\top)^\top \in \mathbb{R}^N$, with $N = \sum_{i=1}^n n_i$.

-   $\mathbf{X} \in \mathbb{R}^{N \times p}$ stacks $\mathbf{X}(\mathbf{u}_i)$.

-   $\boldsymbol{\theta} = (\theta(\mathbf{u}_1),\ldots,\theta(\mathbf{u}_n))^\top \in \mathbb{R}^n$.

-   $\mathbf{Z}$ is an $N \times n$ dimensional block diagonal binary matrix. Each row contains a single 1 in column $i$ that corresponds to the location of $Y_j(\mathbf{u}_i)$. $$
    \begin{align}
    \mathbf{Z} = \begin{bmatrix}
    \mathbf{1}_{n_1} & \mathbf{0} & \dots & \mathbf{0} \\
    \mathbf{0} & \mathbf{1}_{n_2} & \dots & \mathbf{0}  \\
    \vdots & \vdots & \ddots & \vdots \\
    \mathbf{0} & \dots & \mathbf{0} & \mathbf{1}_{n_n}
    \end{bmatrix}.
    \end{align}
    $$

## Modeling {.midi}

We specify the following model: $$\mathbf{Y} = \alpha \mathbf{1}_{N} + \mathbf{X} \boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\theta} + \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim N_N(\mathbf{0},\sigma^2\mathbf{I})$$ with priors

-   $\boldsymbol{\theta}(\mathbf{u}) | \tau,\rho \sim GP(\mathbf{0},C(\cdot,\cdot))$, where $C$ is the MatÃ©rn 3/2 covariance function with magnitude $\tau$ and length scale $\rho$.
-   $\alpha^* \sim N(0,4^2)$. This is the intercept after centering $\mathbf{X}$.
-   $\beta_j | \sigma_{\beta} \sim N(0,\sigma_{\beta}^2)$, $j \in \{1,\dots,p\}$
-   $\sigma \sim \text{Half-Normal}(0, 2^2)$
-   $\tau \sim \text{Half-Normal}(0, 4^2)$
-   $\rho \sim \text{Inv-Gamma}(5, 5)$
-   $\sigma_{\beta} \sim \text{Half-Normal}(0, 2^2)$

## Computational issues with GP {.midi}

Effectively, the prior for $\boldsymbol{\theta}$ is $$\boldsymbol{\theta} | \tau,\rho \sim N_n(\mathbf{0},\mathbf{C}), \quad \mathbf{C} \in \mathbb{R}^{n \times n}.$$ MatÃ©rn 3/2 is an isotropic covariance function, $C(\mathbf{u}_i, \mathbf{u}_j) = C(\|\mathbf{u}_i-\mathbf{u}_j\|)$.

$$\mathbf{C} = \begin{bmatrix}
C(\mathbf{0}) & C(\|\mathbf{u}_1 - \mathbf{u}_2\|) & \cdots & C(\|\mathbf{u}_1 - \mathbf{u}_n\|)\\
C(\|\mathbf{u}_1 - \mathbf{u}_2\|) & C(\mathbf{0}) & \cdots & C(\|\mathbf{u}_2 - \mathbf{u}_n\|)\\
\vdots & \vdots & \ddots & \vdots\\
C(\|\mathbf{u}_{1} - \mathbf{u}_n\|) & C(\|\mathbf{u}_2 - \mathbf{u}_n\|) & \cdots & C(\mathbf{0})\\
\end{bmatrix}.$$

This is not scalable because we need to invert an $n \times n$ dense covariance matrix for each MCMC iteration, which requires $\mathcal{O}(n^3)$ floating point operations (flops), and $\mathcal{O}(n^2)$ memory.

## Scalable GP methods overview {.midi}

The computational issues motivated exploration in scalable GP methods. Existing scalable methods broadly fall under two categories.

::: incremental
1.  Sparsity methods

    -   sparsity in $\mathbf{C}$, e.g., covariance tapering (@furrer2006covariance).
    -   sparsity in $\mathbf{C}^{-1}$, e.g., Vecchia approximation (@vecchia1988estimation) and nearest-neighbor GP (@datta2016hierarchical).

2.  Low-rank methods

    -   approximate $\mathbf{C}$ on a low-dimensional subspace.
    -   e.g., process convolution (@higdon2002space), inducing point method(@snelson2005sparse).
:::

## Hilbert space method for GP {.midi}

-   @solin2020hilbert introduced a Hilbert space method for reduced-rank Gaussian process regression (HSGP).

-   @riutort2023practical discussed how to practically implement HSGP.

-   Tutorial codes are available in different probabilistic programming languages:

    -   [stan](https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper)
    -   [NumPyro](https://num.pyro.ai/en/0.15.2/examples/hsgp.html)
    -   [pyMC](https://juanitorduz.github.io/hsgp_intro/)

## Lecture plan {.midi}

Today:

-   How does HSGP work
-   Why HSGP is scalable
-   How to use HSGP for Bayesian geospatial model fitting and posterior predictive sampling

::: fragment
Thursday:

-   Parameter tuning for HSGP
-   How to implement HSGP in `stan`
:::

## HSGP approximation {.midi}

Given:

-   an isotropic covariance function $C$ which admits a *power spectral density*, e.g., the MatÃ©rn family, and
-   a compact domain $\boldsymbol{\Theta} \in \mathbb{R}^d$ with *smooth* boundaries. For our purposes, we only consider *boxes*, e.g., $[-1,1] \times [-1,1]$.

HSGP approximates the $(i,j)$ element of the corresponding $n \times n$ covariance matrix $\mathbf{C}$ as $$\mathbf{C}_{ij}=C(\|\mathbf{u}_i - \mathbf{u}_j\|) \approx \sum_{k=1}^m s_k\phi_k(\mathbf{u}_i)\phi_k(\mathbf{u}_j).$$

## HSGP approximation {.midi}

$$\mathbf{C}_{ij}=C(\|\mathbf{u}_i - \mathbf{u}_j\|) \approx \sum_{k=1}^m s_k\phi_k(\mathbf{u}_i)\phi_k(\mathbf{u}_j).$$

-   $s_k \in \mathbb{R}^+$ are positive scalars which depends on the covariance function $C$ and its parameters $\tau$ and $\rho$.
-   $\phi_k: \boldsymbol{\Theta} \to \mathbb{R}$ are *basis functions* which only depends on $\boldsymbol{\Theta}$.
-   $m$ is the number of basis functions. Note: even with an infinite sum (i.e., $m \to \infty$), this remains an approximation (see @solin2020hilbert).

## HSGP approximation {.midi}

In matrix notation,

$$\mathbf{C} \approx \boldsymbol{\Phi} \mathbf{S} \boldsymbol{\Phi}^\top.$$

-   $\boldsymbol{\Phi} \in \mathbb{R}^{n \times m}$ is a *feature matrix*. Only depends on $\boldsymbol{\Theta}$ and the observed locations.
-   $\mathbf{S} \in \mathbb{R}^{m \times m}$ is diagonal. Depends on the covariance function $C$ and parameters $\tau$ and $\rho$.

$$
\begin{align}
  \boldsymbol{\Phi} = \begin{bmatrix}
  \phi_1(\mathbf{u}_1) & \dots & \phi_m(\mathbf{u}_1) \\
  \vdots & \ddots & \vdots \\
  \phi_1(\mathbf{u}_n) & \dots & \phi_m(\mathbf{u}_n)
  \end{bmatrix}, \quad 
  \mathbf{S} = \begin{bmatrix}
  s_1 &  &  \\
  & \ddots &  \\
  &  & s_m
  \end{bmatrix}.
\end{align}
$$

## Why HSGP is scalable {.midi}

$$\mathbf{C} \approx \boldsymbol{\Phi} \mathbf{S} \boldsymbol{\Phi}^\top.$$

-   $\boldsymbol{\Phi}$ only depends on $\boldsymbol{\Theta}$ and the observed locations, can be pre-calculated.
-   No matrix inversion.
-   Each MCMC iteration requires $\mathcal{O}(nm + m)$ flops, vs $\mathcal{O}(n^3)$ for a full GP.
-   Ideally $m \ll n$, but HSGP can be faster even for $m>n$.

## Model reparameterization {.midi}

Under HSGP, approximately $$\boldsymbol{\theta} \overset{d}{=} \boldsymbol{\Phi} \mathbf{S}^{1/2}\mathbf{b}, \quad \mathbf{b} \sim N_m(0,\mathbf{I}).$$

Therefore we can reparameterize the model as

$$
\begin{align}
  \mathbf{Y} &= \alpha \mathbf{1}_{N} + X\boldsymbol{\beta} + \mathbf{Z}\boldsymbol{\theta} + \boldsymbol{\epsilon} \\
  &\approx \alpha \mathbf{1}_{N} + X\boldsymbol{\beta} + \underbrace{\mathbf{Z}\boldsymbol{\Phi} \mathbf{S}^{1/2}}_{\mathbf{W}}\mathbf{b} + \boldsymbol{\epsilon}
\end{align}
$$

Note the resemblance to linear regression:

-   $\mathbf{W} \in \mathbb{R}^{n \times m}$ is a known design matrix given parameters $\tau$ and $\rho$.
-   $\mathbf{b}$ is an unknown parameter vector with prior $N_m(0,\mathbf{I})$.

## HSGP in `stan` {.small}

Similarly, we can use the reparameterized model in `stan`.

This is called the [*non-centered parameterization*](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#hierarchical-models-and-the-non-centered-parameterization) in `stan` documentation. It's recommended for computational efficiency for hierarchical models.

```{stan output.var = "model_in_stan", eval = FALSE}
transformed data {
  matrix[n,m] PHI;
  matrix[N,m] Z;
  matrix[N,p] X_centered;
}
parameters {
  real alpha_star;
  real<lower=0> sigma;
  vector[p] beta;
  vector[m] b;
  vector<lower=0>[m] sqrt_S;
  ...
}
model {
  vector[n] theta = PHI * (sqrt_S .* b);
  target += normal_lupdf(y | alpha + X * beta + Z * theta, sigma);
  target += normal_lupdf(b | 0, 1);
  ...
}
```

## Posterior predictive distribution {.midi}

We want to make predictions for $\mathbf{Y}^* = (Y(\mathbf{u}_{n+1}),\ldots, Y(\mathbf{u}_{n+q}))^\top$, observations at $q$ new locations. Define $\boldsymbol{\theta}^* = (\theta(\mathbf{u}_{n+1}),\ldots,\theta(\mathbf{u}_{n+q}))^\top$, $\boldsymbol{\Omega} = (\alpha,\boldsymbol{\beta},\sigma,\tau,\rho)$. Recall:

\begin{align*}
  f(\mathbf{Y}^* | \mathbf{Y}) &= \int f(\mathbf{Y}^*, \boldsymbol{\theta}^*, \boldsymbol{\theta}, \boldsymbol{\Omega} | \mathbf{Y}) d\boldsymbol{\theta}^* d\boldsymbol{\theta} d\boldsymbol{\Omega}\\
  &= \int \underbrace{f(\mathbf{Y}^* | \boldsymbol{\theta}^*, \boldsymbol{\Omega})}_{(1)} \underbrace{f(\boldsymbol{\theta}^* | \boldsymbol{\theta}, \boldsymbol{\Omega})}_{(2)} \underbrace{f(\boldsymbol{\theta},\boldsymbol{\Omega} | \mathbf{Y})}_{(3)} d\boldsymbol{\theta}^* d\boldsymbol{\theta} d\boldsymbol{\Omega}\\
\end{align*}

(1) Likelihood: $f(\mathbf{Y}^* | \boldsymbol{\theta}^*, \boldsymbol{\Omega})$ [-- remains the same as for GP]{.fragment data-fragment-index="1" style="color: #a50f15;"}

(2) Kriging: $f(\boldsymbol{\theta}^* | \boldsymbol{\theta}, \boldsymbol{\Omega})$ [-- we will focus on this next]{.fragment data-fragment-index="2" style="color: #a50f15;"}

(3) Posterior distribution: $f(\boldsymbol{\theta},\boldsymbol{\Omega} | \mathbf{Y})$ [-- we have just discussed]{.fragment data-fragment-index="0" style="color: #a50f15;"}

## Kriging {.midi}

Recall under the GP prior,

$$\begin{bmatrix}
    \boldsymbol{\theta}\\
    \boldsymbol{\theta}^*
  \end{bmatrix} \Bigg| \boldsymbol{\Omega} \sim N_{n+q}\left(\begin{bmatrix}
    \mathbf{0}_n \\
    \mathbf{0}_q
  \end{bmatrix}, \begin{bmatrix}
    \mathbf{C} & \mathbf{C}_{+}\\
    \mathbf{C}_{+}^\top & \mathbf{C}^*
  \end{bmatrix}\right),$$

where $\mathbf{C}$ is the covariance of $\boldsymbol{\theta}$, $\mathbf{C}^*$ is the covariance of $\boldsymbol{\theta}^*$, and $\mathbf{C}_{+}$ is the cross covariance matrix between $\boldsymbol{\theta}$ and $\boldsymbol{\theta}^*$.

Therefore by properties of multivariate normal, $$\boldsymbol{\theta}^* \mid (\boldsymbol{\theta}, \boldsymbol{\Omega}) \sim N_q(\mathbb{E}_{\boldsymbol{\theta}^*},\mathbb{V}_{\boldsymbol{\theta}^*}), \quad \text{where}$$ $$
\begin{align}
  \mathbb{E}_{\boldsymbol{\theta}^*} &= \mathbf{C}_+^\top \mathbf{C}^{-1} \boldsymbol{\theta}\\
  \mathbb{V}_{\boldsymbol{\theta}^*} &= \mathbf{C}^* - \mathbf{C}_+^\top \mathbf{C}^{-1} \mathbf{C}_+.
\end{align}
$$

## Kriging under HSGP {.midi}

Under HSGP, $\mathbf{C}^* \approx \boldsymbol{\Phi}^* \mathbf{S}\boldsymbol{\Phi}^{*\top}$, $\mathbf{C}_+ \approx \boldsymbol{\Phi} \mathbf{S}\boldsymbol{\Phi}^{*\top}$, where $$
\begin{align}
  \boldsymbol{\Phi}^* \in \mathbb{R}^{q \times m} = \begin{bmatrix}
  \phi_1(\mathbf{u}_{n+1}) & \dots & \phi_m(\mathbf{u}_{n+1}) \\
  \vdots & \ddots & \vdots \\
  \phi_1(\mathbf{u}_{n+q}) & \dots & \phi_m(\mathbf{u}_{n+q})
  \end{bmatrix}
\end{align}
$$ is the feature matrix for the new locations. Therefore approximately $$
\begin{align}
  \begin{bmatrix}
    \boldsymbol{\theta} \\
    \boldsymbol{\theta}^*
  \end{bmatrix} \Bigg| \boldsymbol{\Omega} \sim N_{n
  +q} \left(\begin{bmatrix}
    \mathbf{0}_n \\
    \mathbf{0}_q
  \end{bmatrix},
  \begin{bmatrix}
    \boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top & \boldsymbol{\Phi}\mathbf{S} \boldsymbol{\Phi}^{*\top} \\
    \boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top & \boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^{*\top}
  \end{bmatrix} \right).
\end{align}
$$

## Kriging under HSGP {.midi}

Again by properties of multivariate normal, $$\boldsymbol{\theta}^* \mid (\boldsymbol{\theta}, \boldsymbol{\Omega}) \overset{?}{\sim} N_q(\mathbb{E}_{\boldsymbol{\theta}^*}^{HS},\mathbb{V}_{\boldsymbol{\theta}^*}^{HS}),$$

$$
\begin{align}
  \mathbb{E}_{\boldsymbol{\theta}^*}^{HS} &= (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{-1} \boldsymbol{\theta}\\
  \mathbb{V}_{\boldsymbol{\theta}^*}^{HS} &= (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^{*\top}) - (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{-1}(\boldsymbol{\Phi}\mathbf{S} \boldsymbol{\Phi}^{*\top}).
\end{align}
$$

::: incremental
-   If $m \ge n$, $(\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)$ is invertible, this is the kriging distribution under HSGP.
-   But what if $m < n$?
:::

## Kriging under HSGP {.midi}

If $m \le n$, claim $\boldsymbol{\theta}^* \mid (\boldsymbol{\theta}, \boldsymbol{\Omega}) = (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{\dagger} \boldsymbol{\theta},$ where $\mathbf{A}^\dagger$ denotes a generalized inverse of matrix $\mathbf{A}$ such that $\mathbf{A}^\dagger\mathbf{A}=\mathbf{I}$[^1]. Sketch proof below, see details in class.

[^1]: This result holds more generally for $(\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{-}$ where $\mathbf{A}^-$ denotes any generalized inverse of matrix $\mathbf{A}$ such that $\mathbf{A}\mathbf{A}^-\mathbf{A}=\mathbf{A}$. We only prove for $(\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^\dagger$ here for simplicity.

1.  By properties of multivariate normal, $\boldsymbol{\theta}^* \mid (\boldsymbol{\theta}, \boldsymbol{\Omega}) \sim N_q(\mathbb{E}_{\boldsymbol{\theta}^*}^{HS},\mathbb{V}_{\boldsymbol{\theta}^*}^{HS})$, $$
    \begin{align}
      \mathbb{E}_{\boldsymbol{\theta}^*}^{HS} &= (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{\dagger} \boldsymbol{\theta}\\
      \mathbb{V}_{\boldsymbol{\theta}^*}^{HS} &= (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^{*\top}) - (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{\dagger \top}(\boldsymbol{\Phi}\mathbf{S} \boldsymbol{\Phi}^{*\top}).
    \end{align}
    $$

2.  Show if $\boldsymbol{\Phi}$ has full column rank, which is true under HSGP, then $\mathbb{V}_{\boldsymbol{\theta}^*}^{HS} \equiv \mathbf{0}$ because for any vector $\mathbf{v} \in \mathbb{R}^m$, $$\mathbf{S} \boldsymbol{\Phi}^\top(\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{\dagger \top}\boldsymbol{\Phi}\mathbf{v} = \mathbf{v} \tag{1}.$$

## Kriging under HSGP {.midi}

Under the reparameterized model, $\boldsymbol{\theta} = \boldsymbol{\Phi} \mathbf{S}^{1/2}\mathbf{b}$, for $\mathbf{b} \sim N_m(0,\mathbf{I}).$ Therefore $$
\begin{align}
  \boldsymbol{\theta}^* \mid (\boldsymbol{\theta},\boldsymbol{\Omega}) &= (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{\dagger \top} \boldsymbol{\theta} \\
  &= (\boldsymbol{\Phi}^*\mathbf{S}\boldsymbol{\Phi}^\top) (\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)^{\dagger \top}(\boldsymbol{\Phi} \mathbf{S}^{1/2}\mathbf{b}) \\
  &= \boldsymbol{\Phi}^*\mathbf{S}^{1/2}\mathbf{b}. \quad (\text{by equation (1) in the last slide})
\end{align}
$$

During MCMC sampling, we can obtain posterior predictive samples for $\boldsymbol{\theta}^*$ through posterior samples of $\mathbf{b}$ and $\mathbf{S}$. Let superscript $(s)$ denote the $s$th posterior sample:

$$\boldsymbol{\theta}^{*(s)} = \boldsymbol{\Phi}^* \mathbf{S}^{(s) 1/2} \mathbf{b}^{(s)}.$$

## Kriging under HSGP -- alternative view {.midi}

Under the reparameterized model, there is another (perhaps more intuitive) way to recognize the kriging distribution under HSGP when $m \le n$.

We model $\boldsymbol{\theta} = \boldsymbol{\Phi} \mathbf{S}^{1/2}\mathbf{b}$, where $\mathbf{b}$ is treated as the unknown parameter. Therefore for kriging: $$
\begin{align}
  \boldsymbol{\theta}^* \mid (\boldsymbol{\theta},\boldsymbol{\Omega}) &= \boldsymbol{\Phi}^*\mathbf{S}^{1/2}\mathbf{b} \mid (\mathbf{b},\mathbf{S},\boldsymbol{\Omega}) \\
  &=\boldsymbol{\Phi}^*\mathbf{S}^{1/2}\mathbf{b}.
\end{align}
$$

## HSGP kriging in `stan` {.midi}

If $m \le n$, Kriging under HSGP can be easily implemented in `stan`.

```{stan output.var = "model_in_stan", eval = FALSE}
transformed data {
  matrix[q,m] PHI_new;
}
parameters {
  vector[m] b;
  vector<lower=0>[m] sqrt_S;
}
generated quantities {
  vector[q] theta_new = PHI_new * (sqrt_S .* b);
}

```

::: fragment
If $m>n$, we need to invert an $n \times n$ matrix $(\boldsymbol{\Phi}\mathbf{S}\boldsymbol{\Phi}^\top)$ for kriging, which could be computationally prohibitive.
:::

## Recap {.midi}

HSGP is a low rank approximation method for GP.

$$
\begin{align}
  \mathbf{C}_{ij} \approx \sum_{k=1}^m s_k\phi_k(\mathbf{u}_i)\phi_k(\mathbf{u}_j), \quad \mathbf{C} \approx \boldsymbol{\Phi} \mathbf{S} \boldsymbol{\Phi}^\top,
\end{align}
$$

-   for covariance function $C$ which admits a power spectral density.
-   on a box $\boldsymbol{\Theta} \subset \mathbb{R}^d$.
-   with $m$ number of basis functions.

We have talked about:

-   why HSGP is scalable.
-   how to do posterior sampling and posterior predictive sampling in `stan`.

## HSGP parameters {.midi}

@solin2020hilbert showed that HSGP approximation can be made arbitrarily accurate as $\boldsymbol{\Theta}$ and $m$ increase.

::: fragment
But how to choose:

-   size of the box $\boldsymbol{\Theta}$.
-   number of basis functions $m$.
:::

:::: fragment
::: callout-important
## Our goal:

Minimize the run time while maintaining reasonable approximation accuracy.
:::
::::

::: fragment
*Note: we treat estimation of the GP magnitude parameter* $\tau$ as a separate problem, and only consider approximation accuracy of HSGP in terms of the correlation function.
:::

## HSGP approximation box {.midi}

Due to the design of HSGP, the approximation is less accurate near the boundaries of $\boldsymbol{\Theta}$.

-   Suppose all the coordinates are centered. Let $$S_l = \max_i |\mathbf{u}_{il}|, \quad l=1,\dots,d, \quad i= 1, \dots, (n+q)$$ such that $\boldsymbol{\Theta}_S = \prod_{l=1}^d [-S_l,S_l]$ is the smallest box which contains all observed and prediction locations. We should at least ensure $\boldsymbol{\Theta} \supset \boldsymbol{\Theta}_S$.
-   We want the box to be large enough to ensure good boundary accuracy. Let $c_l \ge 1$ be *boundary factors*, we consider $$\boldsymbol{\Theta} = \prod_{l=1}^d [-L_l,L_l], \quad L_l = c_l S_l.$$

## HSGP approximation box and $\rho$ {.midi}

![](./images/21/HSGPbox.png){fig-align="center" height="350"}

How much the approximation accuracy deteriorates towards the boundaries depends on smoothness of the true surface.

-   the larger the length scale $\rho$, the smoother the surface, a smaller box (smaller $c$) can be used for the same level of boundary accuracy.

## HSGP approximation box and $m$ {.midi}

![](./images/21/HSGPbox.png){fig-align="center" height="350"}

The larger the box,

-   the more basis functions we need for the same level of overall accuracy,
-   hence higher run time.

## Zooming out doesn't simplify the problem {.midi}

![](./images/21/HSGPbox2.png){fig-align="center" height="350"}

-   If we scale the coordinates by a constant $b$, the length scale $\rho$ of the underlying GP also needs to be approximately scaled by $b$ to capture the same level of details in the data.
-   We can effectively think of the length scale parameter as $(\rho/\|\mathbf{S}\|)$.

## HSGP basis functions {.midi}

The total number of basis functions $m = \prod_{l=1}^d m_l$, i.e., we need to decide on $m_l$'s, the number of basis functions for each dimension.

-   $m$ scales exponentially in $d$, hence the HSGP computation complexity $\mathcal{O}(mn+m)$ also scales exponentially in $d$. Therefore HSGP is only recommended for $d \le 3$, at most $4$.
-   The higher the $m$, the better the overall approximation accuracy, the higher the runtime.

## Relationship between $c$, $m$ and $\rho$ {.midi}

Let's quickly recap. For simplicity, let $d=1$,

::: incremental
1.  As $(\rho/S)$ decreases, the surface is less smooth,
    -   $c$ needs to increase to retain boundary accuracy.
    -   $m$ needs to increase to retain overall accuracy.
2.  As $c$ increases, $m$ needs to increase to retain overall accuracy.
3.  As $m$ increases, run time increases.
:::

::: fragment
Hence we want to minimize $m$ and $c$ while maintaining approximation accuracy.
:::

## Empirical functional form {.midi}

-   Given $c$ and $\rho/S$, let $m_C(c,\rho/S)$ denote the minimum number of basis functions needed for a near 100% accuracy under covariance function $C$.
-   @riutort2023practical used extensive simulations to obtain an empirical function form of $m_C(c,\rho/S)$ for frequently used MatÃ©rn covariance functions. E.g., for MatÃ©rn 3/2,

$$m_{3/2}(c,\rho/S)=3.24 \frac{c}{\rho/S}, \quad c \ge 4.5 \frac{\rho}S, \quad c \ge 1.2.$$

Notice:

-   linear proportionality between $m$, $c$ and $\rho/S$.
-   for a given $\rho/S$, there exists a minimum $c$, below which the approximation is poor no matter how large $m$ is.

## Question {.midi}

BUT, in real applications, we **do not** know $\rho$.

::: fragment
So how to use $m_C(c,\rho/S)$ to help choose $c$ and $m$?
:::

## Prepare for next class {.midi}

1.  Work on HW 05 which is due Apr 8

2.  Complete reading to prepare for Thursday's lecture

3.  Thursday's lecture:

    -   Parameter tuning for HSGP
    -   How to implement HSGP in `stan`

## References

::: {#refs}
:::
