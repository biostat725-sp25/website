---
title: "Robust Regression"
author: "Prof. Sam Berchuck"
date: "2025-02-11"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    html-math-method: mathjax
filters:
  - parse-latex
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(knitr)
library(mvtnorm)
library(coda)
```

## Review of last lecture

-   On Thursday, we started to branch out from linear regression.

-   We learned about approaches for nonlinear regression.

-   Today we will address approaches for robust regression, which will generalize the assumption of homoskedasticity (and also the normality assumption).

## A motivating research question

-   In today's lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.

    -   A detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).

-   For an example patient, we define $Y_i$ as the serum concentration value and $X_i$ as a child's age, given in years.

## Pulling the data

```{r}
library(Brq)
data("ImmunogG")
head(ImmunogG)
```

## Visualizing IgG data

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 3
#| fig-height: 3
#| layout-ncol: 3
dat_fig <- data.frame(X = ImmunogG$Age, Y = ImmunogG$IgG)
ggplot(dat_fig, aes(x = X, y = Y)) + 
  geom_point() + 
  scale_x_continuous(name = "Age of children in years") + 
  scale_y_continuous(name = "Immunoglobulin G (g/liter)")
ggplot(dat_fig, aes(x = Y)) + 
  geom_histogram(bins = 30) + 
  scale_x_continuous(name = "Immunoglobulin G (g/liter)") + 
  labs(y = "Count")
ggplot(dat_fig, aes(x = Y)) + 
  geom_density() + 
  scale_x_continuous(name = "Immunoglobulin G (g/liter)") + 
  labs(y = "Density")
```

## Modeling the association between age and IgG

-   Linear regression can be written as follows for $i = 1,\ldots,n$,

$$Y_i = \alpha + \beta X_i + \epsilon_i,\quad \epsilon_i \sim N(0,\sigma^2).$$

-   $\beta$ represent the the change in IgG serum concentration with a one year increase in age.

## Linear regression assumptions

\begin{align*}
Y_i &= \alpha + \beta X_i + \epsilon_i,\quad \epsilon_i \sim N(0,\sigma^2)\\
&= \mu_i + \epsilon_i.
\end{align*}

Assumptions:

1.  $Y_i$ are independent observations (independence).

2.  $Y_i$ is linearly related to $X_i$ (linearity).

3.  $\epsilon_i = Y_i - \mu_i$ is normally distributed (normality).

4.  $\epsilon_i$ has constant variance across $X_i$ (homoskedasticity).

## Assessing assumptions

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 4
#| fig-height: 4
#| layout-ncol: 2
reg <- lm(dat_fig$Y ~ dat_fig$X)
dat.fig <- data.frame(
  residuals = resid(reg),
  x = dat_fig$X,
  y = dat_fig$Y
)
ggplot(dat.fig, aes(x = residuals)) + 
  geom_histogram(bins = 30) + 
  scale_x_continuous(name = "Residuals") + 
  labs(y = "Count")
ggplot(dat.fig, aes(x = x, y = residuals)) + 
  geom_point() + 
  scale_x_continuous(name = "Age of children in years") + 
  scale_y_continuous(name = "Residuals")
```

## Robust regression

-   Today we will learn about regression techniques that are robust to the assumptions of linear regression.

-   We will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.

-   We will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression).

## Why is robust regression not more comomon?

-   Despite their desirable properties, robust methods are not widely used. **Why?**

    -   Historically computationally complex.

    -   Not available in statistical software packages.

-   Bayesian modeling using Stan alleviates these bottlenecks!

## Heteroskedasticity

-   Heteroskedasticity is the violation of the assumption of constant variance.

-   How can we handle this?

-   In OLS, there are approaches like [heteroskedastic consistent errors](https://en.wikipedia.org/wiki/Heteroskedasticity-consistent_standard_errors), but this is not a generative model.

-   In the Bayesian framework, we generally like to write down generative models.

<!-- ## Weighted regression -->

<!-- -   A common case is **weighted** regression, where each $Y_i$ represents the mean of $n_i$ observations. Then the scale of each observation is, $$\tau_i^2 = \sigma^2/n_i,$$ where $\sigma^2$ is a global scale parameter. -->

<!-- -   Alternatively, suppose each observation represents the sum of each $n_i$ observations. Then the scale of each observation is, $$\tau_i^2 = n_i \sigma^2.$$ -->

<!-- ## Weighted regression {.midi} -->

<!-- ```{stan output.var = "covariates", eval = FALSE} -->

<!-- data { -->

<!--   int<lower = 1> n; -->

<!--   int<lower = 1> p; -->

<!--   vector[n] Y; -->

<!--   matrix[n, p] X; -->

<!--   int<lower = 1> n_i[n]; -->

<!-- } -->

<!-- parameters { -->

<!--   real alpha; -->

<!--   vector[p] beta; -->

<!--   real<lower = 0> sigma2; -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[n] tau2 = sigma2 / n_i; -->

<!-- } -->

<!-- model { -->

<!--   target += normal_lpdf(Y | alpha + X * beta, sqrt(tau2)); -->

<!-- } -->

<!-- ``` -->

## Modeling the scale with covariates {.midi}

-   One option is to allow the sale to be modeled as a function of covariates.

-   It is common to model the log-transformation of the scale or variance to transform it to $\mathbb{R}$,

$$\log \tau_i = \mathbf{z}_i \boldsymbol{\gamma},$$

where $\mathbf{z}_i = (z_{i1},\ldots,z_{ip})$ are a $p$-dimensional vector of covariates and $\boldsymbol{\gamma}$ are parameters that regress the covariates onto the log standard deviation.

-   Other options include: $\log \tau_i = \mathbf{z}_i \boldsymbol{\gamma} + \nu_i,\quad \nu_i \sim N(0, \sigma^2)$

-   Other options include: $\log \tau_i = f(\mu_i)$

-   Any plausible generative model can be specified!

## Modeling the scale with covariates {.midi}

```{stan output.var = "covariates", eval = FALSE}
data {
  int<lower = 1> n;
  int<lower = 1> p;
  int<lower = 1> q;
  vector[n] Y;
  matrix[n, p] X;
  matrix[n, q] Z;
}
parameters {
  real alpha;
  vector[p] beta;
  vector[q] gamma;
}
transformed parameters {
  vector[n] tau = exp(Z * gamma);
}
model {
  target += normal_lpdf(Y | alpha + X * beta, tau);
}
```

## Heteroskedastic variance

-   We can write the regression model using a observation specific variance, $$Y_i = \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \stackrel{ind}{\sim} N(0,\tau_i^2).$$

-   One way of writing the variance is: $\tau_i^2 = \sigma^2 \lambda_i$.

    -   $\sigma^2$ is a global scale parameter.

    -   $\lambda_i$ is an observation specific scale parameter.

-   In the Bayesian framework, we must place a prior on $\lambda_i$.

<!-- ## Bayesian prior to induce structure -->

<!-- -   Suppse we would like $\sum_{i=1}^n \lambda_i = 1$, $\lambda_i >0$. -->

<!-- -   We could specify the following, -->

<!-- $$\boldsymbol{\lambda} \sim \text{Dirichlet}(\boldsymbol{\alpha}),$$ -->

<!-- where $\boldsymbol{\lambda} = (\lambda_1,\ldots,\lambda_n)$ and $\boldsymbol{\alpha} = (\alpha_1,\ldots,\alpha_n)$. -->

<!-- -   The prior mean is $\mathbb{E}[\lambda_i] = \alpha_i / \alpha_0$, where $\alpha_0 = \sum_{i=1}^n \alpha_i.$ -->

<!-- -   Typically, $\alpha_i = 1 \forall i$. -->

<!-- ## Dirchlet prior in Stan {.midi} -->

<!-- ```{stan output.var = "covariates", eval = FALSE} -->

<!-- data { -->

<!--   int<lower = 1> n; -->

<!--   int<lower = 1> p; -->

<!--   vector[n] Y; -->

<!--   matrix[n, p] X; -->

<!--   vector<lower = 0>[n] alpha; -->

<!-- } -->

<!-- parameters { -->

<!--   vector[p] beta; -->

<!--   real<lower = 0> sigma; -->

<!--   simplex[n] lambda; -->

<!-- } -->

<!-- transformed parameters { -->

<!--   vector[n] tau = sigma * sqrt(lambda); -->

<!-- } -->

<!-- model { -->

<!--   target += normal_lpdf(Y | X * beta, tau); -->

<!--   target += dirichlet_lpdf(lambda | alpha); -->

<!-- } -->

<!-- ``` -->

## A prior to induce a heavy-tail

-   A common prior for $\lambda_i$ is as follows:

$$\lambda_i \stackrel{iid}{\sim} \text{Inverse-Gamma}\left(\frac{\nu}{2},\frac{\nu}{2}\right).$$

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 6
#| fig-height: 3
#| layout-ncol: 1
library(pscl)
x <- seq(0.001, 10, length.out = 1001)
dens1 <- densigamma(x, alpha = 0.5 * 1, beta = 0.5 * 1)
dens2 <- densigamma(x, alpha = 2 / 2, beta = 2 / 2)
dens5 <- densigamma(x, alpha = 5 / 2, beta = 5 / 2)
dens10 <- densigamma(x, alpha = 10 / 2, beta = 10 / 2)
dens100 <- densigamma(x, alpha = 100 / 2, beta = 100 / 2)
dat.fig <- data.frame(
  x = rep(x, 5),
  y = c(dens1, dens2, dens5, dens10, dens100),
  nu = rep(c(1, 2, 5, 10, 100), each = 1001)
)
ggplot(dat.fig, aes(x = x, y = y, color = as.factor(nu))) + 
  geom_line(lwd = 1.25) + 
  labs(x = "X", y = "Density", subtitle = expression(paste("Inverse-Gamma distribution with various ", nu)), color = expression(nu))
```

## A prior to induce a heavy-tail

-   A common prior for $\lambda_i$ is as follows:

$$\lambda_i \stackrel{iid}{\sim} \text{Inverse-Gamma}\left(\frac{\nu}{2},\frac{\nu}{2}\right).$$

-   Under this prior, the marginal likelihood for $Y_i$ is equivalent to a Student-t distribution,

$$Y_i = \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim} t_{\nu}\left(0, \sigma\right).$$

## Understanding the equivalence {.midi}

-   Heteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.

$$Y_i = \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \stackrel{iid}{\sim} t_{\nu}\left(0, \sigma\right).$$

$$\iff$$

\begin{align*}
Y_i &= \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \stackrel{ind}{\sim} N\left(0,\sigma^2 \lambda_i\right)\\
\lambda_i &\stackrel{iid}{\sim} \text{Inverse-Gamma}\left(\frac{\nu}{2},\frac{\nu}{2}\right)
\end{align*}

-   Note that since the number of $\lambda_i$ parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution.

## Understanding the equivalence {.midi}

\begin{align*}
f(Y_i) &= \int_0^{\infty} f(Y_i , \lambda_i) d\lambda_i\\
&= \int_0^{\infty} f(Y_i | \lambda_i) f(\lambda_i) d\lambda_i\\
&= \int_0^{\infty} N(Y_i ; \mu_i, \sigma^2 \lambda_i) \text{Inverse-Gamma}\left(\lambda_i ; \frac{\nu}{2},\frac{\nu}{2}\right) d\lambda_i\\
&= t_{\nu}\left(\mu_i,\sigma\right).
\end{align*}

-   The marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter.

## Understanding the equivalence {.midi}

A random variable $T_i \stackrel{iid}{\sim} t_{\nu}$ can be written as a function of Gaussian and $\chi^2$ random variables, \begin{align*}
T_i &= \frac{Z_i}{\sqrt{\frac{W_i}{\nu}}},\quad Z_i \stackrel{iid}{\sim} N(0,1), \quad W_i \stackrel{iid}{\sim}\chi^2_{\nu}\\
&= \frac{Z_i}{\sqrt{\frac{1}{\nu V_i}}},\quad V_i \stackrel{iid}{\sim} \text{Inv-}\chi^2_{\nu},\quad V_i=W_i^{-1}\\
&= \sqrt{\nu V_i} Z_i,\quad \lambda_i = \nu V_i\\
&= \sqrt{\lambda_i} Z_i, \quad \lambda_i \stackrel{iid}{\sim} \text{Inverse-Gamma}\left(\frac{\nu}{2},\frac{\nu}{2}\right).
\end{align*}

-   We then have: $Y_i = \mu_i + \sigma T_i \sim t_{\nu}(\mu_i, \sigma).$

## Student-t in Stan {.midi}

```{stan output.var = "covariates", eval = FALSE}
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
  real<lower = 0> nu;
}
model {
  target += student_t_lpdf(Y | nu, alpha + X * beta, sigma);
}
```

## Student-t in Stan: mixture {.midi}

```{stan output.var = "covariates", eval = FALSE}
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
  vector[n] lambda;
}
transformed parameters {
  vector[n] tau = sigma * sqrt(lambda);
}
model {
  target += normal_lpdf(Y | alpha + X * beta, tau);
  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);
}
```

## Why heavy-tailed distributions?

-   Replacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.

-   Robust regression refers to regression methods which are less sensitive to outliers or small sample sizes.

-   Linear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.

-   Our heteroskedastic model that we just explored is only one example of a robust regression model.

## Vizualizing heavy tail distributions

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 7
#| fig-height: 5
#| layout-ncol: 1
x <- seq(-4, 4, length.out = 1001)
dens1 <- dt(x, df = 1)
dens2 <- dt(x, df = 2)
dens5 <- dt(x, df = 5)
dens100 <- dt(x, df = 100)
dat.fig <- data.frame(
  x = rep(x, 4),
  y = c(dens1, dens2, dens5, dens100),
  nu = rep(c(1, 2, 5, 100), each = 1001)
)
ggplot(dat.fig, aes(x = x, y = y, color = as.factor(nu))) + 
  geom_line(lwd = 1.25) + 
  labs(x = "X", y = "Density", subtitle = expression(paste("Student-t distribution with various ", nu)), color = expression(nu))
```

## Vizualizing heavy tail distributions

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 7
#| fig-height: 5
#| layout-ncol: 1
library(nimble)
library(extraDistr)
x <- seq(-4, 4, length.out = 1001)
dens_norm <- dnorm(x, mean = 0, sd = 1)
dens_t <- dlst(x, df = 4, mu = 0, sigma = sqrt(0.5))
dens_laplace <- dlaplace(x, mu = 0, sigma = sqrt(0.5))
dat.fig <- data.frame(
  x = rep(x, 3),
  y = c(dens_norm, dens_t, dens_laplace),
  sigma = rep(c("Gaussian", "Student-t", "Laplace"), each = 1001)
)
ggplot(dat.fig, aes(x = x, y = y, color = as.factor(sigma))) + 
  geom_line(lwd = 1) + 
  labs(x = "X", y = "Density", subtitle = expression(paste("Various distributions with zero mean and variance one")), color = "Distribution")
# Laplace: variance = 2 * scale^2, mean = location
# t: variance scale^2 * nu / (nu - 2), mean = location
# normal: variance: scale^2, mean = location
```

## Vizualizing heavy tail distributions

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 7
#| fig-height: 5
#| layout-ncol: 1
library(nimble)
library(extraDistr)
x <- seq(-10, 10, length.out = 1001)
dens_norm <- dnorm(x, mean = 0, sd = 1)
dens_t <- dlst(x, df = 4, mu = 0, sigma = sqrt(0.5))
dens_laplace <- dlaplace(x, mu = 0, sigma = sqrt(0.5))
dat.fig <- data.frame(
  x = rep(x, 3),
  y = c(dens_norm, dens_t, dens_laplace),
  sigma = rep(c("Gaussian", "Student-t", "Laplace"), each = 1001)
)
ggplot(dat.fig, aes(x = x, y = y, color = as.factor(sigma))) + 
  geom_line(lwd = 1) + 
  labs(x = "X", y = "Density", subtitle = expression(paste("Various distributions with zero mean and variance one")), color = "Distribution") + 
  ylim(0,0.005) + 
  xlim(3, 10)
# Laplace: variance = 2 * scale^2, mean = location
# t: variance scale^2 * nu / (nu - 2), mean = location
# normal: variance: scale^2, mean = location
```

## Another example of robust regression

-   Let's revisit our general heteroskedastic regression, $$Y_i = \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \stackrel{ind}{\sim} N(0,\sigma^2 \lambda_i).$$

-   We can induce another form of robust regression using the following prior for $\lambda_i$, $\lambda_i \sim \text{Exponential}(1/2)$.

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 6
#| fig-height: 2.75
#| layout-ncol: 1
library(pscl)
x <- seq(0.001, 10, length.out = 1001)
dens1 <- dexp(x, rate = 0.5)
dat.fig <- data.frame(
  x = rep(x, 1),
  y = c(dens1)
)
ggplot(dat.fig, aes(x = x, y = y)) + 
  geom_line(lwd = 1.25) + 
  labs(x = "X", y = "Density", subtitle = expression(paste("Exponential with rate ", 0.5, " (Mean is 2)")))
```

## Another example of robust regression

-   Let's revisit our general heteroskedastic regression, $$Y_i = \alpha + \mathbf{x}_i \boldsymbol{\beta} + \epsilon_i, \quad \epsilon_i \stackrel{ind}{\sim} N(0,\sigma^2 \lambda_i).$$

-   We can induce another form of robust regression using the following prior for $\lambda_i$, $\lambda_i \sim \text{Exponential}(1/2)$.

-   Under this prior, the induced marginal model is, $$Y_i = \alpha + \mathbf{x}_i\boldsymbol{\beta} + \epsilon_i,\quad \epsilon_i \stackrel{iid}{\sim} \text{Laplace}(\mu = 0, \sigma).$$

-   **This has a really nice interpretation!**

## Laplace distribution

Suppse a variable $Y_i$ follows a Laplace (or double exponential) distribution, then the pdf is given by,

$$f(Y_i | \mu, \sigma) = \frac{1}{2\sigma} \exp\left\{-\frac{|Y_i - \mu|}{\sigma}\right\}$$

-   $\mathbb{E}[Y_i] = \mu$

-   $\mathbb{V}(Y_i) = 2 \sigma^2$

-   Under the Laplace likelihood, estimation of $\mu$ is equivalent to estimating the population median of $Y_i$.

## Median regression using Laplace

Least absolute deviation (LAD) regression minimizes the following objective function,

$$\hat{{\alpha}}_{\text{LAD}},\hat{\boldsymbol{\beta}}_{\text{LAD}} = \arg \min_{\alpha,\boldsymbol{\beta}} \sum_{i=1}^n |Y_i - \mu_i|, \quad \mu_i = \alpha + \mathbf{x}_i\boldsymbol{\beta}.$$

The Bayesian analog is the Laplace distribution,

$$f(\mathbf{Y} | \alpha, \boldsymbol{\beta}, \sigma) = \left(\frac{1}{2\sigma}\right)^n \exp\left\{-\sum_{i=1}^n\frac{|Y_i - \mu_i|}{\sigma}\right\}.$$

## Median regression using Laplace

-   The Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is $|xâˆ’\mu|$, so minimizing the likelihood will also minimize the least absolute distances.

-   Laplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around $\mu$ with scale $\sigma$).

-   Thus, a linear regression with Laplace errors is analogous to a median regression,

-   Why is median regression considered more robust than regression of the mean?

## Laplace regression in Stan {.midi}

```{stan output.var = "covariates", eval = FALSE}
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
}
model {
  target += double_exponential_lpdf(Y | alpha + X * beta, sigma);
}
```

## Laplace regression in Stan: mixture {.midi}

```{stan output.var = "covariates", eval = FALSE}
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
  vector[n] lambda;
}
transformed parameters {
  vector[n] tau = sigma * sqrt(lambda);
}
model {
  target += normal_lpdf(Y | alpha + X * beta, tau);
  target += exponential_lpdf(lambda | 0.5);
}
```

## Returning to IgG

```{r, echo = FALSE, eval = FALSE}
library(rstan)
laplace_code <- "
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
}
model {
  target += double_exponential_lpdf(Y | alpha + X * beta, sigma);
}
generated quantities {
  vector[n] mu = alpha + X * beta;
  vector[n] Y_pred;
  vector[n] log_lik;
  for (i in 1:n) {
    Y_pred[i] = double_exponential_rng(mu[i], sigma);
    log_lik[i] = double_exponential_lpdf(Y[i] | mu[i], sigma);
  }
}
"
t_code <- "
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
  real<lower = 0> nu;
}
model {
  target += student_t_lpdf(Y | nu, alpha + X * beta, sigma);
  target += exponential_lpdf(nu | 0.5);
}
generated quantities {
  vector[n] mu = alpha + X * beta;
  vector[n] Y_pred;
  vector[n] log_lik;
  for (i in 1:n) {
    Y_pred[i] = student_t_rng(nu, mu[i], sigma);
    log_lik[i] = student_t_lpdf(Y[i] | nu, mu[i], sigma);
  }
}
"
normal_code <- "
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  real<lower = 0> sigma;
}
model {
  target += normal_lpdf(Y | alpha + X * beta, sigma);
}
generated quantities {
  vector[n] mu = alpha + X * beta;
  vector[n] Y_pred;
  vector[n] log_lik;
  for (i in 1:n) {
    Y_pred[i] = normal_rng(mu[i], sigma);
    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);
  }
}
"
normal_var_code <- "
data {
  int<lower = 1> n;
  int<lower = 1> p;
  vector[n] Y;
  matrix[n, p] X;
}
parameters {
  real alpha;
  vector[p] beta;
  vector[p] gamma;
}
transformed parameters {
  vector<lower = 0>[n] tau;
  tau = exp(X * gamma);
}
model {
  target += normal_lpdf(Y | alpha + X * beta, tau);
}
generated quantities {
  vector[n] mu = alpha + X * beta;
  vector[n] Y_pred;
  vector[n] log_lik;
  for (i in 1:n) {
    Y_pred[i] = normal_rng(mu[i], tau[i]);
    log_lik[i] = normal_lpdf(Y[i] | mu[i], tau[i]);
  }
}
"
X <- cbind(ImmunogG$Age)
stan_data <- list(
  n = nrow(ImmunogG),
  p = ncol(X),
  Y = ImmunogG$IgG,
  X = X
)
laplace_reg <- stan_model(model_code = laplace_code)
fit_laplace <- sampling(laplace_reg, stan_data)
t_reg <- stan_model(model_code = t_code)
fit_t <- sampling(t_reg, stan_data)
normal_reg <- stan_model(model_code = normal_code)
fit_normal <- sampling(normal_reg, stan_data)
normal_var_reg <- stan_model(model_code = normal_var_code)
fit_normal_var <- sampling(normal_var_reg, stan_data)
saveRDS(fit_laplace, file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_laplace.rds")
saveRDS(fit_t, file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_t.rds")
saveRDS(fit_normal, file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_normal.rds")
saveRDS(fit_normal_var, file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_normal_var.rds")
```

```{r, echo = FALSE}
fit_laplace <- readRDS(file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_laplace.rds")
fit_t <- readRDS(file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_t.rds")
fit_normal <- readRDS(file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_normal.rds")
fit_normal_var <- readRDS(file = "/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/fit_normal_var.rds")
laplace_mu <- rstan::extract(fit_laplace, pars = "Y_pred")$Y_pred
t_mu <- rstan::extract(fit_t, pars = "Y_pred")$Y_pred
normal_mu <- rstan::extract(fit_normal, pars = "Y_pred")$Y_pred
normal_var_mu <- rstan::extract(fit_normal_var, pars = "Y_pred")$Y_pred

post_mean_laplace <- apply(laplace_mu, 2, mean)
post_lower_laplace <- apply(laplace_mu, 2, function(x) quantile(x, probs = c(0.025)))
post_upper_laplace <- apply(laplace_mu, 2, function(x) quantile(x, probs = c(0.975)))
post_mean_t <- apply(t_mu, 2, mean)
post_lower_t <- apply(t_mu, 2, function(x) quantile(x, probs = c(0.025)))
post_upper_t <- apply(t_mu, 2, function(x) quantile(x, probs = c(0.975)))
post_mean_normal <- apply(normal_mu, 2, mean)
post_lower_normal <- apply(normal_mu, 2, function(x) quantile(x, probs = c(0.025)))
post_upper_normal <- apply(normal_mu, 2, function(x) quantile(x, probs = c(0.975)))
post_mean_normal_var <- apply(normal_var_mu, 2, mean)
post_lower_normal_var <- apply(normal_var_mu, 2, function(x) quantile(x, probs = c(0.025)))
post_upper_normal_var <- apply(normal_var_mu, 2, function(x) quantile(x, probs = c(0.975)))
```

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 11
#| fig-height: 6
#| layout-ncol: 1
dat_fig <- data.frame(X = ImmunogG$Age, Y = ImmunogG$IgG)
dat_lines <- data.frame(
  Model = rep(c("Laplace", "Student-t", "Gaussiain", "Gaussian with Covariates in Variance"), each = nrow(ImmunogG)),
  X = rep(ImmunogG$Age, 4),
  Y = rep(ImmunogG$IgG, 4),
  Mean = c(post_mean_laplace, post_mean_t, post_mean_normal, post_mean_normal_var),
  Lower = c(post_lower_laplace, post_lower_t, post_lower_normal, post_lower_normal_var),
  Upper = c(post_upper_laplace, post_upper_t, post_upper_normal, post_upper_normal_var)
)
dat_lines$Model <- as.factor(dat_lines$Model)

ggplot(dat_lines, aes(x = X, y = Mean)) + 
  geom_point(aes(x = X, y = Y), color = "black") + 
  geom_line() + 
  scale_x_continuous(name = "Age of children in years") + 
  scale_y_continuous(name = "Immunoglobulin G (g/liter)") + 
  geom_ribbon(aes(ymin = Lower, ymax = Upper), alpha = 0.3, color = NA) + 
  facet_grid(. ~ Model) + 
  labs(subtitle = "Posterior mean (95% credible interval) estimate for each model")
```

<!-- ## Scale of the residuals -->

<!-- ```{r} -->

<!-- pred_laplace <- apply(rstan::extract(fit_laplace, pars = "Y_pred")$Y_pred, 2, mean) -->

<!-- pred_t <- apply(rstan::extract(fit_t, pars = "Y_pred")$Y_pred, 2, mean) -->

<!-- pred_normal <- apply(rstan::extract(fit_normal, pars = "Y_pred")$Y_pred, 2, mean) -->

<!-- pred_normal_var <- apply(rstan::extract(fit_normal_var, pars = "Y_pred")$Y_pred, 2, mean) -->

<!-- mean((ImmunogG$IgG - pred_laplace)^2) -->

<!-- mean((ImmunogG$IgG - pred_t)^2) -->

<!-- mean((ImmunogG$IgG - pred_normal)^2) -->

<!-- mean((ImmunogG$IgG - pred_normal_var)^2) -->

<!-- median(abs(ImmunogG$IgG - pred_laplace)) -->

<!-- median(abs(ImmunogG$IgG - pred_t)) -->

<!-- median(abs(ImmunogG$IgG - pred_normal)) -->

<!-- median(abs(ImmunogG$IgG - pred_normal_var)) -->

<!-- ``` -->

## Posterior of $\beta$

```{r, echo = FALSE}
summaries <- function(x) {
  c(
    apply(x, 2, mean),
    apply(x, 2, function(x) quantile(x, probs = 0.025)),
    apply(x, 2, function(x) quantile(x, probs = 0.975))
  )
}
beta_laplace <- summaries(rstan::extract(fit_laplace, pars = "beta")$beta)
beta_t <- summaries(rstan::extract(fit_t, pars = "beta")$beta)
beta_normal <- summaries(rstan::extract(fit_normal, pars = "beta")$beta)
beta_normal_var <- summaries(rstan::extract(fit_normal_var, pars = "beta")$beta)

betas <- data.frame(quantile = c("Laplace", "Student-t", "Gaussian", "Gaussian with Covariates in Variance"), 
                    data.frame(rbind(beta_laplace, beta_t, beta_normal, beta_normal_var)))
rownames(betas) <- NULL
colnames(betas) <- c("Model", "Mean", "Lower", "Upper")
kable(betas, digits = 2)
```

## Model Comparison

```{r, echo = FALSE}
library(loo)
get_waic <- function(fit) {
  log_lik <- extract_log_lik(fit)
  waic <- loo::loo(log_lik)
}
waic_laplace <- get_waic(fit_laplace)
waic_t <- get_waic(fit_t)
waic_normal <- get_waic(fit_normal)
waic_normal_var <- get_waic(fit_normal_var)
comparison <- loo_compare(list("Laplace" = waic_laplace,
                 "Student-t" = waic_t,
                 "Gaussian" = waic_normal,
                 "Gaussian with Covariates in Variance" = waic_normal_var))
comparison[, c(1, 3, 7)] |>
  kable(digits = 2)
```

## Examining the Student-t model fit

```{r, echo = FALSE}
print(fit_t, pars = c("alpha", "beta", "sigma", "nu"), probs = c(0.025, 0.975))
```

## Examining the Student-t model fit

```{r, echo = FALSE}
bayesplot::mcmc_hist(fit_t, regex_pars = c("alpha", "beta", "sigma", "nu"))
```

## Examining the Student-t model fit

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 10
#| fig-height: 5
#| layout-ncol: 1
library(pscl)
x <- seq(0.001, 10, length.out = 1001)
dens1 <- dexp(x, rate = 0.5)
nu <- mean(rstan::extract(fit_t, pars = "nu")$nu)
dens2 <- densigamma(x, 0.5 * nu, 0.5 * nu)
dat.fig <- data.frame(
  x = rep(x, 2),
  y = c(dens1, dens2),
  Model = rep(c("Laplace", "Student-t"), each = length(x))
)
ggplot(dat.fig, aes(x = x, y = y, color = Model)) + 
  geom_line(lwd = 1.25) + 
  labs(x = "X", y = "Density", subtitle = expression(paste("Distributions of ", lambda[i])))
```

## Summary of robust regression

-   Robust regression techniques can be used when the assumptions of constant variance and/or normality of the residuals do not hold.

-   Heteroskedastic variance can viewed as inducing extreme value distributions.

-   Extreme value regression using Student-t and Laplace distributions are robust to outliers.

-   Laplace regression is equivalent to median regression.

## Prepare for next class

-   Work on [HW 02](https://biostat725-sp25.netlify.app/hw/hw-02), which is due before next class.

-   Complete reading to prepare for next Thursday's lecture

-   Thursday's lecture: Regularization
