---
title: "Missing Data"
author: "Prof. Sam Berchuck"
date: "2025-02-25"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
    html-math-method: mathjax
filters:
  - parse-latex
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r, echo=FALSE}
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(knitr)
library(mvtnorm)
library(coda)
library(lars)
library(LaplacesDemon)
library(nimble)
library(mice)
```

## Review of last lecture

-   Last, we learned about classification for binary and multiclass problems.

## Missing data in research

- In any real-world dataset, missing values are nearly always going to be present.

  - Missing data can be a source of bias.

  - Missing data be totally innocuous. 

- Cannot determine which because there are no values to inspect.

- Handling missing data is extremely difficult!

## Examples

- Suppose an individual's depression scores are missing in dataset of patients with colon cancer.

- It could be missing because:

  -   A data entry error where some values did not make it into the dataset.

  -   The patient is a man, and men are less likely to complete the depression score in general (i.e., it is not related to the depression itself).

  -   The patient has depression, leading to them not completing the depression survey. 

## Classifactions of missing data

- Missing completely at random (MCAR)

  - This is the ideal case but rarely seen in practice. Usually a data entry problem.

- Missing at random (MAR)

  - The missing value is related to some other variable that has been collected.

- Missing not at random (MNAR)

  - The missing value is related to a variable that was
not collected or not observed.

## Missing data

- Missing data can appear in the outcome and/or predictors.

- Today, we will write down some math for missing data occuring in the outcome space, however this is generalizable to missingness in the predictor space.

## Missing data framework {.midi}

- We are interested in modeling a random variable $Y_{i}$, for $i \in \{1,\ldots,n\}$.

- In a missing data setting, we only observe the outcome in subset of observations, $\mathbf{Y}_{obs} = \{Y_{i}:i \in \mathcal N_{obs}\}$. 

  - $\mathcal N_{obs}$ is the set of indeces in the observed set, such that $|\mathcal N_{obs}|= n_{obs}$ is the number of observed data points.

- The remaining observations are assumed to be missing and are contained in $\mathbf{Y}_{mis} = \{Y_{i}:i \in \mathcal N_{mis}\}$. 

  - $\mathcal N_{mis}$ is the set of indeces of the missing data and $|\mathcal N_{mis}|= n_{mis}$ is the number of missing data points. 

- The full set of data is given by $\mathbf{Y}=(\mathbf{Y}_{obs},\mathbf{Y}_{mis})$.

## Missing data notation

- Define $O_{i}$ as a binary indicator of observation $Y_{i}$ being present, where $O_{i} = 1$ indicates that $Y_{i}$ was observed. 

- The collection of missingness indicators is given by $\mathbf{O} = \{O_{i}:i = 1,\ldots,n\}$. 

- Our observed data then consists of $(\mathbf{Y}_{obs}, \mathbf{O})$.

## Complete data likelihood

- The joint distribution of $(\mathbf{Y}, \mathbf{O})$ can be written as,

$$f(\mathbf{Y}, \mathbf{O} | \mathbf{X}, \boldsymbol{\theta},\boldsymbol{\phi}) = \underbrace{f(\mathbf{Y} | \mathbf{X}, \boldsymbol{\theta})}_{\text{likelihood}} \times  \underbrace{f(\mathbf{O} | \mathbf{Y}, \mathbf{X}, \boldsymbol{\phi})}_{\text{missing model}}.$$

  - The parameter block, $(\boldsymbol{\theta},\boldsymbol{\phi})$, consists of: 
  
    - $\boldsymbol{\theta}$, the **target parameters** of interest (e.g., feature effects on outcome), and 
    
    - $\boldsymbol{\phi}$, the **nuisance parameters**.
    
. . .

**Can we perform inference using this likelihood?**

## Observed data likelihood 

- The likelihood for the observed data must be written by marginalizing over the unobserved outcome variables.

\begin{align*}
f(\mathbf{Y}_{obs}, \mathbf{O} | \mathbf{X}, \boldsymbol{\theta},\boldsymbol{\phi}) &= \int f(\mathbf{Y}_{obs}, \mathbf{Y}_{mis}, \mathbf{O} | \mathbf{X}, \boldsymbol{\theta},\boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&= \int f(\mathbf{Y}, \mathbf{O} |\mathbf{X}, \boldsymbol{\theta},\boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&= \int \underbrace{f(\mathbf{Y} |\mathbf{X}, \boldsymbol{\theta}) f(\mathbf{O} | \mathbf{X},\mathbf{Y}, \boldsymbol{\phi})}_{\text{complete data likelihood}} d\mathbf{Y}_{mis}
\end{align*}

## Missing data models: MCAR

The data are missing completely at random (MCAR) if the missing mechanism is defined as,
\begin{align*}
f(\mathbf{O} | \mathbf{Y},\mathbf{X},\boldsymbol{\phi}) &= f(\mathbf{O} | \mathbf{Y}_{obs}, \mathbf{Y}_{mis},\mathbf{X},\boldsymbol{\phi})\\
&= f(\mathbf{O} | \boldsymbol{\phi}).
\end{align*}

- The missingness does not depend on any data.

## Implications of the missing model: MCAR {.midi}

\begin{align*}
f(\mathbf{Y}_{obs}, \mathbf{O} |\mathbf{X}, \boldsymbol{\theta},\boldsymbol{\phi}) &= \int f(\mathbf{Y} | \mathbf{X},\boldsymbol{\theta}) f(\mathbf{O} | \mathbf{Y}, \mathbf{X},\boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&= \int f(\mathbf{Y} | \mathbf{X},\boldsymbol{\theta}) f(\mathbf{O} | \boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&=  f(\mathbf{O} | \boldsymbol{\phi}) \int f(\mathbf{Y}_{obs} |\mathbf{X}_{obs}, \boldsymbol{\theta}) f(\mathbf{Y}_{mis} |\mathbf{X}_{mis}, \boldsymbol{\theta})d\mathbf{Y}_{mis}\\
&=  f(\mathbf{Y}_{obs} | \mathbf{X}_{obs},\boldsymbol{\theta})f(\mathbf{O} | \boldsymbol{\phi})\int  f(\mathbf{Y}_{mis} |\mathbf{X}_{mis}, \boldsymbol{\theta}) d\mathbf{Y}_{mis}\\
&=  f(\mathbf{Y}_{obs} | \mathbf{X}_{obs},\boldsymbol{\theta})f(\mathbf{O} | \boldsymbol{\phi}).
\end{align*}

## Key points about MCAR assumption

- **No bias:** The analysis based on the observed data will not be biased, as the missingness does not systematically favor any particular pattern in the data. 

- **Reduced power:** While unbiased, MCAR still reduces the statistical power of the analysis due to the smaller sample size resulting from missing data.
  
- **Simple handling methods:** Because of its random nature, MCAR allows for straightforward handling methods like ***listwise deletion*** (i.e., complete-case analysis) or simple imputation techniques (e.g., mean imputation) without introducing bias. 

## Missing data models: MAR

The data are missing at random (MAR) if the missing mechanism is defined as,
\begin{align*}
f(\mathbf{O} | \mathbf{Y},\mathbf{X},\boldsymbol{\phi}) &= f(\mathbf{O} | \mathbf{Y}_{obs}, \mathbf{Y}_{mis},\mathbf{X},\boldsymbol{\phi})\\
&= f(\mathbf{O} | \mathbf{Y}_{obs},\mathbf{X},\boldsymbol{\phi}).
\end{align*}

- The missingness depends on the observed data only.

## Implications of the missing model: MAR {.midi}

\begin{align*}
f(\mathbf{Y}_{obs}, \mathbf{O} |\mathbf{X}, \boldsymbol{\theta},\boldsymbol{\phi}) &= \int f(\mathbf{Y} |\mathbf{X}, \boldsymbol{\theta}) f(\mathbf{O} | \mathbf{Y},\mathbf{X}, \boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&\hspace{-2in}= \int f(\mathbf{Y} | \mathbf{X},\boldsymbol{\theta}) f(\mathbf{O} | \mathbf{Y}_{obs},\mathbf{X}, \boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&\hspace{-2in}=  f(\mathbf{O} | \mathbf{Y}_{obs},\mathbf{X},\boldsymbol{\phi}) \int f(\mathbf{Y}_{obs} | \mathbf{X}_{obs},\boldsymbol{\theta}) f(\mathbf{Y}_{mis} | \mathbf{X}_{mis},\boldsymbol{\theta})d\mathbf{Y}_{mis}\\
&\hspace{-2in}=  f(\mathbf{Y}_{obs} |\mathbf{X}_{obs}, \boldsymbol{\theta})f(\mathbf{O} |\mathbf{Y}_{obs},\mathbf{X}, \boldsymbol{\phi})\int  f(\mathbf{Y}_{mis} | \mathbf{X}_{mis},\boldsymbol{\theta}) d\mathbf{Y}_{mis}\\
&\hspace{-2in}=  f(\mathbf{Y}_{obs} | \mathbf{X}_{obs},\boldsymbol{\theta})f(\mathbf{O} |\mathbf{Y}_{obs}, \mathbf{X},\boldsymbol{\phi}).
\end{align*}

- **Unbiased Parameter Estimates:** Similar to MCAR, we can perform a complete case analysis and can ignore the missing data model! This is never done, however, because it leads to incorrect inference.

## Key points about MAR assumption

- **Complete-case analysis is not acceptable: **

  - Parameter estimation remains unbiased, but estimation of variances and intervals does not.
  
  - Also, smaller sample size leads to less power and worse prediction.
  
  - Under certain missingness settings, parameter estimation may not be unbiased. 

- **Simple handling approaches fail:** Methods like mean imputation will also result in small estimated standard errors.

- **More advanced methods are needed:** Multiple imputation, Bayes.

## Missing data models: MNAR

The data are missing not at random (MNAR) if the missing mechanism is defined as,
\begin{align*}
f(\mathbf{O} | \mathbf{Y},\mathbf{X},\boldsymbol{\phi}) &= f(\mathbf{O} | \mathbf{Y}_{obs}, \mathbf{Y}_{mis},\mathbf{X},\boldsymbol{\phi}).
\end{align*}

- The missingness depends on the observed and missing data.

## Implications of the missing model: MNAR {.midi}

\begin{align*}
f(\mathbf{Y}_{obs}, \mathbf{O} | \mathbf{X},\boldsymbol{\theta},\boldsymbol{\phi}) &= \int f(\mathbf{Y} | \mathbf{X},\boldsymbol{\theta}) f(\mathbf{O} | \mathbf{Y},\mathbf{X}, \boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&\hspace{-2in}= \int f(\mathbf{Y} | \mathbf{X},\boldsymbol{\theta}) f(\mathbf{O} | \mathbf{Y}_{obs},\mathbf{Y}_{mis}, \mathbf{X},\boldsymbol{\phi}) d\mathbf{Y}_{mis}\\
&\hspace{-2in}= \int f(\mathbf{Y}_{obs} |\mathbf{X}_{obs}, \boldsymbol{\theta}) f(\mathbf{Y}_{mis} | \mathbf{X}_{mis},\boldsymbol{\theta})f(\mathbf{O} | \mathbf{Y},\mathbf{X},\boldsymbol{\phi})d\mathbf{Y}_{mis}\\
&\hspace{-2in}= f(\mathbf{Y}_{obs} | \mathbf{X}_{obs},\boldsymbol{\theta}) \int f(\mathbf{Y}_{mis} | \mathbf{X}_{mis},\boldsymbol{\theta})f(\mathbf{O} | \mathbf{Y},\mathbf{X},\boldsymbol{\phi})d\mathbf{Y}_{mis}
\end{align*}

- Under the MNAR assumption, we are NOT allowed to ignore the missing data. We must specify a model for the missing data.

- This is really hard! We can ignore this in our class.

## Summary of missing mechanisms

- Under MCAR and MAR, we are allowed to fit our model to the observed data (i.e., a complete case analysis/listwise deletion). Under these settings the missingness is considered **ignorable**.

- Under MAR, fitting the complete case analysis is not efficient and advanced techniques are needed to guarentee proper statistical inference.

- Under MNAR, we must model the missing data mechanism. This data is considered **non-ignorable**.

## Why Bayesian missing data

Unbiased Parameter Estimates: Multiple imputation helps to ensure that the parameter estimates (e.g., regression coefficients, group means) are unbiased. By creating multiple imputed datasets and combining the results, we can obtain more accurate estimates that reflect the uncertainty due to missing data1.

Accurate Standard Errors: Multiple imputation provides accurate standard errors, which leads to reliable p-values and appropriate statistical inferences. This is crucial for making valid conclusions from the data1.

Flexibility: Multiple imputation is flexible and can handle various types of missing data patterns. It allows for the inclusion of covariates that explain the missingness, which is important for MAR data2.

Handling Complex Data Structures: Multiple imputation can be used with complex data structures, such as hierarchical or longitudinal data, where the missingness mechanism may vary across different levels or time points2.

Robustness to Model Misspecification: Multiple imputation is robust to certain types of model misspecification. For example, even if the imputation model is not perfectly specified, the combined results from multiple imputed datasets can still provide valid inferences3.


## Generating data

- We want to have data under the following criteria:

  1. Full data
  
  2. MCAR data
  
  3. MAR data
  
  4. MNAR data
  
- We then want to fit the following models: Complete case analysis, full Bayesian model, mice version. 

## Next

There are many ways to handle missing data. The simplest approach is to perform a complete case analysis, where we delete entries with a missing value for at least one variable. If the data are missing completely at random (MCAR), then we get unbiased parameter estimate. However:

Credible intervals will usually be too wide, since the sample size has effectively been reduced (drastically).
If missing values occur for other reasons, this yields biased estimates.
The next step up is instead of dropping all the missing entries, we replace missing values of a feature with the mean or median of available values for the feature. This is called single mean/median imputation, and is often recommended in many machine learning tasks if the data are MCAR as it can still lead to unbiased parameter estimates.

There are also some drawbacks with this method, especially if youâ€™re interested in studying the variability. Single mean/median imputation artificially reduces the variance of features, which results in credible intervals which are too narrow. As shown in Figure 2, the peak at the mean is much higher relative to the surrounding values. The mean of the distribution can still be estimated appropriately, but the amount of spread is reduced.

Another drawback is it doesnâ€™t account for relationship between variables, thus reduces correlation. For example, the correlation between the two variables is 0.665 if we only look at the complete cases. If we use single mean imputation, the correlation decreases to 0.589.

Note that if you donâ€™t care about assessing variability in your estimates, which is often the case for classification tasks, then mean imputation can work quite well.


## Let's look at some data

```{r, echo = FALSE}
library(openintro)
fulldata <- data.frame(y = bdims$wgt,
                       x = bdims$hgt)
```
```{r, eval = FALSE}
library(openintro)
fulldata <- data.frame(y = bdims$wgt,
                       x = bdims$hgt)
```
```{r}
#| echo: false
#| fig-align: "center"
#| fig-height: 4
#| layout-ncol: 1
ggplot(fulldata, aes(x = x, y = y)) +
  geom_point() + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)")
```

## Simulate missing data

```{r}
library(mice)
set.seed(54)
n <- nrow(fulldata)

###Simulate MCAR data
O <- rbinom(n, 1, prob = 0.40)
mcardata <- data.frame(
  o = O,
  x = fulldata$x,
  y = fulldata$y
)
mcardata_cc <- mcardata[mcardata$o == 1, ]

###Simulate MAR data
O <- rbinom(n, 1, prob = pnorm(scale(fulldata$x)))
mardata <- data.frame(
  o = O,
  x = fulldata$x,
  y = fulldata$y
)
mardata_cc <- mardata[mardata$o == 1, ]

###Simulate MNAR data
O <- rbinom(n, 1, prob = pnorm(scale(fulldata$y)))
mnardata <- data.frame(
  o = O,
  x = fulldata$x,
  y = fulldata$y
)
mnardata_cc <- mnardata[mnardata$o == 1, ]
```

## Visualize data

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 4
#| fig-height: 2.5
#| layout-ncol: 2
ggplot(fulldata, aes(x = x, y = y)) +
  geom_point() + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)",
       subtitle = "Full data") + 
  theme(plot.margin = margin(t = 0,  # Top margin
                             r = 0,  # Right margin
                             b = 0,  # Bottom margin
                             l = 0))
ggplot(fulldata, aes(x = x, y = y)) +
  geom_point(color = "gray") +
  geom_point(data = mcardata_cc, aes(x = x, y = y)) + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)",
       subtitle = "MCAR") + 
    theme(plot.margin = margin(t = 0,  # Top margin
                             r = 0,  # Right margin
                             b = 0,  # Bottom margin
                             l = 0))
ggplot(fulldata, aes(x = x, y = y)) +
  geom_point(color = "gray") +
  geom_point(data = mardata_cc, aes(x = x, y = y)) + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)",
       subtitle = "MAR") + 
    theme(plot.margin = margin(t = 0,  # Top margin
                             r = 0,  # Right margin
                             b = 0,  # Bottom margin
                             l = 0))
ggplot(fulldata, aes(x = x, y = y)) +
  geom_point(color = "gray") +
  geom_point(data = mnardata_cc, aes(x = x, y = y)) + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)",
       subtitle = "MNAR") + 
    theme(plot.margin = margin(t = 0,  # Top margin
                             r = 0,  # Right margin
                             b = 0,  # Bottom margin
                             l = 0))
```

## Model fits

```{r}
#| echo: false
#| fig-align: "center"
#| fig-width: 4
#| fig-height: 2.5
#| layout-ncol: 2
reg1 <- lm(y ~ x, data = fulldata)
reg2 <- lm(y ~ x, data = mcardata_cc)
reg3 <- lm(y ~ x, data = mardata_cc)
reg4 <- lm(y ~ x, data = mnardata_cc)
regs <- list(reg1, reg2, reg3, reg4)
round2 <- function(x) format(round(x, 2), nsmall = 2)
labels <- c("Full data", "MCAR", "MAR", "MNAR")
datas <- list(fulldata, mcardata, mardata, mnardata)
for (i in 1:4) {
  reg <- regs[[i]]
  data <- datas[[i]]
  if (i > 1) {
    y_bar_mis <- mean(fulldata[which(datas[[i]][, 1] == 0), "y"])
    y_bar_obs <- mean(fulldata[which(datas[[i]][, 1] == 1), "y"])
  }
  if (i == 1) y_bar_obs <- mean(fulldata[, "y"])
  slope <- summary(reg)$coef[2, 1]
  x <- seq(min(fulldata$x), max(fulldata$x), length.out = 1001)
  dat_curve <- data.frame(x = x,
                          y = predict(reg, newdata = data.frame(x = x)))
    if (i == 1) {
      p <- ggplot(fulldata, aes(x = x, y = y)) + 
        geom_point(color = "gray") + 
        geom_point(data = data, mapping = aes(x = x, y = y)) + 
        # geom_line(data = dat_curve, aes(x = x, y = y), color = "blue", lwd = 1.5) + 
        geom_smooth(method = "lm", formula = y ~ x, se = TRUE, fullrange = TRUE, lwd = 1.5) +
        xlim(min(fulldata$x), max(fulldata$x)) + 
        ylim(min(fulldata$y), max(fulldata$y)) + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)",
             subtitle = labels[i]) + 
        annotate("text", x = 148, y = 113, label = as.expression(bquote(bar(Y) == ~ .(round2(y_bar_obs)))), hjust = 0, vjust = 0) + 
        annotate("text", x = 148, y = 103, label = as.expression(bquote("Slope" == ~ .(round2(slope)))), hjust = 0, vjust = 0) + 
    theme(plot.margin = margin(t = 0,  # Top margin
                             r = 0,  # Right margin
                             b = 0,  # Bottom margin
                             l = 0))
    }
    if (i > 1) {
      data <- data[data$o == 1, ]
      p <- ggplot(fulldata, aes(x = x, y = y)) + 
        geom_point(color = "gray") + 
        geom_point(data = data, mapping = aes(x = x, y = y)) + 
        # geom_line(data = dat_curve, aes(x = x, y = y), color = "blue", lwd = 1.5) + 
        geom_smooth(method = "lm", formula = y ~ x, se = TRUE, fullrange = TRUE, lwd = 1.5) +
        xlim(min(fulldata$x), max(fulldata$x)) + 
        ylim(min(fulldata$y), max(fulldata$y)) + 
  labs(x = "Height (centimeters)", 
       y = "Weight (kilograms)",
             subtitle = labels[i]) + 
        annotate("text", x = 148, y = 111, label = as.expression(bquote(bar(Y)[obs] == ~ .(round2(y_bar_obs)))), hjust = 0, vjust = 0) + 
        annotate("text", x = 148, y = 101, label = as.expression(bquote(bar(Y)[mis] == ~ .(round2(y_bar_mis)))), hjust = 0, vjust = 0) + 
        annotate("text", x = 148, y = 91, label = as.expression(bquote("Slope" == ~ .(round2(slope)))), hjust = 0, vjust = 0) + 
    theme(plot.margin = margin(t = 0,  # Top margin
                             r = 0,  # Right margin
                             b = 0,  # Bottom margin
                             l = 0))
    }
  print(p)
}

```

## Complete-case model

\begin{align*}
Y_i &\stackrel{ind}{\sim} N(\alpha + X_i \beta, \sigma^2), \quad i \in \mathcal N_{obs}\\
\alpha &\sim N(0,10)\\
\beta &\sim N(0,10)\\
\sigma &\sim \text{Half-Normal}(0, 10)
\end{align*}

## Full Bayesian model

\begin{align*}
Y_i &\stackrel{ind}{\sim} N(\alpha + X_i \beta, \sigma^2), \quad i \in \mathcal N_{obs}\\
Y_i &\stackrel{ind}{\sim} N(\alpha + X_i \beta, \sigma^2), \quad i \in \mathcal N_{mis}\\
\alpha &\sim N(0,10)\\
\beta &\sim N(0,10)\\
\sigma &\sim \text{Half-Normal}(0, 10)
\end{align*}


## Missing pattern

```{r}
md.pattern(mcardata_cc, rotate.names = TRUE)
```

## Prepare for next class

-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).

-   Complete reading to prepare for next Tuesday's lecture

-   Tuesday's lecture: Missing data
