{
  "hash": "590ad1c89017ef043d4a400296ba2bbd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"AE 10: Bayesian Clustering\"\nsubtitle: \"Clustering patients based on their ED length of stay\"\ndate: \"April 8, 2025\"\n---\n\n\n\n::: callout-important\n## Due date\n\nApplication exercises (AEs) are submitted by pushing your work to the relevant GitHub repo. AEs from Tuesday lectures should be submitted by Friday by 11:59pm ET, and AEs from Thursday lectures should be submitted by Sunday at 11:59pm ET. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\n-   Final `.qmd` and `.pdf` files pushed to your GitHub repo\n-   **Note:** For homeworks and exams, you will also be required to submit your final `.pdf` file submitted on Gradescope\n:::\n\n# Getting Started\n\n## Clone the repo & start new RStudio project\n\n-   Go to the course organization at [github.com/biostat725-sp25](https://github.com/biostat725-sp25) organization on GitHub.\n-   Click on the repo with the prefix **ae-10-**. It contains the starter documents you need to complete the AE.\n-   Click on the green **CODE** button, select **Use SSH** (this might already be selected by default, and if it is, you'll see the text **Clone with SSH**). Click on the clipboard icon to copy the repo URL.\n    -   See the [HW 00 instructions](https://biostat725-sp25.netlify.app/hw/hw-00#connect-rstudio-and-github) if you have not set up the SSH key or configured git.\n-   In RStudio, go to *File* $\\rightarrow$ *New Project* $\\rightarrow$ *Version Control* $\\rightarrow$ *Git*.\n-   Copy and paste the URL of your assignment repo into the dialog box *Repository URL*. Again, please make sure to have *SSH* highlighted under *Clone* when you copy the address.\n-   Click *Create Project*, and the files from your GitHub repo will be displayed in the *Files* pane in RStudio.\n-   Click `ae-10.qmd` to open the template Quarto file. This is where you will write up your code and narrative for the AE.\n\n# R packages\n\nWe will begin by loading R packages that we will use in this AE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)     # data wrangling and visualization\nlibrary(knitr)         # format output\nlibrary(rstan)         # Stan\nlibrary(bayesplot)     # figures for post Stan inference\nlibrary(loo)           # model comparison\nlibrary(patchwork)     # combining figures\nlibrary(extraDistr)    # student-t density function  \n\noptions(mc.cores=4)\n```\n:::\n\n\n\n# Data\n\nThis AE will revisit data from the MIMIC-IV-ED Demo. As a reminder, MIMIC-IV-ED is a publicly accessible database of over 400,000 emergency department (ED) admissions to the Beth Israel Deaconess Medical Center between 2011 and 2019. The emergency department (ED) is a high demand environment requiring rapid triaging of patients for further care.\n\nFor the AE, we will use data on patient lengths of stay, `los`. The data are available in your repo (`ed_los.rds`).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ned_los <- readRDS(\"ed_los.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ed_los, aes(x = los)) + \n  geom_histogram() + \n  labs(x = \"ED Length of Stay (hours)\",\n       y = \"Count\") \n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\nNote the multimodality. As described in the lecture, our goal will be to identify subgroups within these data.\n\n# Model Goals\n\nCompare fit from two models\n\n\\begin{align*}\np\\left(Y_i\\mid \\pi, \\mu, \\sigma\\right) &= \\sum_{h=1}^2 \\pi_h N\\left(Y_i; \\mu_h, \\sigma^2_h\\right) \\\\\n\\mu_h &\\sim N\\left(0,10^2\\right) \\\\\n\\sigma_h &\\sim \\text{Exp}\\left(1\\right) \\\\\n\\pi_1\\sim\\text{Unif}(0,1), &\\quad\\quad \\pi_2=1-\\pi_h\n\\end{align*}\n\nand\n\n\\begin{align*}\np\\left(Y_i\\mid \\pi, \\mu, \\sigma\\right) &= \\sum_{h=1}^2 \\pi_h t_{\\nu_h}\\left(Y_i; \\mu_h, \\sigma^2_h\\right) \\\\\n\\mu_h &\\sim N\\left(0,10^2\\right) \\\\\n\\sigma_h &\\sim N_+\\left(2, 0.5^2\\right) \\\\\n\\nu_h &\\sim \\text{Ga}\\left(5, 0.5\\right) \\\\\n\\pi_1\\sim\\text{Unif}(0,1), &\\quad\\quad \\pi_2=1-\\pi_h\n\\end{align*}\n\n## Fitting the Model\n\nBecause clustering is generally an unsupervised task, no covariates are used to fit the model. We will use a centered outcome $Y_i - \\bar{Y}$ to simplify prior specification. In addition to the outcome $Y$, we must specify the number of components (clusters) $k$ used to fit the mixture. The Stan data object is given by\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <- list(Y = (ed_los$los - mean(ed_los$los)),\n                  n = length(ed_los$los),\n                  k = 2)\n```\n:::\n\n\n\nWe will now fit the model and print the posterior summaries and MCMC convergence diagnostics.\n\n**Gaussian components:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmixture_gauss <- stan_model(\"mixture1.stan\")\nfit_mixture_k2g <- sampling(mixture_gauss, data=stan_data, iter=5000, chains=4, control=list(\"adapt_delta\"=0.99))\n\nprint(fit_mixture_k2g, pars = c(\"pi\", \"mu\", \"sigma\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi[1]     0.37    0.17 0.24  0.16  0.21  0.25  0.47  0.83     2 6.35\npi[2]     0.63    0.17 0.24  0.17  0.53  0.75  0.79  0.84     2 6.35\nmu[1]    -3.17    0.09 0.51 -4.49 -3.37 -3.07 -2.86 -2.49    33 1.05\nmu[2]     0.45    3.96 5.69 -3.17 -2.86 -2.67 -0.50 13.08     2 5.14\nsigma[1] 13.25    4.48 6.46  2.09  9.72 16.12 17.47 20.11     2 4.97\nsigma[2]  5.03    3.28 4.68  2.03  2.27  2.43  5.04 14.78     2 7.56\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 22 13:54:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\nTraceplots and pair plots of posterior samples reveal bimodality:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstan::traceplot(fit_mixture_k2g, pars = c(\"pi\", \"mu\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(fit_mixture_k2g, pars = c(\"mu\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n*Posterior means are not meaningful in this case*\n\n**Student t components:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmixture_t <- stan_model(\"mixture2.stan\")\nfit_mixture_k2t <- sampling(mixture_t, data=stan_data, iter=5000, chains=4)\nprint(fit_mixture_k2t, pars = c(\"pi\", \"mu\", \"sigma\", \"nu\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi[1]     0.81    0.00 0.04  0.73  0.79  0.81  0.83  0.87  5567    1\npi[2]     0.19    0.00 0.04  0.13  0.17  0.19  0.21  0.27  5567    1\nmu[1]    -2.96    0.00 0.22 -3.39 -3.10 -2.96 -2.81 -2.53  7318    1\nmu[2]     8.30    0.02 1.10  6.03  7.66  8.35  9.01 10.28  4306    1\nsigma[1]  2.20    0.00 0.17  1.88  2.09  2.20  2.31  2.55  5987    1\nsigma[2]  2.82    0.00 0.40  2.06  2.55  2.81  3.09  3.64  6745    1\nnu[1]    11.98    0.04 4.44  5.14  8.75 11.38 14.57 22.22  9860    1\nnu[2]     1.54    0.00 0.43  1.03  1.25  1.46  1.74  2.50  9286    1\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 29 12:52:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nrstan::traceplot(fit_mixture_k2t, pars = c(\"pi\", \"mu\", \"sigma\", \"nu\"))\n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n## Visualizing $\\text{Pr}(z_i=z_j|-)$\n\nFor t mixture:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n# Exercises\n\n## Exercise 1\n\nRevisit the Gaussian mixture model fit above. Diagnostic criteria indicate that the posterior samples collected may not faithfully represent the true posterior. Traceplots and pair plots indicate that different chains explored different regions of the posterior. Discuss the following: Why might the true posterior be multimodal? Is this a problem with the model or with the way we compute the posterior?\n\n**Answer:**\n\nPoints students may consider: - Without the `ordered` constraint, the model is trivially multimodal with modes corresponding to switched labels. - The `ordered` constraint on $\\mu$ is intended to resolve non-identifiability (label switching). If, under the data generating process $\\mu_1\\approx\\mu_2$ however, then the constraint fails to prevent switching between the variances. I.e., ordering $\\mu$ does not resolve label switching in a variance mixture of Gaussians model. - Notice that the chains do not mix over modesâ€”each chain is apparently stuck in one of the two modes. Hence, no single chain is actually exploring the disjoint region of high posterior probability. This is a limitation of the posterior computation method. - If chains did mix over modes, however, summarizing the posterior would be nontrivial.\n\n## Exercise 2\n\nExtend the Gaussian mixture model Stan code to simulate data `Y_pred` under the posterior predictive. Compare the density of predicted data to the empirical density of observed data. Do you observe anything surprising?\n\n**Answer:**\n\nThis is a bit tricky because the posterior probability that large $Y$ comes from the small component is numerically zero.\n\nHopefully students not that many of the marginal posteriors for `Y_pred` are well-behaved in terms of ESS and Rhat. Maybe we can estimate the density well without nailing the parameters. The density overlay is reasonable with obvious problems around the interval $(5,15)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n::: {.cell output.var='ex2'}\n\n```{.stan .cell-code}\ngenerated quantities {\n  array[n] real Y_pred;\n  matrix[n,k] lPrZik;\n  int<lower=1, upper=k> z[n];\n  \n  for (i in 1:n){\n    for (h in 1:k){\n      lPrZik[i,h] = log(pi[h]) + normal_lpdf(Y[i] | mu[h], sigma[h]);\n    }\n    lPrZik[i] -= log(sum(exp(lPrZik[i])));\n    \n    // Numerical zeros due to light tails complicate predictive inference\n    if (is_inf(exp(lPrZik[i,1]))){\n      z[i] = 1;\n      Y_pred[i] = normal_rng(mu[z[i]], sigma[z[i]]);\n      continue;\n    }\n    if (is_inf(exp(lPrZik[i,2]))){\n      z[i] = 2;\n      Y_pred[i] = normal_rng(mu[z[i]], sigma[z[i]]);\n      continue;\n    }\n    \n    z[i] = categorical_rng(exp(lPrZik[i]'));\n    \n    Y_pred[i] = normal_rng(mu[z[i]], sigma[z[i]]);\n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nEx2mod <- stan_model(\"Ex2.stan\")\nEx2fit <- sampling(Ex2mod, data=stan_data, chains=4, iter=4000,\n                   control=list(\"adapt_delta\"=0.99))\n\nEx2samps <- extract(Ex2fit)\nplot(density(Ex2samps$Y_pred[Ex2samps$Y_pred>-10 & Ex2samps$Y_pred<100]))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  geom_density(aes(x=x2), bw=1) + \n  ggtitle(\"Fitted density (blue) + data (black)\")\n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  geom_density(aes(x=x2), bw=1) + \n  xlim(min(stan_data$Y)-1, max(stan_data$Y)+1) +\n  ggtitle(\"Fitted density (blue) + data (black)\", subtitle = \"Truncated to sample range\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 114357 rows containing non-finite outside the scale range\n(`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\n## Exercise 3\n\nExtend the student t mixture model Stan code to simulate data `Y_pred` under the posterior predictive. Compare to the predicted data under the Gaussian mixture model and comment on any similarities/differences.\n\n**Answer:**\n\nProbabilities are better behaved here thanks to the heavy tails. The predictive density has extreme tails (unsurprising given $\\nu_1\\approx 1.5$). The tails are outlandish in context. However, the mode at $(5,15)$ is better modeled.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n::: {.cell output.var='ex3'}\n\n```{.stan .cell-code}\ngenerated quantities {\n  array[n] real Y_pred;\n  matrix[n,k] lPrZik;\n  \n  int<lower=1, upper=k> z[n];\n  for (i in 1:n){\n    for (h in 1:k){\n      lPrZik[i,h] = log(pi[h]) + student_t_lpdf(Y[i] | nu[h], mu[h], sigma[h]);\n    }\n    lPrZik[i] -= log(sum(exp(lPrZik[i])));\n    z[i] = categorical_rng(exp(lPrZik[i]'));\n    \n    Y_pred[i] = student_t_rng(nu[z[i]], mu[z[i]], sigma[z[i]]);\n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nEx3mod <- stan_model(\"Ex3.stan\")\nEx3fit <- sampling(Ex3mod, data=stan_data, chains=4, iter=4000)\n\nEx3samps <- extract(Ex3fit)\nplot(density(Ex3samps$Y_pred[Ex3samps$Y_pred>-10 & Ex3samps$Y_pred<100]))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x1=c(Ex3samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x2), bw=1) +\n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  ggtitle(\"Fitted density (blue) + data (black)\")\n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x1=c(Ex3samps$Y_pred), x2=stan_data$Y)) + \n  geom_density(aes(x=x1), bw=1, color=\"blue\") +\n  geom_density(aes(x=x2), bw=1) + \n  xlim(min(stan_data$Y)-1, max(stan_data$Y)+1) +\n  ggtitle(\"Fitted density (blue) + data (black)\", subtitle = \"Truncated to sample range\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 29799 rows containing non-finite outside the scale range\n(`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=c(Ex3samps$Y_pred))) + \n  geom_density(aes(x=x1), bw=1, color=\"red\") +\n  geom_density(aes(x=x2), bw=1, color=\"blue\") + \n  ggtitle(\"t-mixture density (blue) + Gaussiam mixture (red)\")\n```\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(x1=c(Ex2samps$Y_pred), x2=c(Ex3samps$Y_pred))) + \n  geom_density(aes(x=x1), bw=1, color=\"red\") +\n  geom_density(aes(x=x2), bw=1, color=\"blue\") + \n  ggtitle(\"t-mixture density (blue) + Gaussiam mixture (red)\", subtitle = \"Truncated to sample range\") +\n  xlim(min(stan_data$Y)-1, max(stan_data$Y)+1) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 114357 rows containing non-finite outside the scale range\n(`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 29799 rows containing non-finite outside the scale range\n(`stat_density()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ae-10-clustering_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n::: callout-important\nTo submit the AE:\n\n-   Render the document to produce the PDF with all of your work from today's class.\n-   Push all your work to your AE repo on GitHub. You're done! ðŸŽ‰\n:::\n",
    "supporting": [
      "ae-10-clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}