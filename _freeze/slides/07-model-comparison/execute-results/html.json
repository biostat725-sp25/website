{
  "hash": "3e418f6114417cd7966f6d7d791e17f0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model Comparison\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-01-30\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\nOn Tuesday, we learned about various ways to check MCMC convergence and model fit.\n\n-   Traceplots, effective sample size ($n_{eff}$), MC standard error, $\\hat{R}$, sampling issues\n\n-   Posterior predictive checks\n\n-   Model checks using `shinystan`\n\nToday, we will learn about model comparisons.\n\n## Model comparison\n\n-   In statistical modeling, a more complex model almost always results in a better fit to the data.\n\n    -   A more complex model means one with more parameters.\n\n-   If one has 10 observations, one can have a model with 10 parameters that can perfectly predict every single data point (by just having a parameter to predict each data point).\n\n-   There are two problems with overly complex models.\n\n    1.  They become increasingly hard to interpret (think a straight line versus a polynomial).\n\n    2.  They are more at risk of overfitting, such that it does not work for future observations.\n\n## Model fit: an example data set\n\n-   Let's explore the idea of model fit using an example dataset from the `openintro` package called `bdims`.\n\n-   This dataset contains body girth measurements and skeletal diameter measurements.\n\n-   Today we will explore the association between height and weight.\n\n## Model fit: an example data set\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(openintro)\ndat <- data.frame(weight = bdims$wgt * 2.20462, # convert weight to lbs\n                  height = bdims$hgt * 0.393701, # convert height to inches\n                  sex = ifelse(bdims$sex == 1, \"Male\", \"Female\"))\n```\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Models of increasing complexity {.midi}\n\n-   When using height to predict weight, we can models of increasing complexity using higher order polynomials.\n\n-   Let's fit the following models to the subset of 10 data points:\n\n\\begin{align*}\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3\\\\\n\\mathbb{E}[weight_i] &= \\beta_0 + \\beta_1 height_i + \\beta_2 height_i^2 + \\beta_3 height_i^3 + \\beta_4 height_i^4\n\\end{align*}\n\n-   We can compare these models using standard measures of goodness-of-fit, including $R^2$ and root mean squared error (RMSE).\n\n## Overfitting and underfitting\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=408}\n:::\n\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-4-2.png){fig-align='center' width=408}\n:::\n\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-4-3.png){fig-align='center' width=408}\n:::\n\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-4-4.png){fig-align='center' width=408}\n:::\n:::\n\n\n\n## Overfitting and underfitting\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=408}\n:::\n\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-5-2.png){fig-align='center' width=408}\n:::\n\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-5-3.png){fig-align='center' width=408}\n:::\n\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-5-4.png){fig-align='center' width=408}\n:::\n:::\n\n\n\n## Overfitting and underfitting\n\n-   With more complex models, out-of-sample prediction becomes worse.\n\n-   This is because when you use a complex model in a data set, it tailors the coefficients to any sampling errors and noise in the data such that it will not generalize to new observations.\n\n-   Therefore, our goal in model comparison is to choose a model with the following two properties:\n\n    1.  It is complex enough to capture the essence of the data generation process (and thus avoid underfitting),\n\n    2.  It avoids overfitting to make the model usefull for predicting new observations.\n\n## Finding an optimal model {.midi}\n\n-   Trade-off between overfitting and underfitting (in machine learning this is commonly called bias-variance trade-off).\n\n    -   A simple model tends to produce biased predictions because it does not capture the essence of the data generating process.\n\n    -   A model that is overly complex is unbiased but results in a lot of uncertainty in the prediction.\n\n-   Polynomials are merely one example of comparing simple to complex models. You can think about:\n\n    -   Models with and without interactions,\n\n    -   Models with a few predictors versus hundreds of predictors,\n\n    -   Regression analyses versus hierarchical models, etc.\n\n## Model Comparison {.midi}\n\n-   When comparing models, we prefer models that are closer to the *true* data-generating process.\n\n-   We need some ways to quantify the degree of *closeness* to the true model. Note that in this context models refer to the distributional family as well as the parameter values.\n\n-   For example, the model $Y_i \\sim N(5,2)$ is a different model than $Y_i \\sim N(3,2)$, which is a different model than $Y_i \\sim Gamma(2,2)$.\n\n    -   The first two have the same family but different parameter values (different means, same SD), whereas the last two have different distributional families (Normal vs. Gamma).\n\n-   One way to quantify the degree of *closeness* to the true model is using Kullback-Leibler (KL) divergence.\n\n## Kullback-Leibler divergence {.midi}\n\n-   For two models, $M_0$ and $M_1$, the KL divergence is given by,\n\n\\begin{align*}\nD_{KL}\\left(M_0 | M_1\\right) &= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log\\frac{f_{M_0}(\\mathbf{Y})}{f_{M_1}(\\mathbf{Y})} d\\mathbf{Y}\\\\\n&\\hspace{-1.5in}= \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_0}(\\mathbf{Y})d\\mathbf{Y} - \\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y}\n\\end{align*}\n\n-   Note that $D_{KL}$ is not considered a distance, because it is not strictly symmetric, $D_{KL}\\left(M_0 | M_1\\right) \\neq D_{KL}\\left(M_1 | M_0\\right)$.\n\n## Kullback-Leibler divergence {.midi}\n\nAs an example, assume that the data are generated by a true model $M_0$, and we have two candidate models $M_1$ and $M_2$, where\n\n::::: columns\n::: {.column width=\"40%\"}\n-   $M_0: Y_i \\sim N(3,2)$\n\n-   $M_1: Y_i \\sim N(3.5, 2.5)$\n\n-   $M_2: Y_i \\sim Cauchy(3,2)$\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](07-model-comparison_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n:::::\n\n-   $D_{KL}(M_0 |M_1) = 0.063$, $D_{KL}(M_0 | M_1) = 0.259$, so $M_1$ is a better model than $M_2$.\n\n## Comparing models using KL {.midi}\n\n-   Note that in the expression of $D_{KL}$, when talking about the same target model, the first term is always the same and describes the *true* model, $M_0$.\n\n-   Therefore, it is sufficient to compare models on the second term,\\\n    $$\\int_{-\\infty}^{\\infty} f_{M_0}(\\mathbf{Y}) \\log f_{M_1}(\\mathbf{Y})d\\mathbf{Y},$$ which can also be written as, $\\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right].$\n\n-   This term is the **expected log predictive density (elpd)**.\n\n-   A larger elpd is preferred. Why?\n\n## Comparing models using KL\n\n-   In the real world, we do not know $M_0$.\n\n    -   If we knew, then we would just need to choose $M_0$ as our model and there will be no problem about model comparisons.\n\n    -   Even if we knew the true model, we would still need to estimate the parameter values.\n\n-   Thus, we cannot compute elpd, since the expectation is over $f_{M_0}(\\mathbf{Y})$.\n\n-   We need to estimate elpd!\n\n## Comparing models using KL {.midi}\n\n-   elpd is an expectation, so we can think about estimating it using Monte Carlo sampling, $$\\frac{1}{S}\\sum_{s = 1}^S\\log f_{M_1}\\left(\\mathbf{Y}^{(s)}\\right)\\rightarrow \\mathbb{E}_{M_0}\\left[\\log f_{M_1}(\\mathbf{Y})\\right], \\quad \\mathbf{Y}^{(s)} \\sim f_{M_0}(\\mathbf{Y}).$$\n\n    -   We need to find a way to approximate, $f_{M_0}\\left(\\mathbf{Y}^{(s)}\\right)$.\n\n-   A naive way to approximate $f(\\mathbf{Y}^{(s)})$ is to assume that the distribution of the observed data is the true model.\n\n    -   This is equivalent to assuming that $\\mathbf{Y}^{(s)} \\sim \\{\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n\\}$.\n\n    -   This leads to an overly optimistic estimate and favors complex models.\n\n## Comparing models using KL\n\n-   A better way to estimate elpd is to collect data on a new independent sample that is believed to share the same data generating process as the current sample, and estimate elpd on the new sample.\n\n    -   This is called *out-of-sample validation*.\n\n    -   The problem, of course, is we usually do not have the resources to collect a new sample.\n\n-   Therefore, statisticians have worked hard to find ways to estimate elpd from the current sample, and there are two broad approaches, information criteria and cross-validation.\n\n## Overview of comparison methods\n\n1.  **Information criteria:** AIC, DIC, and WAIC, which estimate the elpd in the current sample, minus a correction factor.\n\n2.  **Cross validation**: A method that splits the current sample into $K$ parts, estimates the parameters in $K âˆ’ 1$ parts, and estimates the elpd in the remaining 1 part.\n\n-   A special case is when $K = n$ so that each time one uses $n-1$ data points to estimate the model parameters, and estimates the elpd for the observation that was left out. This is called **leave-one-out cross-validation (LOO-CV)**.\n\n## Information criteria\n\n-   Several information criteria have been proposed that do not require fitting the model several times, including AIC, DIC, and WAIC.\n\n-   We will introduce the information criteria, assuming a likelihood $f(\\mathbf{Y} | \\boldsymbol{\\theta})$ for observed data $\\mathbf{Y} = (Y_1,\\ldots,Y_n)$ with population parameter $\\boldsymbol{\\theta}$.\n\n-   Information criteria are often presented as **deviance**, defined as, $D(\\mathbf{Y}|\\boldsymbol{\\theta}) = âˆ’2 \\log f(\\mathbf{Y}|\\boldsymbol{\\theta})$.\n\n-   Ideally, models will have small deviance.\n\n-   However, if a model is too complex it will have small deviance but be unstable (overfitting).\n\n## Akaike information criteria (AIC)\n\nAkaike information criteria (AIC) estimates the elpd as,\n\n$$\\widehat{\\text{elpd}}_{\\text{AIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) - p,$$ where $p$ is the number of parameters estimated in the model and $\\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}$ is the MLE point estimate.\n\n-   $\\text{AIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{MLE}}) + 2p$\n\n-   $p$ is an adjustment for overfitting, but once we go beyond linear models, we cannot simply add $p$.\n\n-   Informative priors tend to reduce the amount of overfitting.\n\n-   Model with smaller AIC are preferred.\n\n## Deviance information criteria (DIC)\n\nDeviance information criteria (DIC) estimates the elpd as,\n\n$$\\widehat{\\text{elpd}}_{\\text{DIC}} = \\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - p_{\\text{DIC}},$$ where $\\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}$ is a Bayesian point estimate, typically a posterior mean, and $p_{\\text{DIC}}$ is an estimate of the complexity penalty,\n\n$$p_{\\text{DIC}} = 2 \\left(\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(\\mathbf{Y} | \\boldsymbol{\\theta}) \\right]\\right).$$\n\n-   The second term can be estimated as a MC integral.\n\n-   $\\text{DIC} = -2\\log f(\\mathbf{Y} | \\hat{\\boldsymbol{\\theta}}_{\\text{Bayes}}) + 2p_{\\text{DIC}}.$\n\n## Deviance information criteria (DIC) {.midi}\n\n-   Advantages of DIC:\n\n    -   The effective number of parameters is a useful measure of model complexity.\n\n    -   Intuitively, if there are $p$ parameters and we have uninformative priors then $p_D \\approx p$.\n\n    -   However, $p_D \\ll p$ if there are strong priors.\n\n-   Disadvantages of DIC:\n\n    -   DIC can only be used to compare models with the same likelihood.\n\n    -   DIC really only applies when the posterior is approximately normal, and will give misleading results when the posterior is far from normality (e.g., bimodal).\n\n## Watanabe-Akaike information criteria (WAIC) {.midi}\n\nWatanabe-Akaike or widely available information criteria (WAIC) estimates the elpd as,\n\n$$\\widehat{\\text{elpd}}_{\\text{WAIC}} = \\text{lppd} - p_{\\text{WAIC}}.$$\n\n-   The log pointwise predictive density (lppd) is given by, $$\\text{lppd} = \\log \\prod_{i=1}^n f(Y_i | \\mathbf{Y}) =  \\sum_{i=1}^n \\log \\int f\\left(Y_i | \\boldsymbol{\\theta}\\right)f(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}.$$\n\n-   lppd can be estimated as, $\\sum_{i=1}^n \\log \\left(\\frac{1}{S} \\sum_{s = 1}^S f\\left(Y_i | \\boldsymbol{\\theta}^{(s)}\\right)\\right)$, where $\\boldsymbol{\\theta}^{(s)}$ are drawn from the posterior.\n\n## WAIC\n\nThere are two common estimates of $p_{\\text{WAIC}}$, both of which can be estimated using MC samples of the posterior. \\begin{align*}\np_{\\text{WAIC}_1} &= 2 \\sum_{i=1}^n\\left(\\log \\left( \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[f(Y_i | \\boldsymbol{\\theta})\\right]\\right) - \\mathbb{E}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left[\\log f(Y_i | \\boldsymbol{\\theta}) \\right]\\right)\\\\\np_{\\text{WAIC}_2} &= \\sum_{i=1}^n \\mathbb{V}_{\\boldsymbol{\\theta} | \\mathbf{Y}}\\left(\\log f\\left(Y_i | \\mathbf{\\theta}\\right)\\right)\n\\end{align*}\n\n-   $\\text{WAIC} = -2 \\text{lppd} + 2p_{\\text{WAIC}}.$\n\n## WAIC\n\n-   WAIC has the desirable property of averaging over the posterior distribution, instead of conditioning on a point estimate.\n\n-   $p_{\\text{WAIC}}$ can be thought of as an approximation to the number of unconstrained parameters in the model.\n\n-   In practice, $p_{\\text{WAIC}_2}$ is often used, since it is theoretically closer to LOO-CV.\n\n## Cross-validation\n\n-   A common approach to compare models is using cross-validation.\n\n-   This is exactly the same procedure used in classical statistics.\n\n-   This operates under the assumption that the *true* model likely produces better out-of-sample predictions than competing models.\n\n-   *Advantages:* Simple, intuitive, and broadly applicable.\n\n-   *Disadvantages:* Slow because it requires several model fits and it is hard to say a difference is statistically significant.\n\n## K-fold cross-validation {.midi}\n\n0.  Split the data into $K$ equally-sized groups.\n\n1.  Set aside group $k$ as test set and fit the model to the remaining $K âˆ’ 1$ groups.\n\n2.  Make predictions for the test set $k$ based on the model fit to the training data.\n\n3.  Repeat steps 1 and 2 for $k = 1, \\dots, K$ giving a predicted value $\\widehat{Y}_i$ for all $n$ observations.\n\n4.  Measure prediction accuracy, e.g.,\n\n$$MSE = \\frac{1}{n}\\sum_{i=1}^n (Y_i - \\widehat{Y}_i)^2.$$\n\n## Variants of cross-validation\n\n-   Usually $K$ is either 5 or 10.\n\n-   $K = n$ is called leave-one-out cross-validation (LOO-CV), which is great but slow.\n\n-   The predicted value $\\widehat{Y}_i$ can be either the posterior predictive mean or median.\n\n-   Mean squared error (MSE) can be replaced with mean absolute deviation (MAD),\n\n$$MAD = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\widehat{Y}_i|.$$\n\n## Leave-one-out cross-validation (LOO-CV) {.midi}\n\n-   Assume the data are partitioned into a training set, $\\mathbf{Y}_{\\text{train}}$ and a holdout set $\\mathbf{Y}_{\\text{test}}$, thus yielding a posterior distribution $f(\\boldsymbol{\\theta} | \\mathbf{Y}_{\\text{train}})$.\n\n-   In the setting of LOO-CV, we have $n$ different $f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right)$.\n\n-   The Bayesian LOO-CV estimate of out-of-sample predictive fit is $$\\text{lppd}_{\\text{LOO-CV}} = \\sum_{i=1}^n \\log f\\left(\\boldsymbol{\\theta} | \\mathbf{Y}_{-i}\\right),$$\n\n-   The estimated number of parameters can be computed as,\n\n$$p_{\\text{LOO-CV}} = \\text{lppd} - \\text{lppd}_{\\text{LOO-CV}}.$$ - This also referred to as leave-one-out information criteria (LOO-IC)\n\n## LOO-CV\n\nLOO-CV estimates the elpd as,\n\n$$\\widehat{\\text{elpd}}_{\\text{LOO-CV}} = \\text{lppd}_{\\text{LOO-CV}} - p_{\\text{WAIC}} = \\text{lppd}_{\\text{LOO-CV}}.$$\n\n-   Under some common models there are shortcuts for computing it, however in general these do not exist.\n\n-   WAIC can be treated as a fast approximation of LOO-CV.\n\n-   In Stan, LOO-CV is approximated using the [Pareto smoothed importance sampling (PSIS)](https://arxiv.org/abs/1507.04544) to make the process faster, without having to repeat the process $n$ times.\n\n## Computing WAIC and LOO-CV using Stan\n\nWe need to update the **generated quantities** code block.\n\n\n\n::: {.cell output.var='waic'}\n\n```{.stan .cell-code}\ngenerated quantities {\n  ...\n  vector[n] log_lik;\n  for (i in 1:n) log_lik[i] = normal_lpdf(Y[i] | X[i, ] * beta, sigma);\n}\n```\n:::\n\n\n\n## Let's simulate some data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###True parameters\nsigma <- 1.5 # true measurement error\nbeta <- matrix(c(-1.5, 3, 1), ncol = 1) # true beta\n\n###Simulation settings\nn <- 100 # number of observations\nn_pred <- 10 # number of predicted observations\np <- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX <- cbind(1, matrix(rnorm(n * p), ncol = p))\nY <- as.numeric(X %*% beta + rnorm(n, 0, sigma))\nX_pred <- cbind(1, matrix(rnorm(n_pred * p), ncol = p))\nY_pred <- as.numeric(X_pred %*% beta + rnorm(n_pred, 0, sigma))\n```\n:::\n\n\n\n## An example model comparison\n\nTrue Model: $\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}$\n\nModel 1: $\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1}$\n\nModel 2: $\\mathbb{E}[Y_i] = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2}$\n\n## Fit model 1\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Create stan data object\nstan_data_model1 <- list(n = n, p = p - 1, Y = Y, X = X[, -3],\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred[, -3])\n  \n###Compile model separately\nstan_model <- stan_model(file = \"linear_regression_ppd_log_lik.stan\")\n\n###Run model 1 and save\nfit_model1 <- sampling(stan_model, data = stan_data_model1, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model1, file = \"linear_regression_ppd_log_lik_fit_model1.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Fit model 2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Create stan data object\nstan_data_model2 <- list(n = n, p = p, Y = Y, X = X,\n                         beta0 = 0, sigma_beta = 10, a = 3,  b = 1,\n                         n_pred = n_pred, X_pred = X_pred)\n\n###Run model 2 and save\nfit_model2 <- sampling(stan_model, data = stan_data_model2, \n                chains = 4, iter = 1000)\nsaveRDS(fit_model2, file = \"linear_regression_ppd_log_lik_fit_model2.rds\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Computing WAIC\n\n-   Begin by extracting the log-likelihood values from the model.\n\n-   We will use the [`loo` package](https://mc-stan.org/loo/).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Load loo package\nlibrary(loo)\n\n###Extract log likelihood\nlog_lik_model1 <- loo::extract_log_lik(fit_model1, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_model2 <- loo::extract_log_lik(fit_model2, parameter_name = \"log_lik\", merge_chains = TRUE)\n\n###Explore the object\nclass(log_lik_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"matrix\" \"array\" \n```\n\n\n:::\n\n```{.r .cell-code}\ndim(log_lik_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2000  100\n```\n\n\n:::\n:::\n\n\n\n## Computing WAIC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Compute WAIC for the two models\nwaic_model1 <- loo::waic(log_lik_model1)\nwaic_model2 <- loo::waic(log_lik_model2)\n\n###Inspect WAIC for model 1\nwaic_model1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -201.0  6.6\np_waic         3.0  0.5\nwaic         401.9 13.1\n```\n\n\n:::\n:::\n\n\n\n## Computing WAIC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Inspect WAIC for model 2\nwaic_model2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nComputed from 2000 by 100 log-likelihood matrix.\n\n          Estimate   SE\nelpd_waic   -178.9  6.6\np_waic         3.9  0.6\nwaic         357.7 13.1\n```\n\n\n:::\n:::\n\n\n\n## Computing WAIC\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Make a comparison\ncomp_waic <- loo::loo_compare(list(\"true\" = waic_model2, \"misspec\" = waic_model1))\nprint(comp_waic, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.09      5.76 \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(comp_waic, digits = 2, simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \ntrue       0.00      0.00 -178.86      6.57         3.88    0.58    357.72\nmisspec  -22.09      5.76 -200.95      6.56         2.99    0.52    401.90\n        se_waic\ntrue      13.14\nmisspec   13.12\n```\n\n\n:::\n:::\n\n\n\n## Computing LOO-CV/LOO-CI\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Compute LOO-IC for the two models\nloo_model1 <- loo::loo(log_lik_model1)\nloo_model2 <- loo::loo(log_lik_model2)\n\n###Make a comparison\ncomp <- loo::loo_compare(list(\"true\" = loo_model2, \"misspec\" = loo_model1))\nprint(comp, digits = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        elpd_diff se_diff\ntrue      0.00      0.00 \nmisspec -22.08      5.76 \n```\n\n\n:::\n\n```{.r .cell-code}\nprint(comp, digits = 2, simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        elpd_diff se_diff elpd_loo se_elpd_loo p_loo   se_p_loo looic  \ntrue       0.00      0.00 -178.88     6.57        3.90    0.58   357.77\nmisspec  -22.08      5.76 -200.96     6.56        3.00    0.52   401.93\n        se_looic\ntrue      13.14 \nmisspec   13.12 \n```\n\n\n:::\n:::\n\n\n\n## Prepare for next class\n\n-   Work on [HW 02](https://biostat725-sp25.netlify.app/hw/hw-02), which was just assigned\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Bayesian Workflow\n",
    "supporting": [
      "07-model-comparison_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}