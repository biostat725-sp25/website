{
  "hash": "b608277f2a26f679222dc92f2d32067d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian Clustering\"\nauthor: \"Prof. Sam Berchuck (developed with Braden Scherting)\"\ndate: \"2025-04-08\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Learning Objectives\n\n1.  We will introduce the basic mixture modeling framework as a mechanism for model-based clustering and describe computational and inferential challenges.\n2.  Variations of the popular finite Gaussian mixture model (GMM) will be introduced to cluster patients according to ED length-of-stay.\n3.  We present an implementation of mixture modeling in Stan and discuss challenges therein.\n4.  Finally, various posterior summaries will be explored.\n\n## Finding subtypes\n\nRevisiting data on patients admitted to the emergency department (ED) from the MIMIC-IV-ED demo.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n**Can we identify subgroups within this population?**\n\n## The usual setup\n\nMost models introduced in this course are of the form:\n\n$$f\\left(Y_i\\mid X_i\\right) = f\\left(Y_i\\mid \\boldsymbol{\\theta}_i\\left(X_i\\right)\\right).$$\n\n-   $f(\\cdot)$ is the density or distribution function of an assumed family (e.g., Gaussian, binomial),\n\n-   $\\boldsymbol{\\theta}_i$ is a parameter (or parameters) that may depend on individual covariates $X_i$.\n\n## The usual setup\n\n$$f\\left(Y_i\\mid X_i\\right) = f\\left(Y_i\\mid \\boldsymbol{\\theta}_i\\left(X_i\\right)\\right)$$\n\n**Linear regression:**\n\n-   $f$ is the Gaussian density function, and $\\boldsymbol{\\theta}_i(X_i)=(X_i\\beta, \\sigma^2)^\\top$\n\n**Binary classification:**\n\n-   $f$ is the Bernoulli mass function, and $\\boldsymbol{\\theta}_i(X_i)=\\text{logit}(X_i\\beta)^{-1}$\n\n## Limitations of the usual setup\n\nSuppose patients $i=1,\\dots,n$ are administered a diagnostic test. Their outcome $Y_i$ depends only on whether or not they have previously received treatment: $X_i=1$ if yes and $X_i=0$ otherwise. Suppose the diagnostic test has Gaussian-distributed measurement error, so $$Y_i\\mid X_i \\sim N(\\alpha + \\beta X_i, \\sigma^2).$$ Now, suppose past treatment information is not included in patients' recordâ€”*we cannot condition on* $X_i$. Marginalizing, \\begin{align*}\np(Y_i) = &P(X_i=1)\\times N(Y_i\\mid \\alpha + \\beta, \\sigma^2) \\\\\n      & +P(X_i=0)\\times N(Y_i\\mid \\alpha, \\sigma^2).\n\\end{align*}\n\n## Limitations of the usual setup\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 500; mu <- c(1,4.5); s2 <- 1\nx <- sample(1:2, n, T); y <- rnorm(n, mu[x], sqrt(s2))\nggplot(data.frame(y = y), aes(x = y)) + \n  geom_histogram() + \n  labs(x = \"Y\", y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Limitations of the usual setup\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfit <- lm(y ~ 1)\nggplot(data.frame(residuals = fit$residuals), aes(x = residuals)) + \n  geom_histogram() + \n  labs(x = \"Residuals\", y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n*Normality of residuals?*\n\n## Mixture Model {.midi}\n\nMotivation for using a mixture model: *Standard distributional families are not sufficiently expressive*.\n\n-   The inflexibility of the model may be due to unobserved heterogeneity (e.g., unrecorded treatment history).\n\nGenerically, $$f(Y_i) = \\sum_{h = 1}^k \\pi_h \\times f_h(Y_i).$$\n\n**Uses of mixture models:**\n\n1)  Modeling weird densities/distributions (e.g., bimodal).\n2)  Learning latent groups/clusters.\n\n## Mixture Model {.midi}\n\n$$f(Y_i) = \\sum_{h=1}^k \\pi_h\\times f_h(Y_i)$$\n\n-   This mixture is comprised of $k$ components indexed by $h=1,\\dots,k$. For each component, we have a probability density (or mass) function $f_h$ and a mixture weight $\\pi_h$, where $\\sum_{h=1}^k \\pi_k=1$.\n\n-   When $k$ is finite, we call this a **finite mixture model** for $Y_i$.\n\n-   It is common to let, $$f_h(Y_i) = f(Y_i\\mid \\boldsymbol{\\theta}_h).$$\n\n    -   The component densities share a functional form and differ in their parameters.\n\n## Gaussian Mixture Model\n\nLetting $f_h(Y_i) = N(Y_i\\mid \\mu_h, \\sigma^2_h)$ for $h=1,\\dots,k$, yields the Gaussian mixture model. For $Y_i\\in\\mathbb{R}$, \n$$f(Y_i) = \\sum_{h=1}^k \\pi_h N\\left({Y}_i\\mid \\mu_h, \\sigma^2_h\\right)$$\n\nFor multivariate outcomes $\\mathbf{Y}_i\\in\\mathbb{R}^p$,\n\n$$ f(\\mathbf{Y}_i) = \\sum_{h=1}^k \\pi_h N_p\\left(\\mathbf{Y}_i\\mid\\boldsymbol{\\mu}_h, \\boldsymbol{\\Sigma}_h\\right).$$\n\n## Gaussian Mixture Model {.small}\n\nConsider a mixture model with 3 groups:\n\n-   Mixture 1: $\\mu_1 = -1.5, \\sigma_1 = 1$.\n-   Mixture 2: $\\mu_2 = 0, \\sigma_2 = 1.5$.\n-   Mixture 3: $\\mu_3 = 2, \\sigma_3 = 0.6$.\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=480}\n:::\n\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-5-2.png){fig-align='center' width=480}\n:::\n:::\n\n\n\nNotice, both means $\\mu_h$ and variances $\\sigma^2_h$ vary across clusters.\n\n## Generative perspective on GMM\n\nTo simulate from a $k$-component Gaussian mixture with means $\\mu_1,\\dots,\\mu_k$, variances $\\sigma_1^2,\\dots,\\sigma^2_k$, and weights $\\pi_1,\\dots,\\pi_k$:\n\n1.  Sample the component indicator $z_i\\in \\{1, \\dots,k\\}$ with probabilities: $$P(z_i=h) = \\pi_h \\iff z_i \\sim \\text{Categorical}(k, \\{\\pi_1,\\ldots,\\pi_k\\}).$$\n2.  Given $z_i$, sample $Y_i$ from the appropriate component: $$\\left(Y_i\\mid z_i =h\\right) \\sim N\\left(\\mu_h, \\sigma^2_h\\right).$$\n\n## Generative perspective on GMM\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 500 \nmu <- c(1, 4.5)\ns2 <- 1 \n# implicit: pi = c(0.5, 0.5)\nz <- sample(1:2, n, TRUE)\ny <- rnorm(n, mu[z], sqrt(s2))\n```\n:::\n\n\n\n*This is essentially the code used to simulate the missing treatment history example.*\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Marginalizing Component Indicators {.midi}\n\nThe label $z_i$ indicates which component $Y_i$ is drawn from---*think of this as the cluster label*: $f\\left(Y_i\\mid z_i=h\\right) = N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right).$\n\nBut $z_i$ is unknown, so we marginalize to obtain:\n\n\\begin{align*}\nf(Y_i) &= \\int_\\mathcal{Z}f\\left(Y_i\\mid z\\right) f(z)dz \\\\\n&= \\sum_{h=1}^k f\\left(Y_i\\mid z=h\\right) P(z=h) \\\\\n&= \\sum_{h=1}^k N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right) \\times \\pi_h.\n\\end{align*}\n\n*This is key to implementing in* `Stan`.\n\n## Gaussian mixture in Stan {.midi}\n\nComponent indicators $z_i$ are discrete parameters, which cannot be estimated in `Stan`. As before, suppose $f(Y_i) = \\sum_{h=1}^k  \\pi_h N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right)$.\n\nThe log-likelihood is:\n\n\\begin{align*}\n\\log f(Y_i) \n&= \\log \\sum_{h=1}^k  \\exp \\left(\\log\\left[\\pi_h N\\left(Y_i\\mid \\mu_h,\\sigma^2_h \\right) \\right]\\right)\\\\\n&= \\verb|log_sum_exp| \\left[\\log\\pi_1 + \\log N\\left(Y_i\\mid \\mu_1,\\sigma^2_1 \\right),\\right. \\\\ \n&\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\dots, \\\\\n&\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\left.\\log\\pi_k + \\log N\\left(Y_i\\mid \\mu_k,\\sigma^2_k \\right) \\right],\n\\end{align*}\n\n`log_sum_exp` is a `Stan` function.\n\n## Gaussian mixture in Stan\n\n\n\n::: {.cell output.var='tmp'}\n\n```{.stan .cell-code}\n// saved in mixture1.stan\ndata {\n  int<lower = 1> k;          // number of mixture components\n  int<lower = 1> n;          // number of data points\n  array[n] real Y;           // observations\n}\nparameters {\n  simplex[k] pi; // mixing proportions\n  ordered[k] mu; // means of the mixture components\n  vector<lower=0>[k] sigma; // sds of the mixture components\n}\nmodel {\n  target += normal_lpdf(mu |0.0, 10.0);\n  target += exponential_lpdf(sigma | 1.0);\n  vector[k] log_probs = log(pi);\n  for (i in 1:n){\n    vector[k] lps = log_probs;\n    for (h in 1:k){\n      lps[h] += normal_lpdf(Y[i] | mu[h], sigma[h]);\n    }\n    target += log_sum_exp(lps);\n  }\n}\n```\n:::\n\n\n\nOf note: `simplex` and `ordered` types.\n\n## First fit {.midi}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ned <- read.csv(\"exam1_data.csv\")\ndat <- list(Y = (ed$los - mean(ed$los)),\n            n = length(ed$los),\n            k = 2)\nmod1 <- stan_model(\"mixture1.stan\")\nfit1 <- sampling(mod1, data=dat, chains=4, iter=5000, control=list(\"adapt_delta\"=0.99))\nprint(fit1, pars = c(\"pi\", \"mu\", \"sigma\"), probs = c(0.025, 0.975))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5% 97.5% n_eff Rhat\npi[1]     0.37    0.17 0.24  0.16  0.83     2 6.35\npi[2]     0.63    0.17 0.24  0.17  0.84     2 6.35\nmu[1]    -3.17    0.09 0.51 -4.49 -2.49    33 1.05\nmu[2]     0.45    3.96 5.69 -3.17 13.08     2 5.14\nsigma[1] 13.25    4.48 6.46  2.09 20.11     2 4.97\nsigma[2]  5.03    3.28 4.68  2.03 14.78     2 7.56\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 22 13:54:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## What is going on?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npairs(fit1, pars = c(\"mu\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n<!-- ## What is going on? -->\n\n<!-- ```{r echo=T} -->\n\n<!-- samples <- rstan::extract(fit1) -->\n\n<!-- hist(dat$Y, breaks = 75, probability = T, xlim=c(min(dat$Y)-10,max(dat$Y)+10), col=\"white\", main=\"Posterior draws\", ylab=\"\", xlab=\"\") -->\n\n<!-- for (it in sample(1:10000, 15)){ -->\n\n<!--   curve(mean(samples$pi[it,1]) * dnorm(x, mean(samples$mu[it,1]), mean(samples$sigma[it,1])),  -->\n\n<!--         from=min(dat$Y)-10, to=max(dat$Y)+10, ylab=\"dens\", add=T, n = 1001, lty=1, lwd=2, col=rgb(1,0.1,0, 0.5)) -->\n\n<!--   curve(mean(samples$pi[it,2]) * dnorm(x, mean(samples$mu[it,2]), mean(samples$sigma[it,2])),  -->\n\n<!--         from=min(dat$Y)-10, to=max(dat$Y)+10, ylab=\"dens\", add=T, n = 1001, lty=2, lwd=2, col=rgb(0,0.1,1, 0.25)) -->\n\n<!-- } -->\n\n<!-- x <- rep(seq(min(dat$Y)-10, max(dat$Y)+10, length.out=1000), 2) -->\n\n<!-- ``` -->\n\n## Bimodal posterior\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-12-1.png){width=960}\n:::\n:::\n\n\n\n-   In one mode, $\\sigma^2_1 \\ll \\sigma^2_2$ and in the other, $\\sigma^2_1\\gg\\sigma^2_2$\n\n## Bimodal posterior\n\n\n\n::: {.cell layout=\"[[50,50]]\"}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-13-1.png){width=960}\n:::\n\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-13-2.png){width=960}\n:::\n:::\n\n\n\nThe Gaussian clusters have light tails, so outlying values of $Y$ force large values of $\\sigma^2_h$. When $\\sigma^2_h$ is large, small changes to $\\mu_h$ have little impact on the log-likelihood, and the ordering constraint is not sufficient to identify the clusters.\n\n## Things to consider when your mixture model is mixed up\n\nMixture modeling, *especially when clusters are of interest*, can be fickle.\n\n1)  Different mixtures can give similar fit to data, leading to multimodal posteriors that are difficult to sample from (previous slides).\n2)  Clusters will depend on your choice of $f_h$â€”a Gaussian mixture model can only find Gaussian-shaped clusters.\n3)  Increasing $k$ often improves fit, but may muddle cluster interpretation.\n\n## Things to consider when your mixture model is mixed up\n\n1)  Employ informative priors.\n2)  Vary the number of clusters.\n3)  Change the form of the kernel.\n\n## Updated model {.midi}\n\n\n\n::: {.cell output.var='tmp2'}\n\n```{.stan .cell-code}\n// saved in mixture2.stan\ndata {\n  int<lower = 1> k;          // number of mixture components\n  int<lower = 1> n;          // number of data points\n  array[n] real Y;         // observations\n}\nparameters {\n  simplex[k] pi; // mixing proportions\n  ordered[k] mu; // means of the mixture components\n  vector<lower = 0>[k] sigma; // sds of the mixture components\n  vector<lower = 1>[k] nu;\n}\nmodel {\n  target += normal_lpdf(mu | 0.0, 10.0);\n  target += normal_lpdf(sigma | 2.0, 0.5);\n  target += gamma_lpdf(nu | 5.0, 0.5);\n  vector[k] log_probs = log(pi);\n  for (i in 1:n){\n    vector[k] lps = log_probs;\n    for (h in 1:k){\n      lps[h] += student_t_lpdf(Y[i] | nu[h], mu[h], sigma[h]);\n    }\n    target += log_sum_exp(lps);\n  }\n}\n```\n:::\n\n\n\n-   Informative prior on $\\sigma^2_h$.\n-   Mixture of Student-t.\n\n## Updated model fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod2 <- stan_model(\"mixture2.stan\")\nfit2 <- sampling(mod2, data=dat, chains=4, iter=5000, control=list(\"adapt_delta\"=0.99))\nprint(fit2, pars=c(\"pi\", \"mu\", \"sigma\", \"nu\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n          mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\npi[1]     0.81    0.00 0.04  0.73  0.79  0.81  0.83  0.87  5567    1\npi[2]     0.19    0.00 0.04  0.13  0.17  0.19  0.21  0.27  5567    1\nmu[1]    -2.96    0.00 0.22 -3.39 -3.10 -2.96 -2.81 -2.53  7318    1\nmu[2]     8.30    0.02 1.10  6.03  7.66  8.35  9.01 10.28  4306    1\nsigma[1]  2.20    0.00 0.17  1.88  2.09  2.20  2.31  2.55  5987    1\nsigma[2]  2.82    0.00 0.40  2.06  2.55  2.81  3.09  3.64  6745    1\nnu[1]    11.98    0.04 4.44  5.14  8.75 11.38 14.57 22.22  9860    1\nnu[2]     1.54    0.00 0.43  1.03  1.25  1.46  1.74  2.50  9286    1\n\nSamples were drawn using NUTS(diag_e) at Sat Mar 29 12:52:38 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Updated model results\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## From marginal mixture model to clusters {.midi}\n\n`Stan` cannot directly *infer* categorical component indicators $z_i$. Instead, for each individual, we compute\n\n\\begin{align*}\nP\\left(z_i = h \\mid Y_i, \\boldsymbol{\\mu},\\boldsymbol{\\sigma},\\boldsymbol{\\pi} \\right) &= \\frac{f(Y_i\\mid z_i = h, \\mu_h,\\sigma_h)P(z_i=h\\mid \\pi_h)}{\\sum_{h'=1}^k f(Y_i\\mid z_i = h', \\mu_{h'},\\sigma_{h'})P(z_i=h'\\mid \\pi_{h'})}\\\\\n&= \\frac{N(Y_i | \\mu_{h},\\sigma_{h})\\pi_{h}}{\\sum_{h' = 1}^k N(Y_i | \\mu_{h'},\\sigma_{h'})\\pi_{h'}} = p_{ih}.\n\\end{align*}\n\nGiven these cluster membership probabilities, we can recover cluster indicators through simulation: $$(z_i\\mid -) \\sim \\text{Categorical}\\left(k, \\left\\{ p_{i1},\\dots,p_{ik}  \\right\\}\\right).$$\n\n## From marginal mixture model to clusters\n\n\n\n::: {.cell output.var='gq'}\n\n```{.stan .cell-code}\n...\n\ngenerated quantities {\n  matrix[n,k] lPrZik;\n  int<lower=1, upper=k> z[n];\n  for (i in 1:n){\n    for (h in 1:k){\n      lPrZik[i,h] = log(pi[h]) + student_t_lpdf(Y[i] | nu[h], mu[h], sigma[h]);\n    }\n    lPrZik[i] -= log(sum(exp(lPrZik[i])));\n    z[i] = categorical_rng(exp(lPrZik[i]'));\n  }\n}\n```\n:::\n\n\n\n## Co-clustering probabilities\n\nRecovering $z_i$ allows us to make the following pairwise comparison: *what is the probability that unit* $i$ and unit $j$ are in the same cluster? This is the \"co-clustering probability\".\n\nIt is common to arrange these probabilities in a co-clustering matrix $\\mathbf{C}$, where the $i,j$ entry is given by, $$C_{ij}=P\\left( z_i=z_j\\mid- \\right)\\approx \\frac{1}{S}\\sum_{s=1}^S \\mathbb{1}\\left[z_i^{(s)}=z_j^{(s)}\\right].$$\n\n## Co-clustering probabilities\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n\n## How do our results change when we use more components?\n\n$k=3$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n\n\n## How do our results change when we use more components?\n\n$k=4$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n## Co-clusterings across $k$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](23-clustering_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\nThe same general pattern persists when more clusters are used, indicating that $k=2$ is a reasonable choice.\n\n## Prepare for next class\n\n1.  Reminder: On Thursday, we will have a in-class live-coding exercise.\n\n2.  Begin working on Exam 02, which is due for feedback on April 15.\n",
    "supporting": [
      "23-clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}