{
  "hash": "19fbb19ab5c22055993af2bff568f095",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Longitudinal Data\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-03-06\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   During our last lecture, we introduced correlated (or dependent) data sources.\n\n-   We discussed the idea of accounting for dependencies within a group using group-specific parameters.\n\n-   We introduced the random intercept model and studied the induced correlation (forced to be positive) in the marginal model.\n\n-   Today we will look at longitudinal data and introduce a simple model that accounts for group-level changes.\n\n## Longitudinal Data\n\nRepeated measurements taken over time from the same subjects. Examples include:\n\n-   **Monitor Disease Progression**: Track how diseases evolve, such as diabetes or glaucoma.\n\n-   **Evaluate Treatments**: Understand how interventions work over time.\n\n-   **Personalized Health Insights**: Capture individual health trajectories for personalized care.\n\n-   **Study Long-Term Effects**: Evaluate the long-term outcomes of medical treatments or behaviors.\n\n## Example: Glaucoma Disease Progression\n\nImagine we are tracking mean deviation (MD, dB), a key measure of visual field loss in glaucoma patients, over time.\n\n-   Multiple measurements of MD for each patient across several years.\n\n-   We're interested in glaucoma progression, which is defined as the rate of change in MD over time (dB/year).\n\n-   Define $Y_{it}$ as the MD value for eye $i$ ($i = 1,\\ldots,n$) at time $t$ ($t = 1,\\ldots,n_i$) and the time of each observation as $X_{it}$ with $X_{i0} = 0$.\n\n## Rotterdam data\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Treating Eyes Separately\n\nWe can model each eye **separately** using OLS (this is a form of longitudinal analysis!). For $t = 1,\\ldots,n_i$, the model is:\n\n$$Y_{it} = \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma_i^2).$$\n\nWhere:\n\n-   $\\beta_{0i}$ is the intercept for eye $i$.\n\n-   $\\beta_{1i}$ is the slope for eye $i$ (i.e., disease progression).\n\n-   $\\sigma_i^2$ is the residual error for eye $i$.\n\n## OLS regression\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Treating Eyes Separately\n\n-   Fitting OLS separately allows **each eye** to have a unique intercept and slope, which of course is consistent with the data generating process.\n\n-   However, this can lead to eye-specific intercepts and slopes that are not realistic (consider OLS regression with very few data points).\n\n-   Estimating eye-specific intercepts and slopes within the context of the whole study sample should shrink extreme values toward the population average.\n\n## Subject-specific intercepts and slopes\n\nFor $i = 1,\\ldots,n$ and $t=1,\\ldots,n_i$, we can write the model:\n\n\\begin{align*}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{align*}\n\n**Population Parameters**:\n\n-   $\\beta_0$ is the population intercept (i.e., average MD value in the population at time zero).\n\n-   $\\beta_1$ is the population slope (i.e., average disease progression).\n\n-   $\\sigma^2$ is the population residual error.\n\n## Subject-specific intercepts and slopes\n\nFor $i = 1,\\ldots,n$ and $t=1,\\ldots,n_i$, we can write the model:\n\n\\begin{align*}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{align*}\n\n**Subject-Specific Parameters**:\n\n-   $\\theta_{0i}$ is the subject-specific deviation from the intercept for eye $i$.\n\n-   $\\theta_{1i}$ is the subject-specific deviation from the slope for eye $i$.\n\n## Subject-specific intercepts and slopes\n\nFor $i = 1,\\ldots,n$ and $t=1,\\ldots,n_i$, we can write the model:\n\n\\begin{align*}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{align*}\n\n**Key Advantage**:\n\n-   This model defines subject-specific estimates of $\\beta_{0i}$ and $\\beta_{1i}$ relative to the population average, preventing overfitting and making the estimates more stable.\n\n-   **Shrinks** subject-specific parameters to the population average.\n\n## Linear Mixed Model\n\nThe subject-specific intercepts and slope model can be seen as a special case of the linear mixed model (LMM). For $i = 1,\\ldots,n$, LMM is defined as:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\n-   $\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})$ are subject-level observations.\n\n-   $Y_{it}$ is the $t$th observation in subject $i$.\n\n-   $\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i})$, such that $\\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)$.\n\n## Linear Mixed Model\n\nFor $i = 1,\\ldots,n$, the linear mixed model (LMM) is given by:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\n-   $\\mathbf{X}_i$ is an $(n_i \\times p)$-dimensional matrix with row $\\mathbf{x}_{it}$ (intercept is incorporated).\n\n-   $\\mathbf{x}_{it}$ contains variables that are assumed to relate to the outcome only at a population-level.\n\n-   $p$ is the number of population-level variables.\n\n## Linear Mixed Model\n\nFor $i = 1,\\ldots,n$, the linear mixed model (LMM) is given by:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\n-   $\\mathbf{Z}_i$ is an $(n_i \\times q)$-dimensional matrix with row $\\mathbf{z}_{it}$ (intercept is incorporated).\n\n-   $\\mathbf{z}_{it}$ contains variables that are assumed to relate to the outcome with varying effects at a subject-level.\n\n-   $q$ is the number of subject-level variables.\n\n## Linear Mixed Model\n\nFor $i = 1,\\ldots,n$, the linear mixed model (LMM) is given by:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\n-   $\\boldsymbol{\\beta}$ is a $p$-dimensional vector of population-level parameters (or fixed effects).\n\n-   $\\boldsymbol{\\theta}_i$ is a $q$-dimensional vector of group-level parameters (or random effects).\n\n-   $\\sigma^2$ is a population-level parameter that measures residual error.\n\n## Recover the Random Intercept Model\n\nFor $i = 1,\\ldots,n$, the linear mixed model (LMM) is given by:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\nSuppose that $\\mathbf{z}_{it} = 1 \\forall i,t$. Then we get\n\n\\begin{align*}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\theta_{i} + \\epsilon_{it}.\n\\end{align*}\n\n-   LMM is a general form of the random intercept model.\n\n## Random Slope and Intercept Model\n\nFor $i = 1,\\ldots,n$, the linear mixed model (LMM) is given by:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\nSuppose that $\\mathbf{x}_{it} = \\mathbf{z}_{it} = (1, X_{it})$, such that $p = q = 2$. Then,\n\n\\begin{align*}\nY_{it} &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{z}_{it}\\boldsymbol{\\theta}_{i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n&= \\beta_0 + \\beta_1 X_{it} + \\theta_{0i} + \\theta_{1i} X_{it} + \\epsilon_{it}\\\\\n&= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it} + \\epsilon_{it}.\n\\end{align*}\n\nwhere $\\boldsymbol{\\beta} = (\\beta_0,\\beta_1)$ and $\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})$.\n\n## Prior Specification {.midi}\n\nOne choice could be to specify independent priors for the subject-specific intercepts and slopes:\n\n\\begin{align*}\n\\theta_{0i} &\\stackrel{iid}{\\sim} N(0, \\tau_0^2)\\\\\n\\theta_{1i} &\\stackrel{iid}{\\sim} N(0, \\tau_1^2).\n\\end{align*}\n\n-   This is the same assumption we made last lecture, where we assume a normal distribution centered at zero with some variance that reflects variability across subjects.\n\n-   Often times this assumption is oversimplified. For example in glaucoma progression, we often assume that if someone has a higher baseline MD they will a more negative slope (i.e., negative correlation).\n\n## Prior Specification {.midi}\n\nWe can instead model the subject-specific parameters as correlated themselves using a bi-variate normal distribution. Define $\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})^\\top$ and then $\\boldsymbol{\\theta}_i \\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})$.\n\n$$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n    \\tau_{0}^2 & \\tau_{01}\\\\\n    \\tau_{01} & \\tau_1^2\\\\\n  \\end{bmatrix}.$$\n\n-   $\\tau_{01} = \\rho \\tau_0 \\tau_1$.\n\n-   $\\rho$ is the correlation between the subject-specific intercepts and slopes.\n\n**Let's talk about efficient ways to generate multivariate random variables!**\n\n## Generating Multivariate Normal RNGs {.midi}\n\nSuppose we would like to generate samples of a random variable $\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n\nTo sample efficiently, we can decompose the covariance structure:\n\n\\begin{align*}\n\\boldsymbol{\\Sigma} &= \\begin{bmatrix}\n    \\tau_{0}^2 & \\rho \\tau_0 \\tau_1\\\\\n    \\rho \\tau_0 \\tau_1 & \\tau_1^2\\\\\n  \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    1 & \\rho\\\\\n    \\rho & 1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}\\\\\n&=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}.\n\\end{align*}\n\n-   $\\mathbf{D}$ is a $p$-dimensional matrix with the standard deviations on the diagonal.\n\n-   $\\boldsymbol{\\Phi}$ is the correlation matrix.\n\n## Generating Multivariate Normal RNGs\n\nWe can further decompose the covariance by computing the cholesky decomposition of the correlation matrix:\n\n\\begin{align*}\n\\boldsymbol{\\Sigma} &=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}\\\\\n&= \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D},\n\\end{align*} where $\\mathbf{L}$ is the lower triangular Cholesky decomposition for $\\boldsymbol{\\Phi}$, such that $\\boldsymbol{\\Phi} = \\mathbf{L} \\mathbf{L}^\\top$.\n\n## Generating Multivariate Normal RNGs\n\nWe can generate samples $\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ using the following approach:\n\n$$\\mathbf{x}_i = \\boldsymbol{\\mu} + \\mathbf{D} \\mathbf{L} \\mathbf{z}_i,$$\n\nwhere $\\mathbf{z}_i = (z_{0i},z_{1i})$ and $z_{ij} \\stackrel{iid}{\\sim} N(0,1)$, so that $\\mathbb{E}[\\mathbf{z}_i] = \\mathbf{0}_2$ and $\\mathbb{C}(\\mathbf{z}_i) = \\mathbf{I}_2$.\n\n\\begin{align*}\n\\mathbb{E}[\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i] &= \\boldsymbol{\\mu} +  \\mathbf{D}\\mathbf{L}\\mathbb{E}[\\mathbf{z}_i] = \\boldsymbol{\\mu}\\\\\n\\mathbb{C}(\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i) &= \\mathbf{D}\\mathbf{L}\\mathbb{C}(\\mathbf{z}_i)\\left(\\mathbf{D}\\mathbf{L}\\right)^\\top \\\\\n&= \\mathbf{D}\\mathbf{L}\\mathbf{L}^\\top\\mathbf{D}\\\\\n&=\\boldsymbol{\\Sigma}.\n\\end{align*}\n\n## Generating Multivariate Normal RNGs {.midi}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSigma <- matrix(c(3, 1, 1, 3), nrow = 2, ncol = 2, byrow = TRUE)\nmu <- matrix(c(2, 5), ncol = 1)\nD <- matrix(0, nrow = 2, ncol = 2)\ndiag(D) <- diag(sqrt(Sigma))\nPhi <- cov2cor(Sigma)\nL <- t(chol(Phi))\nn_samples <- 1000\nz <- matrix(rnorm(n_samples * 2), nrow = 2, ncol = n_samples)\nmu_mat <- matrix(rep(mu, n_samples), nrow = 2, ncol = n_samples) \nX <- mu_mat + D %*% L %*% z\napply(X, 1, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.946052 4.950226\n```\n\n\n:::\n\n```{.r .cell-code}\ncov(t(X))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]      [,2]\n[1,] 3.0612065 0.9283299\n[2,] 0.9283299 2.9567120\n```\n\n\n:::\n:::\n\n\n\n## Conditional specification\n\nFor the conditional specification, we can write the model at the observational level, $Y_{it}$. This is because conditionally on the $\\boldsymbol{\\theta}_i$, $Y_{it}$ and $Y_{it'}$ are independent.\n\nFor $i$ ($i = 1,\\ldots,n$) and $t$ ($t = 1,\\ldots, n_i$), the model is:\n\n\\begin{align*}\nY_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}_i &= (\\beta_{0} + \\theta_{0i}) + (\\beta_1 + \\theta_{0i}) X_{it}+ \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma} &\\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{align*}\n\nwhere $\\boldsymbol{\\Omega} = (\\beta_0, \\beta_1, \\sigma^2, \\boldsymbol{\\Sigma})$.\n\n## Conditional Specification\n\n-   Moments for the Conditional Model:\n\n\\begin{align*}\n\\mathbb{E}[Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i] &= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it}\\\\\n\\mathbb{V}(Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{it}, Y_{jt'} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i,\\boldsymbol{\\theta}_{t'}) &= 0,\\quad \\forall i,j,t,t'\n\\end{align*}\n\n<!-- ## Understanding the conditional mean -->\n\n<!-- \\begin{align*} -->\n\n<!-- \\mathbb{E}[Y_{it} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}_i] &= \\beta_{0i} + \\beta_{1i} X_{it}\\\\ -->\n\n<!-- &= (\\beta_0 + \\theta_{0i}) + (\\beta_1 + \\theta_{1i}) X_{it}\\\\ -->\n\n<!-- &= \\beta_0 + \\beta_1 X_{it} + \\theta_{0i} + \\theta_{1i} X_{it}\\\\ -->\n\n<!-- &= \\mathbf{x}_{it} \\boldsymbol{\\beta} + \\mathbf{x}_{it} \\boldsymbol{\\theta}_i, -->\n\n<!-- \\end{align*} -->\n\n<!-- where $\\mathbf{x}_{it} = (1, X_{it})$ and $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1)$. -->\n\n## Conditional Specification\n\n-   Define $\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})$ and $\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)$.\n\n-   The posterior for the conditional model can be written as:\n\n\\begin{align*}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{t = 1}^{n_i} f(Y_{it} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\boldsymbol{\\theta}_i | \\boldsymbol{\\Sigma}) f(\\boldsymbol{\\Omega}),\n\\end{align*} where $\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_1,\\ldots,\\boldsymbol{\\theta}_n)$.\n\n<!-- ## Marginal Specification {.midi} -->\n\n<!-- To derive a marginal model it is useful to write the model at the level of the independent observations, $\\mathbf{Y}_i$. -->\n\n<!-- $$\\mathbf{Y}_i = \\begin{bmatrix} -->\n\n<!--     Y_{i1}\\\\ -->\n\n<!--     Y_{i2}\\\\ -->\n\n<!--     \\vdots\\\\ -->\n\n<!--     Y_{in_i} -->\n\n<!--   \\end{bmatrix} =  -->\n\n<!--   \\begin{bmatrix} -->\n\n<!--     \\mathbf{x}_{i1} \\boldsymbol{\\beta} + \\mathbf{x}_{i1} \\boldsymbol{\\theta}_i + \\epsilon_{i1}\\\\ -->\n\n<!--     \\mathbf{x}_{i2} \\boldsymbol{\\beta} + \\mathbf{x}_{i2} \\boldsymbol{\\theta}_i + \\epsilon_{i2}\\\\ -->\n\n<!--     \\vdots \\\\ -->\n\n<!--     \\mathbf{x}_{in_i} \\boldsymbol{\\beta} + \\mathbf{x}_{in_i} \\boldsymbol{\\theta}_i + \\epsilon_{in_i} -->\n\n<!--   \\end{bmatrix} = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{X}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i,$$ where $\\mathbf{x}_{it} = (1, X_{it})$, $\\mathbf{X}_i$ is an $n_i \\times 2$ dimensional matrix with rows $\\mathbf{x}_{it}$. -->\n\n<!-- -   $\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i}) \\stackrel{ind}{\\sim} N(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i})$. -->\n\n## Marginal Specification\n\nThe LMM model is given by:\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}_i + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\n-   Moments for the Marginal Model:\n\n\\begin{align*}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{align*}\n\n## Marginal Specification\n\nFor $i = 1,\\ldots,n$, \\begin{align*}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\mathbf{X}_i\\boldsymbol{\\beta},\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{align*} where $\\boldsymbol{\\Omega} = (\\boldsymbol{\\beta},\\sigma,\\boldsymbol{\\Sigma})$ are the population parameters.\n\n## Recovering the Subject-Specific Parameters {.midi}\n\n-   We can still recover the $\\boldsymbol{\\theta}_i$ when we fit the marginal model, we only need to compute $f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})$ for all $i$.\n\n-   We can obtain this full conditional by specifying the joint distribution,\n\n$$f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\boldsymbol{\\theta}_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\mathbf{X}_i \\boldsymbol{\\beta} \\\\\n    \\mathbf{0}_{n_1}\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\boldsymbol{\\Upsilon}_i & \\mathbf{Z}_i\\boldsymbol{\\Sigma}\\\\\n    \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top & \\boldsymbol{\\Sigma}\n  \\end{bmatrix}\\right).$$\n\nWe can then use the [conditional specification of a multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions) to find, $f(\\boldsymbol{\\theta}_i | \\mathbf{Y}_i, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\boldsymbol{\\theta}_i},\\mathbb{V}_{\\boldsymbol{\\theta}_i})$, where\n\n\\begin{align*}\n\\mathbb{E}_{\\boldsymbol{\\theta}_i} &= \\mathbf{0}_{n_i} + \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\boldsymbol{\\theta}_i} &= \\boldsymbol{\\Sigma} - \\boldsymbol{\\Sigma} \\mathbf{Z}_i^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{Z}_i\\boldsymbol{\\Sigma}.\n\\end{align*}\n\n## Marginal Specification\n\nIt is not as straightforward to gain intuition behind the induced correlation structure, but we can shed some light by studying the scalar form of the covariance:\n\n\\begin{align*}\n\\mathbb{V}(Y_{it}| \\boldsymbol{\\Omega}) &= \\tau_0^2 + 2 \\tau_{01} X_{it}^2 + \\tau_1^2 X_{it}^2 + \\sigma^2,\\\\\n\\mathbb{C}(Y_{it}, Y_{it'} | \\boldsymbol{\\Omega}) &= \\tau_0^2 - \\rho \\tau_0 \\tau_1 (X_{it} - X_{it'}) + \\tau_1^2 X_{it} X_{it'}.\n\\end{align*}\n\n## Visualizing the dependency\n\n$\\tau_0 = 1,\\tau_1 = 1,\\sigma^2 =2, \\rho = 0.5$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n## Visualizing the dependency\n\n$\\tau_0 = 1,\\tau_1 = 1,\\sigma^2 =2, \\rho = -0.5$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n## Marginal Specification\n\n-   The posterior for the conditional model can be written as:\n\n\\begin{align*}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{align*}\n\n## Specifying a Prior Distribution for $\\boldsymbol{\\Omega}$\n\n-   We must set a prior for $f(\\boldsymbol{\\Omega}) = f(\\boldsymbol{\\beta}) f(\\sigma) f(\\boldsymbol{\\Sigma})$.\n\n-   We can place standard priors on $\\boldsymbol{\\beta}$ and $\\sigma$.\n\n-   $\\boldsymbol{\\Sigma}$ is a covariance (i.e., positive definite matrix), so we must be careful here.\n\n-   It is natural to place a prior on the decomposition, $\\boldsymbol{\\Sigma} = \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D}$.\n\n    -   For each of the standard deviations $(\\tau_0,\\tau_1)$ we can place standard priors for scales (e.g., half-normal).\n\n    -   For $\\mathbf{L}$ we can place a [Lewandowski-Kurowicka-Joe (LKJ)](https://mc-stan.org/docs/functions-reference/correlation_matrix_distributions.html#cholesky-lkj-correlation-distribution) distribution, $\\mathbf{L} \\sim LKJ(\\eta)$.\n\n## What Does the LKJ Prior Do?\n\n-   The LKJ prior allows you to model the correlation structure in a flexible and non-informative way.\n\n-   It is defined by a single parameter, $\\eta > 0$, which controls the concentration of the prior.\n\n    -   When $(\\eta = 1)$, it is an uninformative prior (i.e., uniform on the correlations).\n\n    -   When $(\\eta > 1)$, the prior favors more highly correlated random effects.\n\n    -   When $(\\eta < 1)$, the prior favors weaker correlations.\n\n-   When $q=2$, $\\eta = 1$ is equivalent to $\\rho \\sim \\text{Uniform}(-1,1)$.\n\n## LKJ Prior Formula\n\n-   The LKJ prior on a correlation matrix $\\mathbf{L}$ is given by:\n\n$$f(\\mathbf{L} | \\eta) \\propto \\prod_{j = 2}^q L_{jj}^{q-j+2\\eta-2}.$$\n\nWhere:\n\n-   $\\eta$ is the concentration parameter.\n\n-   $q$ is the size of the correlation matrix.\n\n-   $L_{jk}$ is the observation in the $j$th row and $k$th column of $\\mathbf{L}$.\n\n## LKJ Prior in Stan\n\n\n\n::: {.cell output.var='chol'}\n\n```{.stan .cell-code}\nparameters {\n  cholesky_factor_corr[2] L;  // Cholesky factor of correlation matrix\n}\nmodel {\n  L ~ lkj_corr_cholesky(eta);\n}\n```\n:::\n\n\n\n## Stan code for independent intercept and slope\n\n\n\n::: {.cell output.var='ind'}\n\n```{.stan .cell-code}\n// lmm-independent.stan\ndata {\n  int<lower = 1> n;\n  int<lower = 1> N;\n  vector[N] Time;\n  vector[N] MD;\n  int<lower = 1, upper = n> Ids[N];\n}\nparameters {\n  real beta0;\n  real beta1;\n  real<lower = 0> sigma;\n  vector[n] z0;\n  vector[n] z1;\n  real<lower = 0> tau0;\n  real<lower = 0> tau1;\n}\ntransformed parameters {\n  vector[n] theta0;\n  vector[n] theta1;\n  theta0 = tau0 * z0;\n  theta1 = tau1 * z1;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = (beta0 + theta0[Ids[i]]) + (beta1 + theta1[Ids[i]]) * Time[i];\n  }\n  target += normal_lpdf(MD | mu, sigma);\n  // subject-specific parameters\n  target += std_normal_lpdf(z0);\n  target += std_normal_lpdf(z1);\n  // population parameters\n  target += normal_lpdf(beta0 | 0, 3);\n  target += normal_lpdf(beta1 | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau0 | 0, 3);\n  target += normal_lpdf(tau1 | 0, 3);\n}\n```\n:::\n\n\n\n## Stan code for conditional LMM\n\n\n\n::: {.cell output.var='cond'}\n\n```{.stan .cell-code}\n// lmm-conditional.stan\ndata {\n  int<lower = 1> N;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int<lower = 1> q;\n  matrix[N, p] X;\n  matrix[N, q] Z;\n  vector[N] Y;\n  int<lower = 1, upper = n> Ids[N];\n  real<lower = 0> eta;\n}\nparameters {\n  vector[p] beta;\n  real<lower = 0> sigma;\n  matrix[q, n] z;\n  vector<lower = 0>[q] tau;\n  cholesky_factor_corr[q] L;\n}\ntransformed parameters {\n  matrix[q, n] theta;\n  theta = diag_pre_multiply(tau, L) * z;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = X[i, ] * beta + Z[i, ] * theta[, Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  // subject-specific parameter\n  target += std_normal_lpdf(to_vector(z));\n  // population parameters\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n  target += lkj_corr_cholesky_lpdf(L | eta);\n}\ngenerated quantities {\n  corr_matrix[q] Phi = L * transpose(L);\n  real rho = Phi[1, 2];\n  vector[n] subject_intercepts = beta[1] + to_vector(theta[1, ]);\n  vector[n] subject_slopes = beta[2] + to_vector(theta[2, ]);\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = X[i, ] * beta + Z[i, ] * theta[, Ids[i]];\n    Y_pred[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n  }\n}\n```\n:::\n\n\n\n## Stan code for marginal LMM\n\nNeed [ragged data structure](https://mc-stan.org/docs/stan-users-guide/sparse-ragged.html#ragged-data-structs.section).\n\n\n\n::: {.cell output.var='cond'}\n\n```{.stan .cell-code}\n// lmm-marginal.stan\ndata {\n  int<lower = 1> N;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int<lower = 1> q;\n  matrix[N, p] X;\n  matrix[N, q] Z;\n  vector[N] Y;\n  int<lower = 1> n_is[n];\n  real<lower = 0> eta;\n}\nparameters {\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector<lower = 0>[q] tau;\n  cholesky_factor_corr[q] L;\n}\ntransformed parameters {\n  cov_matrix[q] Sigma;\n  Sigma = diag_pre_multiply(tau, L) * transpose(diag_pre_multiply(tau, L));\n}\nmodel {\n  // evaluate the likelihood for the marginal model using ragged data structure\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    matrix[n_i, p] X_i;\n    matrix[n_i, q] Z_i;\n    for (j in 1:p) X_i[, j] = segment(X[, j], pos, n_i);\n    for (j in 1:q) Z_i[, j] = segment(Z[, j], pos, n_i);\n    vector[n_i] mu_i = X_i * beta;\n    matrix[n_i, n_i] Upsilon_i = (sigma * sigma) * diag_matrix(rep_vector(1.0, n_i)) + Z_i * Sigma * transpose(Z_i);\n    target += multi_normal_lpdf(Y_i | mu_i, Upsilon_i);\n    pos = pos + n_i;\n  }\n  // population parameters\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n  target += lkj_corr_cholesky_lpdf(L | eta);\n}\ngenerated quantities {\n  corr_matrix[q] Phi = L * transpose(L);\n  real rho = Phi[1, 2];\n  matrix[q, n] theta;\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = n_is[i];\n    vector[n_i] Y_i = segment(Y, pos, n_i);\n    matrix[n_i, p] X_i;\n    matrix[n_i, q] Z_i;\n    for (j in 1:p) X_i[, j] = segment(X[, j], pos, n_i);\n    for (j in 1:q) Z_i[, j] = segment(Z[, j], pos, n_i);\n    vector[n_i] mu_i = X_i * beta;\n    matrix[q, n_i] M = Sigma * transpose(Z_i) * inverse_spd(Z_i * Sigma * transpose(Z_i) + (sigma * sigma) * diag_matrix(rep_vector(1.0, n_i)));\n    vector[q] mean_theta_i = M * (Y_i - mu_i);\n    matrix[q, q] cov_theta_i = Sigma - M * Z_i * Sigma;\n    theta[, i] = multi_normal_rng(mean_theta_i, cov_theta_i);\n    pos = pos + n_i;\n  }\n  vector[n] subject_intercepts = beta[1] + to_vector(theta[1, ]);\n  vector[n] subject_slopes = beta[2] + to_vector(theta[2, ]);\n}\n```\n:::\n\n\n\n## Glaucoma Disease Progression Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglaucoma_longitudinal <- readRDS(\"glaucoma_longitudinal.rds\")\nhead(glaucoma_longitudinal)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  pat_id eye_id mean_deviation      time      age      iop\n1      1      1          -7.69 0.0000000 51.55616 10.87303\n2      1      1          -9.95 0.5753425 51.55616 10.87303\n3      1      1          -9.58 1.0547945 51.55616 10.87303\n4      1      1          -9.53 1.5726027 51.55616 10.87303\n5      1      1          -9.18 2.0136986 51.55616 10.87303\n6      1      1          -9.63 2.5671233 51.55616 10.87303\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(unique(glaucoma_longitudinal$eye_id))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 278\n```\n\n\n:::\n\n```{.r .cell-code}\nnrow(glaucoma_longitudinal)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4863\n```\n\n\n:::\n:::\n\n\n\n## Fitting the Conditional Model in Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data <- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  Ids = glaucoma_longitudinal$eye_id,\n  eta = 1\n)\nlmm_conditional <- stan_model(model_code = \"lmm-conditional.stan\")\nfit_lmm_conditional <- sampling(lmm_conditional, stan_data, iter = 5000, pars = c(\"z\", \"theta\"), include = FALSE)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(fit_lmm_conditional, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-16-1.png){width=960}\n:::\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_lmm_conditional, regex_pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## Posterior Summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit_lmm_conditional, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -8.19    0.04 0.47 -9.15 -8.50 -8.19 -7.86 -7.34   178 1.03\nbeta[2] -0.10    0.00 0.03 -0.16 -0.12 -0.10 -0.08 -0.05   778 1.00\nsigma    1.27    0.00 0.01  1.24  1.26  1.26  1.27  1.29 15569 1.00\ntau[1]   8.07    0.02 0.34  7.46  7.83  8.05  8.29  8.79   364 1.01\ntau[2]   0.44    0.00 0.02  0.40  0.42  0.44  0.45  0.48  2064 1.00\nrho     -0.27    0.00 0.06 -0.38 -0.31 -0.27 -0.23 -0.16   949 1.00\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  4 15:18:54 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Fitting the Marginal Model in Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(~ time, data = glaucoma_longitudinal)\nstan_data <- list(\n  N = nrow(glaucoma_longitudinal),\n  n = n_eyes,\n  p = ncol(X),\n  q = ncol(X),\n  X = X,\n  Z = X,\n  Y = glaucoma_longitudinal$mean_deviation,\n  n_is = as.numeric(table(glaucoma_longitudinal$eye_id)),\n  eta = 1\n)\nlmm_marginal <- stan_model(model_code = \"lmm-marginal.stan\")\nfit_lmm_marginal <- sampling(lmm_marginal, stan_data, iter = 5000)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(fit_lmm_marginal, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\nbayesplot::mcmc_acf(fit_lmm_marginal, regex_pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n\n\n## Posterior Summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit_lmm_marginal, pars = c(\"beta\", \"sigma\", \"tau\", \"rho\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=5000; warmup=2500; thin=1; \npost-warmup draws per chain=2500, total post-warmup draws=10000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -8.23       0 0.47 -9.16 -8.55 -8.23 -7.92 -7.31 12896    1\nbeta[2] -0.10       0 0.03 -0.16 -0.12 -0.10 -0.08 -0.05 13230    1\nsigma    1.27       0 0.01  1.24  1.26  1.26  1.27  1.29 13937    1\ntau[1]   8.05       0 0.34  7.42  7.82  8.04  8.27  8.75 14264    1\ntau[2]   0.44       0 0.02  0.40  0.42  0.44  0.45  0.48 13997    1\nrho     -0.27       0 0.06 -0.38 -0.31 -0.28 -0.24 -0.16 12487    1\n\nSamples were drawn using NUTS(diag_e) at Tue Mar  4 11:31:23 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Comparing LMM versus OLS\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-24-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Comparing LMM versus OLS\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-25-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Prepare for next class\n\n-   Work on HW 04, which will be assigned soon. It's not due until March 25.\n\n-   Enjoy your spring break!\n",
    "supporting": [
      "16-longitudinal_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}