{
  "hash": "446c045683c31cf5a52a9c7e5a8dc1b8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Longitudinal Data\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-03-06\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   During our last lecture, we introduced correlated (or dependent) data sources.\n\n-   We discussed the idea of accounting for dependencies within a group using group-specific parameters.\n\n-   We introduced the random intercept model and studied the induced correlation (forced to be positive) in the marginal model.\n\n-   Today we will look at longitudinal data and introduce a simple model that accounts for group-level changes.\n\n## Longitudinal Data\n\nRepeated measurements taken over time from the same subjects. Examples include:\n\n- **Monitor Disease Progression**: Track how diseases evolve, such as diabetes or glaucoma.\n\n- **Evaluate Treatments**: Understand how interventions work over time.\n\n- **Personalized Health Insights**: Capture individual health trajectories for personalized care.\n\n- **Study Long-Term Effects**: Evaluate the long-term outcomes of medical treatments or behaviors.\n\n## Example: Glaucoma Disease Progression\n\nImagine we are tracking mean deviation (MD, dB), a key measure of visual field loss in glaucoma patients, over time.\n\n- Multiple measurements of MD for each patient across several years.\n\n- We're interested in glaucoma progression, which is defined as the rate of change in MD over time (dB/year).\n\n- Define $Y_{it}$ as the MD value for eye $i$ ($i = 1,\\ldots,n$) at time $t$ ($t = 1,\\ldots,n_i$) and the time of each observation as $X_{it}$ with $X_{i0} = 0$.\n\n## Rotterdam data\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Treating Eyes Separately\n\nWe can model each eye **separately** using OLS (this is a form of longitudinal analysis!). For $t = 1,\\ldots,n_i$, the model is:\n\n$$Y_{it} = \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma_i^2).$$\n\nWhere:\n\n- $\\beta_{0i}$ is the intercept for eye $i$.\n\n- $\\beta_{1i}$ is the slope for eye $i$ (i.e., disease progression).\n\n- $\\sigma_i^2$ is the residual error for eye $i$.\n\n## OLS regression\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-longitudinal_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n\n## Treating Eyes Separately\n\n- Fitting OLS separately allows **each eye** to have a unique intercept and slope, which of course is consistent with the data generating process.\n\n- However, this can lead to eye-specific intercepts and slopes that are not realistic (consider OLS regression with very few data points).\n\n- Estimating eye-specific intercepts and slopes within the context of the whole study sample should shrink extreme values toward the population average.\n\n## Subject-specific intercepts and slopes\n\nFor $i = 1,\\ldots,n$ and $t=1,\\ldots,n_i$, we can write the model:\n\n\\begin{align*}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{align*}\n\n**Population Parameters**:\n\n- $\\beta_0$ is the population intercept (i.e., average MD value in the population at time zero).\n\n- $\\beta_1$ is the population slope (i.e., average disease progression).\n\n- $\\sigma^2$ is the population residual error.\n\n## Subject-specific intercepts and slopes\n\nFor $i = 1,\\ldots,n$ and $t=1,\\ldots,n_i$, we can write the model:\n\n\\begin{align*}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{align*}\n\n**Subject-Specific Parameters**:\n\n- $\\theta_{0i}$ is the subject-specific deviation from the intercept for eye $i$.\n\n- $\\theta_{1i}$ is the subject-specific deviation from the slope for eye $i$.\n\n## Subject-specific intercepts and slopes\n\nFor $i = 1,\\ldots,n$ and $t=1,\\ldots,n_i$, we can write the model:\n\n\\begin{align*}\nY_{it} &= \\beta_{0i} + X_{it} \\beta_{1i} + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0, \\sigma^2),\\\\\n\\beta_{0i} &= \\beta_0 + \\theta_{0i},\\\\\n\\beta_{1i} &= \\beta_1 + \\theta_{1i}.\n\\end{align*}\n\n**Key Advantage**:\n\n- This model defines subject-specific estimates of $\\beta_{0i}$ and $\\beta_{1i}$ relative to the population average, preventing overfitting and making the estimates more stable.\n\n- **Shrinks** subject-specific parameters to the population average.\n\n## Prior Specification {.midi}\n\nOne choice could be to specify independent priors for the subject-specific intercepts and slopes:\n\n\\begin{align*}\n\\theta_{0i} &\\stackrel{iid}{\\sim} N(0, \\tau_0^2)\\\\\n\\theta_{1i} &\\stackrel{iid}{\\sim} N(0, \\tau_1^2).\n\\end{align*}\n\n- This is the same assumption we made last lecture, where we assume a normal distribution centered at zero with some variance that reflects variability across subjects.\n\n- Often times this assumption is oversimplified. For example in glaucoma progression, we often assume that if someone has a higher baseline MD they will a more negative slope (i.e., negative correlation).\n\n## Prior Specification {.midi}\n\nWe can instead model the subject-specific parameters as correlated themselves using a bi-variate normal distribution. Define $\\boldsymbol{\\theta}_i = (\\theta_{0i},\\theta_{1i})^\\top$ and then $\\boldsymbol{\\theta}_i \\stackrel{iid}{\\sim} N_2(\\mathbf{0}_2,\\boldsymbol{\\Sigma})$.\n\n$$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n    \\tau_{0}^2 & \\tau_{01}\\\\\n    \\tau_{01} & \\tau_1^2\\\\\n  \\end{bmatrix}.$$\n\n\n- $\\tau_{01} = \\rho \\tau_0 \\tau_1$.\n\n- $\\rho$ is the correlation between the subject-specific intercepts and slopes.\n\n**Let's talk about efficient ways to generate multivariate random variables!**\n\n## Generating Multivariate Normal RNGs {.midi}\n\nSuppose we would like to generate samples of a random variable $\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$.\n\nTo sample efficiently, we can decompose the covariance structure:\n\n\\begin{align*}\n\\boldsymbol{\\Sigma} &= \\begin{bmatrix}\n    \\tau_{0}^2 & \\rho \\tau_0 \\tau_1\\\\\n    \\rho \\tau_0 \\tau_1 & \\tau_1^2\\\\\n  \\end{bmatrix}\\\\\n&= \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    1 & \\rho\\\\\n    \\rho & 1\\\\\n  \\end{bmatrix}  \\begin{bmatrix}\n    \\tau_{0} & 0\\\\\n    0 & \\tau_1\\\\\n  \\end{bmatrix}\\\\\n&=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}.\n\\end{align*}\n\n- $\\mathbf{D}$ is a $p$-dimensional matrix with the standard deviations on the diagonal.\n\n- $\\boldsymbol{\\Phi}$ is the correlation matrix.\n\n## Generating Multivariate Normal RNGs\n\nWe can further decompose the covariance by computing the cholesky decomposition of the correlation matrix:\n\n\\begin{align*}\n\\boldsymbol{\\Sigma} &=  \\mathbf{D} \\boldsymbol{\\Phi} \\mathbf{D}\\\\\n&= \\mathbf{D} \\mathbf{L} \\mathbf{L}^\\top \\mathbf{D},\n\\end{align*}\nwhere $\\mathbf{L}$ is the lower triangular Cholesky decomposition for $\\boldsymbol{\\Phi}$, such that $\\boldsymbol{\\Phi} = \\mathbf{L} \\mathbf{L}^\\top$.\n\n## Generating Multivariate Normal RNGs\n\nWe can generate samples $\\mathbf{x}_i \\stackrel{iid}{\\sim} N_2(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$ using the following approach:\n\n$$\\mathbf{x}_i = \\boldsymbol{\\mu} + \\mathbf{D} \\mathbf{L} \\mathbf{z}_i,$$\n\nwhere $\\mathbf{z}_i = (z_{0i},z_{1i})$ and $z_{ij} \\stackrel{iid}{\\sim} N(0,1)$, so that $\\mathbb{E}[\\mathbf{z}_i] = \\mathbf{0}_2$ and $\\mathbb{C}(\\mathbf{z}_i) = \\mathbf{I}_2$.\n\n\\begin{align*}\n\\mathbb{E}[\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i] &= \\boldsymbol{\\mu} +  \\mathbf{D}\\mathbf{L}\\mathbb{E}[\\mathbf{z}_i] = \\boldsymbol{\\mu}\\\\\n\\mathbb{C}(\\boldsymbol{\\mu} + \\mathbf{D}\\mathbf{L}\\mathbf{z}_i) &= \\mathbf{D}\\mathbf{L}\\mathbb{C}(\\mathbf{z}_i)\\left(\\mathbf{D}\\mathbf{L}\\right)^\\top \\\\\n&= \\mathbf{D}\\mathbf{L}\\mathbf{L}^\\top\\mathbf{D}\\\\\n&=\\boldsymbol{\\Sigma}.\n\\end{align*}\n\n\n## Linear Mixed Model\n\n$Y_{ij} = \\beta_{0i} + \\beta_{1i} x_{ij} + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim} N(0,\\sigma^2)$\n\n## Stan code for independent intercept and slope\n\n\n\n::: {.cell output.var='ind'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> N;\n  vector[N] Time;\n  vector[N] MD;\n  int<lower = 1, upper = n> Ids[N];\n}\nparameters {\n  real beta0;\n  real beta1;\n  real<lower = 0> sigma;\n  vector[n] z0;\n  vector[n] z1;\n  real<lower = 0> tau0;\n  real<lower = 0> tau1;\n}\ntransformed parameters {\n  vector[n] theta0;\n  vector[n] theta1;\n  theta0 = tau0 * z0;\n  theta1 = tau1 * z1;\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = (beta0 + theta0[Ids[i]]) + (beta1 + theta1[Ids[i]]) * Time[i];\n  }\n  target += normal_lpdf(MD | mu, sigma);\n  // subject-specific parameters\n  target += std_normal_lpdf(z0);\n  target += std_normal_lpdf(z1);\n  // population parameters\n  target += normal_lpdf(beta0 | 0, 3);\n  target += normal_lpdf(beta1 | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau0 | 0, 3);\n  target += normal_lpdf(tau1 | 0, 3);\n}\n```\n:::\n\n\n\n## Stan code for dependent intercept and slope \n\n\n\n::: {.cell output.var='ind'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> N;\n  vector[N] Time;\n  vector[N] MD;\n  int<lower = 1, upper = n> Ids[N];\n}\nparameters {\n  real beta0;\n  real beta1;\n  real<lower = 0> sigma;\n  vector[2, n] z;\n  real<lower = 0> tau0;\n  real<lower = 0> tau1;\n  real<lower = -1, upper = 1> rho;\n}\ntransformed parameters {\n  vector[n, 2] theta;\n  cov_matrix[2] Sigma;\n  Sigma[1, 1] = tau0 * tau0;\n  Sigma[2, 1] = rho * tau0 * tau1;\n  Sigma[1, 2] = rho * tau0 * tau1;\n  Sigma[2, 2] = tau1 * tau1;\n  theta = transpose(diag_pre_multiply(tau, L) * z);\n\n}\nmodel {\n  // likelihood\n  vector[N] mu;\n  for (i in 1:N) {\n    mu[i] = (beta0 + theta0[Ids[i]]) + (beta1 + theta1[Ids[i]]) * Time[i];\n  }\n  target += normal_lpdf(MD | mu, sigma);\n  // subject-specific parameters\n  target += std_normal_lpdf(z0);\n  target += std_normal_lpdf(z1);\n  // population parameters\n  target += normal_lpdf(beta0 | 0, 3);\n  target += normal_lpdf(beta1 | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau0 | 0, 3);\n  target += normal_lpdf(tau1 | 0, 3);\n}\n```\n:::\n\n\n\n## Prepare for next class\n\n-   Work on Exam 01, which is due before next Thursday's class!\n\n-   Next Tuesday's class will be office hours. I will be available in the lecture room during the meeting time.\n\n-   Next Thursday's lecture: Longitudinal Data\n",
    "supporting": [
      "16-longitudinal_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}