{
  "hash": "d71481043e4a809982043b144762dc93",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Robust Regression\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-11\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   On Thursday, we started to branch out from linear regression.\n\n-   We learned about approaches for nonlinear regression.\n\n-   Today we will address approaches for robust regression, which will generalize the assumption of homoskedasticity (and also the normality assumption).\n\n## A motivating research question\n\n-   In today's lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\n    -   A detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\n-   For an example patient, we define $Y_i$ as the serum concentration value and $X_i$ as a child's age, given in years.\n\n## Pulling the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5\n```\n\n\n:::\n:::\n\n\n\n## Visualizing IgG data\n\n\n\n::: {.cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=288}\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-3-2.png){fig-align='center' width=288}\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-3-3.png){fig-align='center' width=288}\n:::\n:::\n\n\n\n## Modeling the association between age and IgG\n\n-   Linear regression can be written as follows for $i = 1,\\ldots,n$,\n\n$$Y_i = \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).$$\n\n-   $\\beta$ represent the change in IgG serum concentration with a one year increase in age.\n\n## Linear regression assumptions\n\n\\begin{align*}\nY_i &= \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{align*}\n\nAssumptions:\n\n1.  $Y_i$ are independent observations (independence).\n\n2.  $Y_i$ is linearly related to $X_i$ (linearity).\n\n3.  $\\epsilon_i = Y_i - \\mu_i$ is normally distributed (normality).\n\n4.  $\\epsilon_i$ has constant variance across $X_i$ (homoskedasticity).\n\n## Assessing assumptions\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-4-2.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Robust regression\n\n-   Today we will learn about regression techniques that are robust to the assumptions of linear regression.\n\n-   We will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\n\n-   We will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression).\n\n## Why is robust regression not more comomon?\n\n-   Despite their desirable properties, robust methods are not widely used. **Why?**\n\n    -   Historically computationally complex.\n\n    -   Not available in statistical software packages.\n\n-   Bayesian modeling using Stan alleviates these bottlenecks!\n\n## Heteroskedasticity\n\n-   Heteroskedasticity is the violation of the assumption of constant variance.\n\n-   How can we handle this?\n\n-   In OLS, there are approaches like [heteroskedastic consistent errors](https://en.wikipedia.org/wiki/Heteroskedasticity-consistent_standard_errors), but this is not a generative model.\n\n-   In the Bayesian framework, we generally like to write down generative models.\n\n<!-- ## Weighted regression -->\n\n<!-- -   A common case is **weighted** regression, where each $Y_i$ represents the mean of $n_i$ observations. Then the scale of each observation is, $$\\tau_i^2 = \\sigma^2/n_i,$$ where $\\sigma^2$ is a global scale parameter. -->\n\n<!-- -   Alternatively, suppose each observation represents the sum of each $n_i$ observations. Then the scale of each observation is, $$\\tau_i^2 = n_i \\sigma^2.$$ -->\n\n<!-- ## Weighted regression {.midi} -->\n\n<!-- ```{stan output.var = \"covariates\", eval = FALSE} -->\n\n<!-- data { -->\n\n<!--   int<lower = 1> n; -->\n\n<!--   int<lower = 1> p; -->\n\n<!--   vector[n] Y; -->\n\n<!--   matrix[n, p] X; -->\n\n<!--   int<lower = 1> n_i[n]; -->\n\n<!-- } -->\n\n<!-- parameters { -->\n\n<!--   real alpha; -->\n\n<!--   vector[p] beta; -->\n\n<!--   real<lower = 0> sigma2; -->\n\n<!-- } -->\n\n<!-- transformed parameters { -->\n\n<!--   vector[n] tau2 = sigma2 / n_i; -->\n\n<!-- } -->\n\n<!-- model { -->\n\n<!--   target += normal_lpdf(Y | alpha + X * beta, sqrt(tau2)); -->\n\n<!-- } -->\n\n<!-- ``` -->\n\n## Modeling the scale with covariates {.midi}\n\n-   One option is to allow the sale to be modeled as a function of covariates.\n\n-   It is common to model the log-transformation of the scale or variance to transform it to $\\mathbb{R}$,\n\n$$\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma},$$\n\nwhere $\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})$ are a $p$-dimensional vector of covariates and $\\boldsymbol{\\gamma}$ are parameters that regress the covariates onto the log standard deviation.\n\n-   Other options include: $\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)$\n\n-   Other options include: $\\log \\tau_i = f(\\mu_i)$\n\n-   Any plausible generative model can be specified!\n\n## Modeling the scale with covariates {.midi}\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int<lower = 1> q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n}\n```\n:::\n\n\n\n## Heteroskedastic variance\n\n-   We can write the regression model using a observation specific variance, $$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).$$\n\n-   One way of writing the variance is: $\\tau_i^2 = \\sigma^2 \\lambda_i$.\n\n    -   $\\sigma^2$ is a global scale parameter.\n\n    -   $\\lambda_i$ is an observation specific scale parameter.\n\n-   In the Bayesian framework, we must place a prior on $\\lambda_i$.\n\n<!-- ## Bayesian prior to induce structure -->\n\n<!-- -   Suppse we would like $\\sum_{i=1}^n \\lambda_i = 1$, $\\lambda_i >0$. -->\n\n<!-- -   We could specify the following, -->\n\n<!-- $$\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),$$ -->\n\n<!-- where $\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)$ and $\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)$. -->\n\n<!-- -   The prior mean is $\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0$, where $\\alpha_0 = \\sum_{i=1}^n \\alpha_i.$ -->\n\n<!-- -   Typically, $\\alpha_i = 1 \\forall i$. -->\n\n<!-- ## Dirchlet prior in Stan {.midi} -->\n\n<!-- ```{stan output.var = \"covariates\", eval = FALSE} -->\n\n<!-- data { -->\n\n<!--   int<lower = 1> n; -->\n\n<!--   int<lower = 1> p; -->\n\n<!--   vector[n] Y; -->\n\n<!--   matrix[n, p] X; -->\n\n<!--   vector<lower = 0>[n] alpha; -->\n\n<!-- } -->\n\n<!-- parameters { -->\n\n<!--   vector[p] beta; -->\n\n<!--   real<lower = 0> sigma; -->\n\n<!--   simplex[n] lambda; -->\n\n<!-- } -->\n\n<!-- transformed parameters { -->\n\n<!--   vector[n] tau = sigma * sqrt(lambda); -->\n\n<!-- } -->\n\n<!-- model { -->\n\n<!--   target += normal_lpdf(Y | X * beta, tau); -->\n\n<!--   target += dirichlet_lpdf(lambda | alpha); -->\n\n<!-- } -->\n\n<!-- ``` -->\n\n## A prior to induce a heavy-tail\n\n-   A common prior for $\\lambda_i$ is as follows:\n\n$$\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).$$\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## A prior to induce a heavy-tail\n\n-   A common prior for $\\lambda_i$ is as follows:\n\n$$\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).$$\n\n-   Under this prior, the marginal likelihood for $Y_i$ is equivalent to a Student-t distribution,\n\n$$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).$$\n\n## Understanding the equivalence {.midi}\n\n-   Heteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n$$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).$$\n\n$$\\iff$$\n\n\\begin{align*}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{align*}\n\n-   Note that since the number of $\\lambda_i$ parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution.\n\n## Understanding the equivalence {.midi}\n\n\\begin{align*}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{align*}\n\n-   The marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter.\n\n## Understanding the equivalence {.midi}\n\nA random variable $T_i \\stackrel{iid}{\\sim} t_{\\nu}$ can be written as a function of Gaussian and $\\chi^2$ random variables, \\begin{align*}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), \\quad W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu},\\quad V_i=W_i^{-1}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).\n\\end{align*}\n\n-   We then have: $Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).$\n\n## Student-t in Stan {.midi}\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  real<lower = 0> nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, alpha + X * beta, sigma);\n}\n```\n:::\n\n\n\n## Student-t in Stan: mixture {.midi}\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector[n] lambda;\n  real<lower = 0> nu;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}\n```\n:::\n\n\n\n## Why heavy-tailed distributions?\n\n-   Replacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\n\n-   Robust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\n\n-   Linear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\n\n-   Our heteroskedastic model that we just explored is only one example of a robust regression model.\n\n## Vizualizing heavy tail distributions\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Vizualizing heavy tail distributions\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Vizualizing heavy tail distributions\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Another example of robust regression\n\n-   Let's revisit our general heteroskedastic regression, $$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).$$\n\n-   We can induce another form of robust regression using the following prior for $\\lambda_i$, $\\lambda_i \\sim \\text{Exponential}(1/2)$.\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Another example of robust regression\n\n-   Let's revisit our general heteroskedastic regression, $$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).$$\n\n-   We can induce another form of robust regression using the following prior for $\\lambda_i$, $\\lambda_i \\sim \\text{Exponential}(1/2)$.\n\n-   Under this prior, the induced marginal model is, $$Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} \\text{Laplace}(\\mu = 0, \\sigma).$$\n\n-   **This has a really nice interpretation!**\n\n## Laplace distribution\n\nSuppse a variable $Y_i$ follows a Laplace (or double exponential) distribution, then the pdf is given by,\n\n$$f(Y_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|Y_i - \\mu|}{\\sigma}\\right\\}$$\n\n-   $\\mathbb{E}[Y_i] = \\mu$\n\n-   $\\mathbb{V}(Y_i) = 2 \\sigma^2$\n\n-   Under the Laplace likelihood, estimation of $\\mu$ is equivalent to estimating the population median of $Y_i$.\n\n## Median regression using Laplace\n\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\n$$\\hat{{\\alpha}}_{\\text{LAD}},\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\alpha,\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mu_i|, \\quad \\mu_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}.$$\n\nThe Bayesian analog is the Laplace distribution,\n\n$$f(\\mathbf{Y} | \\alpha, \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mu_i|}{\\sigma}\\right\\}.$$\n\n## Median regression using Laplace\n\n-   The Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is $|xâˆ’\\mu|$, so minimizing the likelihood will also minimize the least absolute distances.\n\n-   Laplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around $\\mu$ with scale $\\sigma$).\n\n-   Thus, a linear regression with Laplace errors is analogous to a median regression,\n\n-   Why is median regression considered more robust than regression of the mean?\n\n## Laplace regression in Stan {.midi}\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | alpha + X * beta, sigma);\n}\n```\n:::\n\n\n\n## Laplace regression in Stan: mixture {.midi}\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}\n```\n:::\n\n\n\n## Returning to IgG\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=1056}\n:::\n:::\n\n\n\n<!-- ## Scale of the residuals -->\n\n<!-- ```{r} -->\n\n<!-- pred_laplace <- apply(rstan::extract(fit_laplace, pars = \"Y_pred\")$Y_pred, 2, mean) -->\n\n<!-- pred_t <- apply(rstan::extract(fit_t, pars = \"Y_pred\")$Y_pred, 2, mean) -->\n\n<!-- pred_normal <- apply(rstan::extract(fit_normal, pars = \"Y_pred\")$Y_pred, 2, mean) -->\n\n<!-- pred_normal_var <- apply(rstan::extract(fit_normal_var, pars = \"Y_pred\")$Y_pred, 2, mean) -->\n\n<!-- mean((ImmunogG$IgG - pred_laplace)^2) -->\n\n<!-- mean((ImmunogG$IgG - pred_t)^2) -->\n\n<!-- mean((ImmunogG$IgG - pred_normal)^2) -->\n\n<!-- mean((ImmunogG$IgG - pred_normal_var)^2) -->\n\n<!-- median(abs(ImmunogG$IgG - pred_laplace)) -->\n\n<!-- median(abs(ImmunogG$IgG - pred_t)) -->\n\n<!-- median(abs(ImmunogG$IgG - pred_normal)) -->\n\n<!-- median(abs(ImmunogG$IgG - pred_normal_var)) -->\n\n<!-- ``` -->\n\n## Posterior of $\\beta$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Model                                | Mean| Lower| Upper|\n|:------------------------------------|----:|-----:|-----:|\n|Laplace                              | 0.73|  0.56|  0.89|\n|Student-t                            | 0.69|  0.55|  0.82|\n|Gaussian                             | 0.69|  0.56|  0.83|\n|Gaussian with Covariates in Variance | 0.76|  0.62|  0.89|\n\n\n:::\n:::\n\n\n\n## Model Comparison\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|                                     | elpd_diff| elpd_loo|   looic|\n|:------------------------------------|---------:|--------:|-------:|\n|Student-t                            |      0.00|  -624.42| 1248.84|\n|Gaussian                             |     -2.01|  -626.43| 1252.86|\n|Gaussian with Covariates in Variance |     -6.09|  -630.51| 1261.02|\n|Laplace                              |    -13.10|  -637.52| 1275.05|\n\n\n:::\n:::\n\n\n\n## Examining the Student-t model fit\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd 2.5% 97.5% n_eff Rhat\nalpha   3.32    0.00 0.21 2.91  3.74  2084    1\nbeta[1] 0.69    0.00 0.07 0.55  0.82  2122    1\nsigma   1.72    0.00 0.10 1.53  1.91  2744    1\nnu      7.80    0.05 2.35 4.40 13.29  2617    1\n\nSamples were drawn using NUTS(diag_e) at Sun Feb  9 14:54:53 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Examining the Student-t model fit\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-21-1.png){width=960}\n:::\n:::\n\n\n\n## Examining the Student-t model fit\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-revealjs/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Summary of robust regression\n\n-   Robust regression techniques can be used when the assumptions of constant variance and/or normality of the residuals do not hold.\n\n-   Heteroskedastic variance can viewed as inducing extreme value distributions.\n\n-   Extreme value regression using Student-t and Laplace distributions are robust to outliers.\n\n-   Laplace regression is equivalent to median regression.\n\n## Prepare for next class\n\n-   Work on [HW 02](https://biostat725-sp25.netlify.app/hw/hw-02), which is due before next class.\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Regularization\n",
    "supporting": [
      "10-robust_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}