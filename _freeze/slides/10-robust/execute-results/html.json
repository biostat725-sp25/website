{
  "hash": "ed6bf9a6eef2d6a70e4bf40b515fff35",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Robust Regression\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-11\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Review of last lecture\n\n-   On Thursday, we started to branch out from linear regression.\n\n-   We learned about approaches for nonlinear regression.\n\n-   Today we will address approaches for robust regression, which will generalize the assumption of homoskedasticity (and also the normality assumption).\n\n## A motivating research question\n\n-   In today's lecture, we will look at data on serum concentration (grams per litre) of immunoglobulin-G (IgG) in 298 children aged from 6 months to 6 years.\n\n    -   A detailed discussion of this data set may be found in Isaacs et al. (1983) and Royston and Altman (1994).\n\n-   For an example patient, we define $Y_i$ as the serum concentration value and $X_i$ as a child's age, given in years.\n\n## Pulling the data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Brq)\ndata(\"ImmunogG\")\nhead(ImmunogG)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  IgG Age\n1 1.5 0.5\n2 2.7 0.5\n3 1.9 0.5\n4 4.0 0.5\n5 1.9 0.5\n6 4.4 0.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Visualizing IgG data\n\n\n\n\n\n\n::: {.cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=288}\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-3-2.png){fig-align='center' width=288}\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-3-3.png){fig-align='center' width=288}\n:::\n:::\n\n\n\n\n\n\n## Modeling the association between age and IgG\n\n-   Linear regression can be written as follows for $i = 1,\\ldots,n$,\n\n$$Y_i = \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2).$$\n\n-   $\\beta$ represent the the change in IgG serum concentration with a one year increase in age.\n\n## Linear regression assumptions\n\n\\begin{align*}\nY_i &= \\alpha + \\beta X_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{align*}\n\nAssumptions:\n\n1.  $Y_i$ are independent observations (independence).\n\n2.  $Y_i$ is linearly related to $X_i$ (linearity).\n\n3.  $\\epsilon_i = Y_i - \\mu_i$ is normally distributed (normality).\n\n4.  $\\epsilon_i$ has constant variance across $X_i$ (homoskedasticity).\n\n## Assessing assumptions\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-4-2.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n\n\n\n## Robust regression\n\n-   Today we will learn about regression techniques that are robust to the assumptions of linear regression.\n\n-   We will introduce the idea of robust regression by exploring ways to generalize the homoskedastic variance assumption in linear regression.\n\n-   We will touch on heteroskedasticity, heavy-tailed distributions, and median regression (more generally quantile regression).\n\n## Heteroskedasticity\n\n-   Heteroskedasticity is the violation of the assumption of constant variance.\n\n-   How can we handle this?\n\n-   In OLS, there are approaches like [heteroskedastic consistent errors](https://en.wikipedia.org/wiki/Heteroskedasticity-consistent_standard_errors), but this is not a generative model.\n\n-   In the Bayesian framework, we generally like to write down generative models.\n\n## Weighted regression\n\n-   A common case is **weighted** regression, where each $Y_i$ represents the mean of $n_i$ observations. Then the scale of each observation is, $$\\tau_i^2 = \\sigma^2/n_i,$$ where $\\sigma^2$ is a global scale parameter.\n\n-   Alternatively, suppose each observation represents the sum of each $n_i$ observations. Then the scale of each observation is, $$\\tau_i^2 = n_i \\sigma^2.$$\n\n## Weighted regression {.midi}\n\n\n\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n  int<lower = 1> n_i[n];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma2;\n}\ntransformed parameters {\n  vector[n] tau2 = sigma2 / n_i;\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, sqrt(tau2));\n}\n```\n:::\n\n\n\n\n\n\n## Modeling the scale with covariates {.midi}\n\n-   The scale can also be modeled with covariates.\n\n-   It is common to model the log-transformation of the scale or variance to transform it to $\\mathbb{R}$,\n\n$$\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma},$$\n\nwhere $\\mathbf{z}_i = (z_{i1},\\ldots,z_{ip})$ are a $p$-dimensional vector of covariates and $\\boldsymbol{\\gamma}$ are parameters that regress the covariates onto the log standard deviation.\n\n-   Other options include: $\\log \\tau_i = \\mathbf{z}_i \\boldsymbol{\\gamma} + \\nu_i,\\quad \\nu_i \\sim N(0, \\sigma^2)$\n\n-   Other options include: $\\log \\tau_i = f(\\mu_i)$\n\n-   Any plausible generative model can be specified!\n\n## Modeling the scale with covariates {.midi}\n\n\n\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int<lower = 1> q;\n  vector[n] Y;\n  matrix[n, p] X;\n  matrix[n, q] Z;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  vector[q] gamma;\n}\ntransformed parameters {\n  vector[n] tau = exp(Z * gamma);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n}\n```\n:::\n\n\n\n\n\n\n## Heteroskedastic variance\n\n-   We can write the regression model using a observation specific variance, $$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\tau_i^2).$$\n\n-   One way of writing the variance is: $\\tau_i^2 = \\sigma^2 \\lambda_i$.\n\n    -   $\\sigma^2$ is a global scale parameter.\n\n    -   $\\lambda_i$ is an observation specific scale parameter.\n\n-   In the Bayesian framework, we must place a prior on $\\lambda_i$.\n\n<!-- ## Bayesian prior to induce structure -->\n\n<!-- -   Suppse we would like $\\sum_{i=1}^n \\lambda_i = 1$, $\\lambda_i >0$. -->\n\n<!-- -   We could specify the following, -->\n\n<!-- $$\\boldsymbol{\\lambda} \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha}),$$ -->\n\n<!-- where $\\boldsymbol{\\lambda} = (\\lambda_1,\\ldots,\\lambda_n)$ and $\\boldsymbol{\\alpha} = (\\alpha_1,\\ldots,\\alpha_n)$. -->\n\n<!-- -   The prior mean is $\\mathbb{E}[\\lambda_i] = \\alpha_i / \\alpha_0$, where $\\alpha_0 = \\sum_{i=1}^n \\alpha_i.$ -->\n\n<!-- -   Typically, $\\alpha_i = 1 \\forall i$. -->\n\n<!-- ## Dirchlet prior in Stan {.midi} -->\n\n<!-- ```{stan output.var = \"covariates\", eval = FALSE} -->\n<!-- data { -->\n<!--   int<lower = 1> n; -->\n<!--   int<lower = 1> p; -->\n<!--   vector[n] Y; -->\n<!--   matrix[n, p] X; -->\n<!--   vector<lower = 0>[n] alpha; -->\n<!-- } -->\n<!-- parameters { -->\n<!--   vector[p] beta; -->\n<!--   real<lower = 0> sigma; -->\n<!--   simplex[n] lambda; -->\n<!-- } -->\n<!-- transformed parameters { -->\n<!--   vector[n] tau = sigma * sqrt(lambda); -->\n<!-- } -->\n<!-- model { -->\n<!--   target += normal_lpdf(Y | X * beta, tau); -->\n<!--   target += dirichlet_lpdf(lambda | alpha); -->\n<!-- } -->\n<!-- ``` -->\n\n## A prior to induce a heavy-tail\n\n-   A common prior for $\\lambda_i$ is as follows:\n\n$$\\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right).$$\n\n-   Under this prior, the marginal likelihood for $Y_i$ is equivalent to a Student-t distribution,\n\n$$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim t_{\\nu}\\left(0, \\sigma\\right).$$\n\n## Understanding the equivalence {.midi}\n\n-   Heteroskedastic variances assumption is equivalent to assuming a heavy-tailed distribution.\n\n$$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} t_{\\nu}\\left(0, \\sigma\\right).$$\n\n$$\\iff$$\n\n\\begin{align*}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N\\left(0,\\sigma^2 \\lambda_i\\right)\\\\\n\\lambda_i &\\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\n\\end{align*}\n\n-   Note that since the number of $\\lambda_i$ parameters is equal to the number of observations, this model will not have a proper posterior distribution without a proper prior distribution.\n\n## Understanding the equivalence {.midi}\n\n\\begin{align*}\nf(Y_i) &= \\int_0^{\\infty} f(Y_i , \\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} f(Y_i | \\lambda_i) f(\\lambda_i) d\\lambda_i\\\\\n&= \\int_0^{\\infty} N(Y_i ; \\mu_i, \\sigma^2 \\lambda_i) \\text{Inverse-Gamma}\\left(\\lambda_i ; \\frac{\\nu}{2},\\frac{\\nu}{2}\\right) d\\lambda_i\\\\\n&= t_{\\nu}\\left(\\mu_i,\\sigma\\right).\n\\end{align*}\n\n-   The marginal likelihood can be viewed as a mixture of a Gaussian likelihood with an Inverse-Gamma scale parameter.\n\n## Understanding the equivalence {.midi}\n\n\\begin{align*}\nT_i &= \\frac{Z_i}{\\sqrt{\\frac{W_i}{\\nu}}},\\quad Z_i \\stackrel{iid}{\\sim} N(0,1), W_i \\stackrel{iid}{\\sim}\\chi^2_{\\nu}\\\\\n&= \\frac{Z_i}{\\sqrt{\\frac{1}{\\nu V_i}}},\\quad V_i \\stackrel{iid}{\\sim} \\text{Inv-}\\chi^2_{\\nu}\\\\\n&= \\sqrt{\\nu V_i} Z_i,\\quad \\lambda_i = \\nu V_i\\\\\n&= \\sqrt{\\lambda_i} Z_i, \\quad \\lambda_i \\stackrel{iid}{\\sim} \\text{Inverse-Gamma}\\left(\\frac{\\nu}{2},\\frac{\\nu}{2}\\right)\\\\\n&\\sim t_{\\nu}\n\\end{align*}\n\n-   We then have: $Y_i = \\mu_i + \\sigma T_i \\sim t_{\\nu}(\\mu_i, \\sigma).$\n\n## Student-t in Stan {.midi}\n\n\n\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  real<lower = 0> nu;\n}\nmodel {\n  target += student_t_lpdf(Y | nu, alpha + X * beta, sigma);\n}\n```\n:::\n\n\n\n\n\n\n## Student-t in Stan: mixture {.midi}\n\n\n\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += inv_gamma_lpdf(lambda | 0.5 * nu, 0.5 * nu);\n}\n```\n:::\n\n\n\n\n\n\n## Why heavy-tailed distributions?\n\n-   Replacing the normal distribution with a distribution with heavy-tails (e.g., Student-t, Laplace) is a common approach to robust regression.\n\n-   Robust regression refers to regression methods which are less sensitive to outliers or small sample sizes.\n\n-   Linear regression, including Bayesian regression with normally distributed errors is sensitive to outliers, because the normal distribution has narrow tail probabilities.\n\n-   Our heteroskedastic model that we just explored is only one example of a robust regression model.\n\n## Vizualizing heavy tail distributions\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n## Vizualizing heavy tail distributions\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n\n```\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'nimble'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:stats':\n\n    simulate\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:base':\n\n    declare\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'extraDistr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:nimble':\n\n    dcat, dinvgamma, pinvgamma, qinvgamma, rcat, rinvgamma\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:purrr':\n\n    rdunif\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n## Another example of robust regression\n\n-   Let's revisit our general heteroskedastic regression, $$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0,\\sigma^2 \\lambda_i).$$\n\n-   We can induce another form of robust regression using the following prior for $\\lambda_i$, $\\lambda_i \\sim \\text{Exponential}(1/2)$.\n\n-   Under this prior, the induced marginal model is, $$Y_i = \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} \\text{Laplace}(\\mu = 0, \\sigma).$$\n\n-   $f(\\epsilon_i | \\mu, \\sigma) = \\frac{1}{2\\sigma} \\exp\\left\\{-\\frac{|\\epsilon_i - \\mu|}{\\sigma}\\right\\}$\n\n## Median regression using Laplace\n\nLeast absolute deviation (LAD) regression minimizes the following objective function,\n\n$$\\hat{\\boldsymbol{\\beta}}_{\\text{LAD}} = \\arg \\min_{\\boldsymbol{\\beta}} \\sum_{i=1}^n |Y_i - \\mathbf{x}_i\\boldsymbol{\\beta}|.$$\n\nThe Bayesian analog is the Laplace distribution,\n\n$f(\\mathbf{Y} | \\boldsymbol{\\beta}, \\sigma) = \\left(\\frac{1}{2\\sigma}\\right)^n \\exp\\left\\{-\\sum_{i=1}^n\\frac{|Y_i - \\mathbf{x}_i \\boldsymbol{\\beta}|}{\\sigma}\\right\\}.$\n\n## Median regression using Laplace\n\n-   The Laplace distribution is analogous to least absolute deviations because the kernel of the distribution is $|xâˆ’\\mu|$, so minimizing the likelihood will also minimize the least absolute distances.\n\n-   Laplace distribution is also known as the double-exponential distribution (symmetric exponential distributions around $\\mu$ with scale $\\sigma$).\n\n-   Thus, a linear regression with Laplace errors is analogous to a median regression,\n\n-   Why is median regression considered more robust than regression of the mean?\n\n## Laplace regression in Stan {.midi}\n\n\n\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n}\nmodel {\n  target += double_exponential_lpdf(Y | alpha + X * beta, sigma);\n}\n```\n:::\n\n\n\n\n\n\n## Laplace regression in Stan: mixture {.midi}\n\n\n\n\n\n\n::: {.cell output.var='covariates'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector[n] lambda;\n}\ntransformed parameters {\n  vector[n] tau = sigma * sqrt(lambda);\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta, tau);\n  target += exponential_lpdf(lambda | 0.5);\n}\n```\n:::\n\n\n\n\n\n\n## Returning to IgG\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Asymmetric Laplace distribution\n\nA random variable, $Y \\sim ALD_p(\\mu,\\sigma)$ is said to follow an asymmetric Laplace distribution (ALD) if the pdf is given by,\n\n$$f(Y) = \\frac{p(1-p)}{\\sigma} \\exp \\left\\{-\\rho_p\\left(\\frac{Y - \\mu}{\\sigma}\\right)\\right\\},$$\n\nwhere $p \\in (0,1)$ is the percentile and $$\\rho_p(x) = x\\left(p - 1(u < 0)\\right) = \\frac{|x| + (2p - 1)x}{2}.$$\n\n-   When $p = 0.5$ it reduces to a regular Laplace distribution.\n\n## General quantile regression\n\n\n\n\n\n\n::: {.cell output.var='ald'}\n\n```{.stan .cell-code}\nfunctions {\n  real asym_laplace_lpdf(real y, real mu, real sigma, real tau) {\n    return log(tau) + log1m(tau)\n      - log(sigma)\n      - 2 * ((y < mu) ? (1 - tau) * (mu - y) : tau * (y - mu)) / sigma;\n  }\n}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real<lower = 0, upper = 1> tau;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n}\nmodel {\n  for (i in 1:n) target += asym_laplace_lpdf(Y[i] | alpha + X[i, ] * beta, sigma, tau);\n}\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n## Quantile regression\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-robust_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Posterior of $\\beta_1$\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| quantile|      mean|     lower|     upper|\n|--------:|---------:|---------:|---------:|\n|    0.025| 0.3291804| 0.2639468| 0.3899443|\n|    0.250| 0.5130600| 0.3788182| 0.6523600|\n|    0.500| 0.7268102| 0.5705129| 0.8836400|\n|    0.750| 0.8093408| 0.6919899| 0.9363650|\n|    0.975| 1.1741866| 0.9783041| 1.3936860|\n\n\n:::\n:::\n\n\n\n\n\n\n## Scale-mixture representation\n\nThe above may also be written as a mixture of exponential and normal distributions. Letting, $Z_i \\sim Exponential(1)$ and $\\sigma \\sim Exponential(1)$.\n\n$$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\sigma \\theta Z_i + \\epsilon_i,\\quad \\epsilon_i \\sim N\\left(0, \\tau^2 \\sigma^2 Z_i\\right),$$\n\nwhere $$\\theta = \\frac{1-2p}{p(1-p)},\\quad\\tau = \\sqrt{\\frac{2}{p(1-p)}}.$$\n\n## Scale-mixture in Stan\n\n\n\n\n\n\n::: {.cell output.var='ald'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n  real<lower = 0, upper = 1> q;\n}\ntransformed data {\n  real theta = (1 - 2 * q) / (q * (1 - q));\n  real tau = sqrt(2 / (q * (1 - q)));\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector<lower=0>[n] z;\n}\nmodel {\n  target += normal_lpdf(Y | alpha + X * beta + sigma * theta * z, tau * sqrt(z) * sigma);\n  target += exponential_lpdf(sigma | 1);\n  target += exponential_lpdf(z | 1);\n}\n```\n:::\n\n\n\n\n\n\n## Prepare for next class\n\n-   Work on [HW 02](https://biostat725-sp25.netlify.app/hw/hw-02), which is due before next class.\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Regularization\n",
    "supporting": [
      "10-robust_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}