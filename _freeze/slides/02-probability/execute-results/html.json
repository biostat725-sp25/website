{
  "hash": "79b474383627438f2bc71d3c84a41a6b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Probability and Bayesian Statistics\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-01-14\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Agenda\n\n-   Review of probability concepts\n\n-   Introduction to Bayesian linear regression\n\n-   Monte Carlo sampling\n\n# Bayesian Linear Regression: Closed Form Posterior\n\n## Defining the model\n\nSuppose we have an observation $Y_i$ for subject $i$ ($i=1,\\ldots,n$), that is modeled as follows,\n\n\\begin{align*}\nY_i &= \\beta_0 + x_{i1} \\beta_1 + \\cdots + x_{ip} \\beta_p + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2)\\\\\n&= \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i.\n\\end{align*}\n\n-   $\\mathbf{x}_i = (1, x_{i1},\\ldots,x_{ip})$ is a $(p+1)$-dimensional row vector of covariates (and intercept).\n-   $\\boldsymbol{\\beta} = (\\beta_0, \\beta_1,\\ldots,\\beta_p)^\\top$ is a $(p+1)$-dimensional column vector of population regression parameters.\n-   $\\epsilon_i$ is a Gaussian measurement error term with variance $\\sigma^2$.\n-   For the purpose of today's lecture, we assume $\\sigma$ is known.\n\n## Defining the likelihood\n\nThe individual likelihood contribution for subject $i$ is given by, $$Y_i|\\boldsymbol{\\beta} \\sim N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2) \\Leftrightarrow f(Y_i|\\boldsymbol{\\beta}) = N(\\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2),$$ and the full data likelihood (or observed data likelihood) is given by, $$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n f(Y_i|\\boldsymbol{\\beta}),$$ where $\\mathbf{Y} = (Y_1,\\ldots,Y_n)$.\n\n## Defining the likelihood (matrix version)\n\nWe can also write the likelihood directly, $$\\mathbf{Y} | \\boldsymbol{\\beta} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\sigma^2 \\mathbf{I}_n),$$ where $\\mathbf{X}$ is an $n \\times (p + 1)$ dimensional matrix with row $\\mathbf{x}_i$ and $\\mathbf{I}_n$ is an $n$-dimensional identity matrix. Thus, the mean of the observed data is modeled as a linear function of the parameters,\n\n$$\n\\mathbb{E}[ \\mathbf{Y} | \\boldsymbol{\\beta} ]  =\n  \\begin{bmatrix}\n    1 & x_{12} & \\ldots & x_{1p} \\\\\n    1 & x_{22} & \\ldots & x_{2p} \\\\\n    \\vdots & \\vdots & & \\vdots\\\\\n    1 & x_{n2} & \\ldots & x_{np}\n  \\end{bmatrix}\n    \\begin{bmatrix}\n    \\beta_0\\\\\n    \\beta_1\\\\\n    \\vdots\\\\\n    \\beta_p\n  \\end{bmatrix} = \\mathbf{X} \\boldsymbol{\\beta}.\n$$\n\n## Linear regression estimation\n\n-   Ordinary least squares (OLS)\n\n<center>$\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmin}} || \\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}||^2$</center>\n\n-   Maximum likelihood estimation (MLE)\n\n<center>$\\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = \\underset{\\boldsymbol{\\beta}}{\\operatorname{argmax}} f(\\mathbf{Y} | \\boldsymbol{\\beta})$</center>\n\n-   $\\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y}$\n\n## Bayesian estimation\n\nEstimation in a Bayesian setting is uniquely encoded through the posterior, \\begin{align*}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\frac{f(\\mathbf{Y}, \\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{f(\\mathbf{Y})}\\\\\n&= \\frac{f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})}{\\int f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})d\\boldsymbol{\\beta}}.\n\\end{align*}\n\nTo compute the posterior distribution, we need the prior $f(\\boldsymbol{\\beta})$.\n\n## Prior definition\n\nLet's assume that the prior for $\\boldsymbol{\\beta}$ is Gassian,\n\n$$f(\\boldsymbol{\\beta}) = N(\\boldsymbol{\\beta}_0,\\sigma_{\\beta}^2 \\mathbf{I}_{p+1}).$$\n\n-   $\\boldsymbol{\\beta}_0$ is the prior mean (i.e., our a-priori guess for the likely value of $\\boldsymbol{\\beta}$)\n-   $\\sigma_{\\beta}^2$ is the prior variance (i.e., encodes our certainty for our a-priori guess)\n\n## Computing the posterior\n\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\begin{align*}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{align*}\n\n## Techniques to find this posterior\n\n-   Brute force: complete the square\n\n-   Easy: kernel recognition\n\n::: callout-note\n## Definition\n\nThe part of the pdf/pmf that depends on the variable is called the **kernel**.\n\nExample:\n\n-   The kernel of the multivariate normal pdf, $\\mathbf{Y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})$, is\n\n\\begin{align*}\nf(\\mathbf{Y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) &\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)^\\top \\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{Y} - \\boldsymbol{\\mu}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}\n\\end{align*}\n:::\n\n## Using the kernel to find the posterior {.small}\n\n\\begin{align*}\nf(\\boldsymbol{\\beta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y} | \\boldsymbol{\\beta}) f(\\boldsymbol{\\beta})\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)^\\top\\left(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}\\right)}{\\sigma^2}\\right]\\right\\}\\\\\n&\\quad \\times \\exp\\left\\{-\\frac{1}{2}\\left[\\frac{\\left(\\boldsymbol{\\beta} - \\boldsymbol{\\beta}_0\\right)^\\top\\left(\\boldsymbol{\\beta}_0 - \\boldsymbol{\\beta}_0\\right)}{\\sigma_{\\beta}^2}\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\left(\\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2} + \\frac{\\mathbf{I}_n}{\\sigma_{\\beta}^2} \\right)\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\left(\\frac{\\mathbf{X}^\\top\\mathbf{Y}}{\\sigma^2} + \\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2}\\right)\\right]\\right\\}\\\\\n&\\propto \\exp\\left\\{-\\frac{1}{2}\\left[\\boldsymbol{\\beta}\\mathbf{A}\\boldsymbol{\\beta} - 2\\boldsymbol{\\beta}^\\top\\mathbf{a}\\right]\\right\\}\n\\end{align*}\n\nThis is the kernel of a multivariate normal for $\\boldsymbol{\\beta}$, with $\\mathbf{A} = \\boldsymbol{\\Sigma}^{-1}$ and $\\mathbf{a} = \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}$. It's easy to see then that, $f(\\boldsymbol{\\beta} | \\mathbf{Y}) = N(\\mathbf{A}^{-1}\\mathbf{a},\\mathbf{A}^{-1}).$\n\n::: callout-note\n## Recall the kernel for the multivariate normal: $\\exp\\left\\{-\\frac{1}{2}\\left[\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1}\\mathbf{Y} - 2\\mathbf{Y}^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}\\right]\\right\\}$\n:::\n\n## Back to the posterior\n\nWith this prior, the posterior can be found in closed-form. The posterior is Gaussian with the following moments: \\begin{align*}\n\\mathbb{V}(\\boldsymbol{\\beta} | \\mathbf{Y}) &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\\\\n\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] &= \\left(\\frac{\\mathbf{I}_{p+1}}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{X}}{\\sigma^2}\\right)^{-1}\\left(\\frac{\\boldsymbol{\\beta}_0}{\\sigma_{\\beta}^2} + \\frac{\\mathbf{X}^\\top \\mathbf{Y}}{\\sigma^2}\\right)\n\\end{align*}\n\n. . .\n\n-   $\\sigma_{\\beta}^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = (\\mathbf{X}^\\top \\mathbf{X})^{-1}\\mathbf{X}^\\top \\mathbf{Y} = \\hat{\\boldsymbol{\\beta}}_{\\text{OLS}} = \\hat{\\boldsymbol{\\beta}}_{\\text{MLE}}$\n\n. . .\n\n-   $\\sigma^2 \\rightarrow \\infty: \\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}] = \\boldsymbol{\\beta}_0$\n\n## How can we use the posterior?\n\nLet's simulate some data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###True parameters\nsigma <- 1.5 # true measurement error\nbeta <- matrix(c(-1.5, 3), ncol = 1) # true beta\n\n###Simulation settings\nn <- 100 # number of observations\np <- length(beta) - 1 # number of covariates\n\n###Simulate data\nset.seed(54) # set seed\nX <- cbind(1, matrix(rnorm(n * p), ncol = p))\nY <- as.numeric(X %*% beta + rnorm(n, 0, sigma))\n```\n:::\n\n\n\n## Visualize simulated data\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-probability_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](02-probability_files/figure-revealjs/unnamed-chunk-3-2.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Inspecting the prior\n\nFirst, we define hyperparameteters.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta0 <- matrix(0, nrow = p + 1, ncol = 1)\nsigma_beta <- 10\n```\n:::\n\n::: {.cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-probability_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](02-probability_files/figure-revealjs/unnamed-chunk-5-2.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](02-probability_files/figure-revealjs/unnamed-chunk-5-3.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Comparison with OLS/MLE\n\nCompute posterior moments\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar_beta <- chol2inv(chol(t(X) %*% X / sigma^2 + diag(p + 1) / sigma_beta^2))\nmean_beta <- var_beta %*% (beta0 / sigma_beta^2 + t(X) %*% Y / sigma^2)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> parameter </th>\n   <th style=\"text-align:right;\"> true </th>\n   <th style=\"text-align:right;\"> bayes </th>\n   <th style=\"text-align:right;\"> ols/mle </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> intercept </td>\n   <td style=\"text-align:right;\"> -1.5 </td>\n   <td style=\"text-align:right;\"> -1.475944 </td>\n   <td style=\"text-align:right;\"> -1.476236 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> slope </td>\n   <td style=\"text-align:right;\"> 3.0 </td>\n   <td style=\"text-align:right;\"> 3.296296 </td>\n   <td style=\"text-align:right;\"> 3.296984 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n<!-- ## Summarizing the posterior -->\n\n<!-- - Summarizing a posterior (with known parametric form) is straightforward for basic quantities of interest -->\n\n<!-- 1. Posterior means, medians, modes, and variances -->\n\n<!-- 2. Joint, conditional, and marginal probabilities, for example:  $P(\\beta_j < c | \\mathbf{Y})$ -->\n\n<!-- 3. $\\alpha$-quantiles: $\\{q_{\\alpha} : P(\\beta_j < q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)$ -->\n\n<!-- 4. $\\ldots$ -->\n\n## Summarizing the posterior\n\n-   Posteriors are often not available in closed form.\n\n-   Even when we have a closed form posterior, it can be difficult to compute summaries of interest.\n\n-   For example, consider $P(\\beta_j < c |\\mathbf{Y})$. What are our options to calculate this probability?\n\n    -   Direct integration (by hand)\n\n    -   Numerical integration/software packages (e.g., $\\texttt{pnorm}$)\n\nThese methods work well for standard posterior quantities and distributions\n\n## Summarizing the posterior\n\nHowever, sometimes we will want to summarize other aspects of a posterior distribution\n\n-   $P(\\beta_j \\in A|\\mathbf{Y})$ for some arbitrary set $A$\n\n-   Means and standard deviations of some function of $\\beta_j$, $g\\left(\\beta_j\\right)$\n\n-   The posterior distribution of functions of many parameters:\n\n    -   $|\\beta_1 - \\beta_2|$, $\\beta_1/\\beta_2$, $\\max\\left\\{\\beta_1,\\ldots,\\beta_p\\right\\}$, $\\dots$\n\n-   Obtaining exact values for these posterior quantities can be difficult or even impossible\n\nWhat are our options?\n\n# Introduction to Monte Carlo approximation\n\n## Monte Carlo (MC) approximation\n\n-   Integration method based on random sampling\n\n-   The general principles and procedures remain relatively constant across a broad class of problems\n\n-   Suppose we have $S$ iid samples from our posterior distribution: $\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)$\n\n-   Then the empirical distribution of $\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}$ would approximate $f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)$, with the approximation improving as $S$ increases\n\n-   The empirical distribution of $\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}$ is known as a MC approximation to $f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)$\n\n## MC approximation\n\nLet $g\\left(\\boldsymbol{\\beta}\\right)$ be (just about) any function of $\\boldsymbol{\\beta}$. The law of large numbers says that if,\n\n$$\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right),$$ then, $$\\frac{1}{S}\\sum_{s=1}^S g\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\boldsymbol{\\beta}\\right)f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)d\\boldsymbol{\\beta},$$ as $S\\rightarrow \\infty$.\n\n## MC approximation\n\n-   Implications (as $S\\rightarrow \\infty$):\n\n    -   $\\overline{\\boldsymbol{\\beta}}=\\frac{1}{S}\\sum_{s=1}^S \\boldsymbol{\\beta}^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{\\beta}|\\mathbf{Y}\\right]$\n\n    -   $\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)$\n\n    -   $\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\boldsymbol{\\beta}\\leq \\mathbf{c} | \\mathbf{Y}\\right)$\n\n    -   $\\alpha$-quantile of $\\left\\{\\boldsymbol{\\beta}^{\\left(1\\right)},\\ldots,\\boldsymbol{\\beta}^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}$\n\n-   Just about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample\n\n## Posterior inference for arbitrary functions\n\nInterest in the posterior distribution of a function of $\\boldsymbol{\\beta}$, $g\\left(\\boldsymbol{\\beta}\\right)$\n\n-   MC sampling plan:\n\n    -   Draw $\\boldsymbol{\\beta}^{\\left(1\\right)}$ from $f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)$, calculate $g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right)$\n\n        $\\ldots$\n\n    -   Draw $\\boldsymbol{\\beta}^{\\left(S\\right)}$ from $f\\left(\\boldsymbol{\\beta}|\\mathbf{Y}\\right)$, calculate $g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)$\n\n    $\\Rightarrow g\\left(\\boldsymbol{\\beta}^{\\left(1\\right)}\\right),\\ldots,g\\left(\\boldsymbol{\\beta}^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\boldsymbol{\\beta}\\right)|\\mathbf{Y}\\right)$\n\n-   Therefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)\n\n## How many samples to take? {.small}\n\nWe can use a central limit theorem: $\\sqrt{S}\\left(\\overline{\\boldsymbol{\\beta}}-\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)$,\n\nwhere $\\sigma^2 = \\mathbb{V}\\left(\\overline{\\boldsymbol{\\beta}}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\boldsymbol{\\beta}^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\boldsymbol{\\beta} | \\mathbf{Y}\\right)$.\n\n$\\implies \\overline{\\boldsymbol{\\beta}}\\approx N\\left(\\mathbb{E}[\\boldsymbol{\\beta} | \\mathbf{Y}],\\sigma^2/S\\right)$\n\n-   MC standard error: $\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}$, $\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\boldsymbol{\\beta}^{\\left(s\\right)}-\\overline{\\boldsymbol{\\beta}}\\right)^2$\n\n-   Approximate 95% MC confidence interval for the posterior mean: $\\overline{\\boldsymbol{\\beta}} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}$\n\n-   Choose $S$ large enough to report the posterior mean with your desired precision\n\n-   Reporting MC standard errors for the posterior mean is a good way to indicate that $S$ is large enough\n\n## Returning to linear regression\n\nLet's obtain $S = 1,000$ samples from our posterior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mvtnorm) # multivariate rng\nbeta_samples <- rmvnorm(1000, mean_beta, var_beta)\n```\n:::\n\n\n\nWe can compute the posterior mean and variance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(apply(beta_samples, 2, mean))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.474821  3.298521\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(apply(beta_samples, 2, var))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02316256 0.02189517\n```\n\n\n:::\n:::\n\n\n\n## Assessing accuracy\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-probability_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Additional posterior summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# median\napply(beta_samples, 2, median)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -1.475117  3.292516\n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% credible intervals\napply(beta_samples, 2, function(x) quantile(x, probs = c(0.025, 0.975)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]     [,2]\n2.5%  -1.763541 3.017254\n97.5% -1.188284 3.590548\n```\n\n\n:::\n\n```{.r .cell-code}\n# evaluating probability\nmean(beta_samples[, 1] < -1.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4358\n```\n\n\n:::\n\n```{.r .cell-code}\n# summarizing arbitrary functions of the parameters\nbeta_new <- beta_samples[, 1] * beta_samples[, 2]^3\nc(mean(beta_new), quantile(beta_new, probs = c(0.025, 0.975)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               2.5%     97.5% \n-53.15221 -71.43863 -38.00378 \n```\n\n\n:::\n:::\n",
    "supporting": [
      "02-probability_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}