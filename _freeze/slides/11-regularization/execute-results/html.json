{
  "hash": "8c2112a104a38a0f4a97b2bf950ec8ee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Regularization\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-13\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   On Tuesday, we learned about robust regression.\n\n    -   Heteroskedasticity\n\n    -   Heavy-tailed distributions\n\n    -   Quantile regression\n\n-   These were all models for the observed data $Y_i$.\n\n-   Today, we will focus on prior specifications for $\\boldsymbol{\\beta}$.\n\n## Sparsity in regression problems\n\n-   Supervised learning can be cast as the problem of estimating a set of coefficients $\\boldsymbol{\\beta} = \\{\\beta_j\\}_{j=1}^{p}$ that determines some functional relationship between a set of $\\{x_j\\}_{j = 1}^p$ and a target variable $y$.\n\n-   This is a central focus of statistics and machine learning.\n\n-   Challenges arise in \"large-$p$\" problems where, in order to avoid overly complex models that predict poorly, some form of dimension reduction is needed.\n\n-   Finding a sparse solution, where some $\\beta_j$ are zero, is desirable.\n\n## Bayesian sparse estimation\n\n-   From a Bayesian-learning perspective, there are two main sparse-estimation alternatives: discrete mixtures and shrinkage priors.\n\n-   Discrete mixtures have been very popular, with the spike-and-slab prior being the gold standard.\n\n    -   Easy to force $\\beta_j$ to exactly zero, but require discrete parameter specification.\n\n-   Shrinkage priors force $\\beta_j$ to zero using regularization, but struggle to get exact zeros.\n\n    -   In recent years, shrinkage priors have become dominant in Bayesian sparsity priors.\n\n## Horseshoe prior\n\n-   Let's assume $\\mathbf{Y} \\stackrel{}{\\sim}N\\left(\\mathbf{X}\\boldsymbol{\\beta},\\sigma^2\\mathbf{I}_n\\right)$, where $\\boldsymbol{\\beta}$ is assumed to be sparse.\n\n-   The horseshoe prior is specified as,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{align*} where $\\mathcal C^+(0, 1)$ is a half-Cauchy distribution for the standard deviation $\\lambda_j$.\n\n## Half-Cauchy distribution\n\nA random variable $X \\sim \\mathcal C^+(\\mu,\\sigma)$ follows a half-Cauchy distribution with location $\\mu$ and scale $\\sigma > 0$ and has the following density,\n\n$$f(X | \\mu, \\sigma) = \\frac{2}{\\pi \\sigma}\\frac{1}{1 + (X - \\mu)^2 / \\sigma^2},\\quad X \\geq \\mu$$\n\n-   The Half-Cauchy distribution with $\\mu = 0$ is a useful prior for non-negative parameters that may be very large, as allowed by the very heavy tails of the Half-Cauchy distribution.\n\n## Half-Cauchy distribution in Stan\n\nIn Stan, the half-Cauchy distribution can be specified by putting a constraint on the parameter definition.\n\n\n\n::: {.cell output.var='half-cauchy'}\n\n```{.stan .cell-code}\nparameters {\n  real<lower = 0> lambda;\n}\nmodel {\n  target += cauchy_lpdf(lambda | 0, 1);\n}\n```\n:::\n\n\n\n## Horseshoe prior\n\n-   The horseshoe prior is specified as,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j &\\sim \\mathcal C^+(0, 1),\n\\end{align*} where $\\mathcal C^+(0, 1)$ is a half-Cauchy distribution for the standard deviation $\\lambda_j$.\n\n-   $\\lambda_j$'s are the *local* shrinkage parameters.\n\n-   $\\tau$ is the *global* shrinkage parameter.\n\n## Horseshoe prior\n\nThe horseshoe prior has two interesting features that make it particularly useful as a shrinkage prior for sparse problems.\n\n1.  It has flat, Cauchy-like tails that allow strong signals to remain large (that is, un-shrunk) a posteriori.\n\n2.  It has an infinitely tall spike at the origin that provides severe shrinkage for the zero elements of $\\boldsymbol{\\beta}$.\n\nAs we will see, these are key elements that make the horseshoe an attractive choice for handling sparse vectors.\n\n## Relation to other shrinkage priors\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau &\\sim N(0, \\lambda_j^2 \\tau^2)\\\\\n\\lambda_j^2 &\\sim f(\\lambda_j)\n\\end{align*}\n\n1.  $\\lambda_j = \\lambda$, implies a Gaussian prior for $\\beta_j$ (Ridge regression).\n\n2.  $f(\\lambda_j) = \\text{Exponential}(2)$, implies independent Laplacian priors for $\\beta_j$ (LASSO).\n\n3.  $f(\\lambda_j) = \\text{Inverse-Gamma}(a,b)$, implies independent Student-t priors for $\\beta_j$ (relevance vector machine).\n\n## Horsehoe density\n\n![](images/10/carvalho1.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"4.5in\"}\n\n[From Carvalho 2009](https://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf)\n\n## Shrinkage of each prior\n\n-   Define the posterior mean of $\\beta_j$ as $\\bar{\\beta}_j$ and the maximum likelihood estimator for $\\beta_j$ as $\\hat{\\beta}_j$.\n\n-   The following relationship holds: $\\bar{\\beta}_j = (1 - \\kappa_j) \\hat{\\beta}_j$,\n\n$$\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}s_j^2\\lambda_j^2}.$$\n\n-   $\\kappa_j$ is called the shrinkage factor for $\\beta_j$.\n\n-   $s_j^2 = \\mathbb{V}(x_j)$ is the variance for each predictor.\n\n## Standardization of predictors\n\n-   In regularization problems, predictors are standardized (to mean zero and standard deviation one).\n\n-   This means that so that $s_j = 1$.\n\n-   Shrinkage parameter:\n\n$$\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}\\lambda_j^2}.$$\n\n-   $\\kappa_j = 1$, implies complete shrinkage.\n\n-   $\\kappa_j = 0$, implies no shrinkage.\n\n## Shrinkage parameter\n\n![](images/10/carvalho2.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"5in\"}\n\n[From Carvalho 2009](https://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf)\n\n## Horseshoe shrinkage parameter\n\n-   Choosing $\\lambda_j âˆ¼ \\mathcal C^+(0, 1)$ implies $\\kappa_j âˆ¼ \\text{Beta}(0.5, 0.5)$, a density that is symmetric and unbounded at both 0 and 1.\n\n-   This horseshoe-shaped shrinkage profile expects to see two things a priori:\n\n    1.  Strong signals ($\\kappa \\approx 0$, no shrinkage), and\n\n    2.  Zeros ($\\kappa \\approx 1$, total shrinkage).\n\n## Spike-and-slab prior\n\n-   The prior is often written as a two-component mixture of Gaussians,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, c^2, \\epsilon &\\sim \\lambda_j N(0, c^2) + (1-\\lambda_j) N(0,\\omega^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{align*}\n\n-   $\\omega \\ll c$ and the indicator variable $\\lambda_j \\in \\{0, 1\\}$ denotes whether $\\beta_j$ is close to zero (comes from the \"spike\", $\\lambda_j = 0$) or nonzero (comes from the \"slab\", $\\lambda_j = 1$).\n\n## Spike-and-slab prior\n\n-   Often $\\omega = 0$ (the spike is a true spike), and the prior can be written as,\n\n\\begin{align*}\n\\beta_j | \\lambda_j, c^2 &\\sim N(0, \\lambda_j^2 c^2)\\\\\n\\lambda_j &\\sim \\text{Bernoulli}(\\pi).\n\\end{align*}\n\n-   Instead of giving continuous priors for $\\lambda_j$'s as in the horseshoe, here only two values are allowed (0,1).\n\n-   The shrinkage factor $\\kappa_j$ only has mass at $\\kappa_j = \\frac{1}{1 + n\\sigma^{-2}\\tau^{2}}$ and $\\kappa_j = 1$ with probabilities $\\pi$ and $1-\\pi$,\n\n## Similarity to horseshoe {.midi}\n\n-   Letting $c \\rightarrow \\infty$, all the mass is concentrated at the extremes $\\kappa_j = 0$ and $\\kappa_j = 1$ (this resembles the horseshoe).\n\n![](images/10/aki1.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"3in\"}\n\n[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)\n\n-   The horseshoe can be seen as a continuous approximation to the spike-and-slab prior as $c \\rightarrow \\infty$.\n\n## Regularized horseshoe prior\n\n\\begin{align*}\n\\beta_j | \\lambda_j, \\tau, c &\\sim N\\left(0, \\tau^2 \\tilde{\\lambda}_j^2\\right),\\quad \\tilde{\\lambda}_j^2 = \\frac{c^2 \\lambda_j^2}{c^2 + \\tau^2 \\lambda_j^2},\\\\\n\\lambda_j &\\sim \\mathcal C^+(0,1).\n\\end{align*}\n\n-   When $\\tau^2 \\lambda_j^2 \\ll c^2$ (i.e., $\\beta_j$ close to zero), $\\beta_j \\sim  N\\left(0, \\tau^2\\lambda_j^2\\right)$\n\n-   When $\\tau^2 \\lambda_j^2 \\gg c^2$, (i.e., $\\beta_j$ far from zero), $\\beta_j \\sim  N\\left(0, c^2\\right)$\n\n-   $c \\rightarrow \\infty$ recovers the original horseshoe.\n\n**Why is this an appealing extension?**\n\n## Regularized horseshoe compared to spike-and-slab\n\n-   The regularized horseshoe prior is comparable to the spike-and-slab with finite $c$.\n\n![](images/10/aki2.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"3in\"}\n\n[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)\n\n## Choosing a prior for $c^2$\n\n-   Unless substantial knowledge about the scale of the relevant coefficients exists, it is recommended to place a prior for $c$ instead of fixing it.\n\n-   Often a reasonable choice is, $$c^2 \\sim \\text{Inv-Gamma}(\\alpha, \\beta), \\quad \\alpha = \\nu/2, \\beta = \\nu s^2 / 2,$$\n\n-   This translates to a $t_{\\nu}(0,s^2)$ slab for the coefficients far from 0.\n\n-   Another motivation for using inverse-Gamma is that it has a heavy right tail accompanied by a light left tail thereby preventing much mass from accumulating near zero.\n\n## Choosing a prior for $\\tau$\n\n-   Carvalho et al. 2009 suggest $\\tau \\sim \\mathcal C^+(0,1)$.\n\n-   Polson and Scott 2011 recommend $\\tau | \\sigma \\sim \\mathcal C^+(0, \\sigma^2)$.\n\n-   Another prior comes from a quantity called the effective number of nonzero coefficients,\\\n    $$m_{eff} = \\sum_{j=1}^p (1 - \\kappa_j).$$\n\n## Global shrinkage parameter $\\tau$\n\n-   The prior mean can be shown to be,\n\n$$\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = \\frac{\\tau \\sigma^{-1} \\sqrt{n}}{1 + \\tau \\sigma^{-1} \\sqrt{n}}p.$$\n\n-   Setting $\\mathbb{E}\\left[m_{eff} | \\tau, \\sigma\\right] = p_0$ (prior guess for the number of non-zero coefficients) yields for $\\tau$,\n\n$$\\tau_0 = \\frac{p_0}{p - p_0} \\frac{\\sigma}{\\sqrt{n}}$$\n\n## Global shrinkage parameter $\\tau$\n\n![](images/10/aki3.png){fig-alt=\"workflow\" fig-align=\"center\" height=\"5in\"}\n\n[From Piironena and Vehtari 2017](https://doi.org/10.1214/17-EJS1337SI)\n\n## Prepare for next class\n\n-   We are going to jump into an AE on body fat, but first some reminders.\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03), which was just assigned.\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Classification\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}