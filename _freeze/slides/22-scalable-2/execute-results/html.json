{
  "hash": "d4557dadc09bd2e5b9d608fa0e718eea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scalable Gaussian Processes #2\"\nauthor: \"Christine Shen\"\ndate: \"2025-04-03\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n```\n\n\n:::\n:::\n\n\n\n\n\n## Geospatial analysis on hemoglobin dataset {.midi}\n\n::: midi\nWe wanted to perform geospatial analysis on a dataset with \\~8,600 observations at \\~500 locations, and make predictions at \\~440 locations on a grid.\n:::\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](22-scalable-2_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n\n\n## Geospatial model {.midi}\n\nWe specify the following model: $$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})$$ with priors\n\n-   $\\boldsymbol{\\theta} | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))$, where $C$ is the MatÃ©rn 3/2 covariance function with magnitude $\\tau$ and length scale $\\rho$\n-   $\\alpha^* \\sim N(0,4^2)$. This is the intercept after centering $\\mathbf{X}$.\n-   $\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)$, $j \\in \\{1,\\dots,p\\}$\n-   $\\sigma \\sim \\text{Half-Normal}(0, 2^2)$\n-   $\\tau \\sim \\text{Half-Normal}(0, 4^2)$\n-   $\\rho \\sim \\text{Inv-Gamma}(5, 5)$\n-   $\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)$\n\n## Review of the last lecture {.midi}\n\n::: incremental\n1.  Gaussian process (GP) is not scalable as it requires $\\mathcal{O}(n^3)$ flops per MCMC iteration.\n\n2.  Introduced HSGP, a Hilbert space low-rank approximation method for GP. $$\\mathbf{C} \\approx \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^T, \\quad \\text{where}$$\n\n    -   $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}$ only depends on the *approximation box* $\\boldsymbol{\\Theta}$ and observed locations.\n    -   $\\mathbf{S} \\in \\mathbb{R}^{m \\times m}$ is diagonal. It depends on the covariance function $C$ and parameters $\\tau$ and $\\rho$.\n    -   $m$ is the number of basis functions.\n\n3.  Model reparameterization under HSGP.\n\n4.  Bayesian model fitting and kriging under HSGP.\n:::\n\n## HSGP parameters {.midi}\n\n@solin2020hilbert showed that HSGP approximation can be made arbitrarily accurate as $\\boldsymbol{\\Theta}$ and $m$ increase.\n\n::::: fragment\n:::: incremental\n::: callout-important\n## Our goal:\n\n-   Minimize the run time while maintaining reasonable approximation accuracy.\n-   Find minimum $\\boldsymbol{\\Theta}$ and $m$ with reasonable accuracy.\n:::\n::::\n:::::\n\n::: fragment\n*Note: we treat estimation of the GP magnitude parameter* $\\tau$ as a separate problem, and only consider approximation accuracy of HSGP in terms of the correlation function.\n:::\n\n## HSGP approximation box {.midi}\n\nDue to the design of HSGP, the approximation is less accurate near the boundaries of $\\boldsymbol{\\Theta}$.\n\n-   Suppose all the coordinates are centered. Let $$S_l = \\max_i |\\mathbf{u}_{il}|, \\quad l=1,\\dots,d, \\quad i= 1, \\dots, (n+q)$$ such that $\\boldsymbol{\\Theta}_S = \\prod_{l=1}^d [-S_l,S_l]$ is the smallest box which contains all observed and prediction locations. We should at least ensure $\\boldsymbol{\\Theta} \\supset \\boldsymbol{\\Theta}_S$.\n-   We want the box to be large enough to ensure good boundary accuracy. Let $c_l \\ge 1$ be *boundary factors*, we consider $$\\boldsymbol{\\Theta} = \\prod_{l=1}^d [-L_l,L_l], \\quad L_l = c_l S_l.$$\n\n## HSGP approximation box and $\\rho$ {.midi}\n\n![](./images/22/HSGPbox.png){fig-align=\"center\" height=\"350\"}\n\nHow much the approximation accuracy deteriorates towards the boundaries depends on smoothness of the true surface.\n\n-   the larger the length scale $\\rho$, the smoother the surface, a smaller box (smaller $c$) can be used for the same level of boundary accuracy.\n\n## HSGP approximation box and $m$ {.midi}\n\n![](./images/22/HSGPbox.png){fig-align=\"center\" height=\"350\"}\n\nThe larger the box,\n\n-   the more basis functions we need for the same level of overall accuracy,\n-   hence higher run time.\n\n## Zooming out doesn't simplify the problem {.midi}\n\n![](./images/22/HSGPbox2.png){fig-align=\"center\" height=\"350\"}\n\n-   If we scale the coordinates by a constant $b$, the length scale $\\rho$ of the underlying GP also needs to be approximately scaled by $b$ to capture the same level of details in the data.\n-   We can effectively think of the length scale parameter as $(\\rho/\\|\\mathbf{S}\\|)$.\n\n## HSGP basis functions {.midi}\n\nThe total number of basis functions $m = \\prod_{l=1}^d m_l$, i.e., we need to decide on $m_l$'s, the number of basis functions for each dimension.\n\n-   $m$ scales exponentially in $d$, hence the HSGP computation complexity $\\mathcal{O}(nm+m)$ also scales exponentially in $d$. Therefore HSGP is only recommended for $d \\le 3$, at most $4$.\n-   The higher the $m$, the better the overall approximation accuracy, the higher the runtime.\n\n## Relationship between $c$, $m$ and $\\rho/S$ {.midi}\n\nLet's quickly recap. For simplicity, let $d=1$,\n\n::: incremental\n1.  As $(\\rho/S)$ decreases, the surface is less smooth,\n    -   $c$ needs to increase to retain boundary accuracy.\n    -   $m$ needs to increase to retain overall accuracy.\n2.  As $c$ increases, $m$ needs to increase to retain overall accuracy.\n3.  As $m$ increases, run time increases.\n:::\n\n## Empirical functional form {.midi}\n\nStill assuming $d=1$. If $\\rho$ is known to us,\n\n-   given $c$ and $\\rho/S$ and the covariance function $C$, we can compute $m(c,\\rho/S)$, the minimum number of basis functions needed for a near 100% approximation accuracy of the correlation matrix.\n-   @riutort2023practical used extensive simulations to obtain an empirical function form of $m(c,\\rho/S)$ for frequently used MatÃ©rn covariance functions. E.g., for MatÃ©rn 3/2,\n\n$$m_{3/2}(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.$$\n\n## Empirical functional form {.midi}\n\n$$m(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.$$\n\n::: incremental\n-   Notice the linear proportionality between $m$, $c$ and $\\rho/S$.\n-   For a given $\\rho/S$, there exists a minimum $c(\\rho/S)=\\min(4.5\\rho/S,1.2)$, below which the approximation is poor no matter how large $m$ is.\n-   From $m(c,\\rho/S)$, we also have $$\\rho(m,c,S)=3.42 S \\frac cm,$$ the minimum $\\rho$ (least smooth surface) that can be well approximated given $c$, $m$ and $S$.\n:::\n\n## Question {.midi}\n\nBUT, in real applications, we **do not** know $\\rho$.\n\n::: fragment\nSo how to make use of $m(c,\\rho/S)$ to help choose $c$ and $m$?\n:::\n\n## An iterative algorithm {.midi}\n\nPseudo-codes for HSGP parameter tuning assuming $d=1$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu = center(observed and prediction locations)\nS = box size (u)\nmax_iter = 30\n\n# initialization\nj = 0\ncheck = FALSE\nrho = 0.5*S # the practical paper recommends setting the initial guess of rho to be 0.5 to 1 times S\nc = c(rho/S) # minimum c given rho and S\nm = m(c,rho/S) # minimum m given c, and rho/S\nL = c*S\ndiagnosis = logical(max_iter) # store checking results for each iteration\n\nwhile (!check & j<=max_iter){\n  \n  fit = runHSGP(L,m) # stan run\n  j = j + 1\n\n  rho_hat = mean(fit$rho) # obtain fitted value for rho\n  # check the fitted is larger than the minimum rho that can be well approximated\n  diagnosis[j] = (rho_hat + 0.01 >= rho)\n  if (j==1) {\n    \n    if (diagnosis[j]){\n      # if the diagnosis check is passed, do one more run just to make sure\n      m = m + 2\n      c = c(rho_hat/S)\n      rho = rho(m,c,S)\n    } else {\n      # if the check failed, update our knowledge about rho\n      rho = rho_hat\n      c = c(rho/S)\n      m = m(c,rho/S)\n    }\n  } else {\n    if (diagnosis[j] & diagnosis[j-2]){\n      # if the check passed for the last two runs, we finish tuning\n      check = TRUE\n    } else if (diagnosis[j] & !diagnosis[j-2]){\n      # if the check failed last time but passed this time, do one more run\n      m = m + 2\n      c = c(rho_hat/S)\n      rho = rho(m,c,S)      \n    } else if (!diagnosis[j]){\n      # if the check failed, update our knowledge about rho\n      rho = rho_hat\n      c = c(rho/S)\n      m = m(c,rho/S)\n    }\n  }\n  L = c*S\n}\n```\n:::\n\n\n\n\n\n## HSGP implementation codes {.midi}\n\nPlease clone the repo for AE 09 for HSGP implementation codes.\n\n## Side notes on HSGP implementation {.midi}\n\nA few random things to keep in mind for implementation in practice:\n\n::: incremental\n1.  If your HSGP run is suspiciously VERY slow, check the number of basis functions being used in the run and make sure it is reasonable.\n2.  Check whether $m \\le n$ before using the kriging results.\n3.  Because HSGP is a low-rank approximation method, the GP magnitude parameter $\\tau$ will always be overestimated. However, we can account for this and use a bias-adjusted $\\tau$ instead. See the AE 09 `stan` codes for parameter `tau_adj`.\n4.  If $d>1$, we need to do parameter tuning for each dimension.\n:::\n\n## Side notes on HSGP implementation {.midi}\n\nA few random things to keep in mind for implementation in practice:\n\n::: incremental\n5.  It is possible to use different length scale parameters for each dimension. See [demo codes here](https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper) for examples.\n\n6.  The iterative algorithm described in @riutort2023practical (i.e., pseudo-codes on slide 15) can be further improved:\n\n    -   it sometimes stops at a \\textit{bad} place.\n    -   it sometimes runs into a circular loop. See AE 09 `stan` codes for one possible fix.\n    -   it errs on the safe side and only changes $m$ if it might be too small.\n\n7.  Due to identifiability issues, we always look at the spatial intercept $\\alpha \\mathbf{1}+\\boldsymbol{\\theta}$ together instead of just $\\boldsymbol{\\theta}$.\n:::\n\n## GP vs HSGP spatial intercept posterior mean {.midi}\n\n![](./images/22/intercept_p_m.png){fig-align=\"center\" height=\"600\"}\n\n## GP vs HSGP spatial intercept posterior SD {.midi}\n\n![](./images/22/intercept_p_sd.png){fig-align=\"center\" height=\"600\"}\n\n## GP vs HSGP parameter posterior density {.midi}\n\n![](./images/22/density.png){fig-align=\"center\" height=\"550\"}\n\n## GP vs HSGP correlation function {.midi}\n\n![](./images/22/correlation.png){fig-align=\"center\" height=\"550\"}\n\n## GP vs HSGP effective sample size {.midi}\n\n![](./images/22/ESS.png){fig-align=\"center\" height=\"550\"}\n\n## Prepare for next class {.midi}\n\n1.  Work on HW 05 which is due Apr 8\n2.  Complete reading to prepare for Tuesday's lecture\n3.  Tuesday's lecture: Bayesian Clustering\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "22-scalable-2_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}