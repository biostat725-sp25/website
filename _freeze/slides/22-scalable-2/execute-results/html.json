{
  "hash": "77ac803400ce16ec74542ba3303450b0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scalable Gaussian Processes #2\"\nauthor: \"Christine Shen\"\ndate: \"2025-04-03\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Geospatial analysis on hemoglobin dataset {.midi}\n\n::: midi\nWe wanted to perform geospatial analysis on a dataset with \\~8,600 observations at \\~500 locations, and make predictions at \\~440 locations on a grid.\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](22-scalable-2_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n## Geospatial model {.midi}\n\nWe specify the following model: $$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})$$ with priors\n\n-   $\\boldsymbol{\\theta}(\\mathbf{u}) | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))$, where $C$ is the MatÃ©rn 3/2 covariance function with magnitude $\\tau$ and length scale $\\rho$\n-   $\\alpha^* \\sim N(0,4^2)$. This is the intercept after centering $\\mathbf{X}$.\n-   $\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)$, $j \\in \\{1,\\dots,p\\}$\n-   $\\sigma \\sim \\text{Half-Normal}(0, 2^2)$\n-   $\\tau \\sim \\text{Half-Normal}(0, 4^2)$\n-   $\\rho \\sim \\text{Inv-Gamma}(5, 5)$\n-   $\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)$\n\n## Review of the last lecture {.midi}\n\n1.  Gaussian process (GP) requires $\\mathcal{O}(n^3)$ flops per MCMC iteration, hence it is not scalable.\n\n2.  Introduced HSGP, a Hilbert space low-rank approximation method for GP. $$\\mathbf{C} \\approx \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^T, \\quad \\text{where}$$\n\n    -   $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}$ only depends on the *approximation box* $\\boldsymbol{\\Theta}$ and observed locations\n    -   $\\mathbf{S} \\in \\mathbb{R}^{m \\times m}$ is diagonal, $m$ is the number of basis functions\n\n3.  Model reparameterization under HSGP\n\n4.  Kriging under HSGP\n\n5.  Parameter tuning for HSGP\n\n## Parameter tuning for HSGP {.midi}\n\n:::::: columns\n::: {.column width=\"60%\"}\nTo implement HSGP, we need to decide on:\n\n1.  number of basis functions $m=\\prod_{l=1}^d m_l$\n\n    -   one number $m_l$ for each dimension of the GP.\n\n2.  size of the approximation box $\\boldsymbol{\\Theta}$\n\n    -   scale of the coordinates $\\mathbf{S}=(S_1,S_2)$.\n    -   boundary factors $\\mathbf{c}=(c_1,c_2)$.\n:::\n\n:::: {.column width=\"40%\"}\n::: {style=\"margin-top: 30px;\"}\n![](./images/21/HSGPbox.png){fig-align=\"center\" height=\"300\"}\n:::\n::::\n::::::\n\n:::: fragment\n::: callout-important\n## Our goal:\n\nMinimize the run time while maintaining reasonable approximation accuracy.\n:::\n::::\n\n## Review of what we discussed last time {.midi}\n\nFor simplicity, let $d=1$\n\n::: incremental\n1.  Estimation of the GP magnitude $\\tau$ is treated as a separate problem. We only consider approximation accuracy in terms of the correlation matrix.\n2.  Scaling the coordinates do not change the problem. We consider the length scale parameter as $(\\rho/S)$.\n3.  As $(\\rho/S)$ decreases, the surface is less smooth,\n    -   $c$ needs to increase to retain boundary accuracy.\n    -   $m$ needs to increase to retain overall accuracy.\n4.  For a given $\\rho/S$, there exists a minimum $c$, below which the approximation is poor no matter how large $m$ is. This minimum value increases as $\\rho/S$ increases.\n5.  As $c$ increases, $m$ needs to increase to retain overall accuracy.\n:::\n\n## Review of what we discussed last time {.midi}\n\n::: incremental\n6.  As $m$ increases, run time increases. Hence we want to minimize $m$ and $c$ while maintaining certain accuracy level.\n\n7.  @riutort2023practical presented an empirical functional form of $m$ as a function of $c$ and $\\rho/S$ for MatÃ©rn 3/2 covariance function: $$m(c,\\rho/S)=3.24 \\frac{c}{\\rho/S}, \\quad c \\ge 4.5 \\frac{\\rho}S, \\quad c \\ge 1.2.$$\n\n8.  Note we also have\n\n    -   $\\rho(m,c,S)=3.42 Sc/m$: the minimum $\\rho$ (least smooth surface) that can be well approximated given $c$, $m$ and $S$.\n    -   $c(\\rho,S)=\\min(4.5\\rho/S,1.2)$: the minimum $c$ for HSGP to work.\n:::\n\n::: fragment\nQuestion: in real applications, we **do not** know $\\rho$. So how to proceed?\n:::\n\n## An iterative algorithm {.midi}\n\nPseudo-codes for HSGP parameter tuning assuming $d=1$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nu = centered(data locations)\nS = box size (u)\nmax_iter = 30\n\n# initialization\nj = 0\ncheck = FALSE\nrho = 0.5*S\nc = c(rho,S) # minimum c given rho and S\nm = m(c,rho/S) # minimum m given c, and rho/S\nL = c*S\ndiag = logical(max_iter) # store checking results for each iteration\n\nwhile (!check & j<=max_iter){\n  \n  fit = runHSGP(rho,L,m) # stan run\n  j = j + 1\n\n  rho_hat = mean(fit$rho_hat) # obtain fitted value for rho\n  # check the fitted is larger than the minimum rho that can be well approximated\n  diag[j] = (rho_hat + 0.01 >= rho)\n  if (j==1) {\n    \n    if (diag[j]){\n      # if the diagnosis check is passed, do one more run just to make sure\n      m = m + 2\n      c = c(rho_hat,S)\n      rho = rho(m,c,S)\n    } else {\n      # if the check failed, update our knowledge about rho\n      rho = rho_hat\n      c = c(rho,S)\n      m = m(c,rho/S)\n    }\n  } else {\n    if (diag[j] & diag[j-2]){\n      # if the check passed for the last two runs, we finish tuning\n      check = TRUE\n    } else if (diag[j] & !diag[j-2]){\n      # if the check failed last time but passed this time, do one more run\n      m = m + 2\n      c = c(rho_hat,S)\n      rho = rho(m,c,S)      \n    } else if (!diag[j]){\n      # if the check failed, update our knowledge about rho\n      rho = rho_hat\n      c = c(rho,S)\n      m = m(c,rho/S)\n    }\n  }\n  L = c*S\n}\n```\n:::\n\n\n\n## HSGP implementation codes {.midi}\n\nPlease clone the repo for AE 09 for HSGP implementation codes.\n\n## Side notes on HSGP implementation {.midi}\n\nA few random things to keep in mind for implementation in practice:\n\n::: incremental\n1.  Make sure the starting total number of basis functions used is reasonable before the run.\n2.  Because HSGP is a low-rank approximation method, $\\tau$ will always be overestimated. However, we can adjust for this and use a bias-adjusted $\\tau$ instead.\n3.  If $d>1$, we need to do parameter tuning for each dimension. It is possible to use different length scale parameter for each dimension. See [demo codes here](https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper) for examples.\n4.  The iterative algorithm described in @riutort2023practical sometimes run into a loop and doesn't converge. AE codes show one way to avoid it.\n5.  Due to identifiability issues, we always look at the spatial intercept $\\alpha+\\boldsymbol{\\theta}(\\mathbf{u})$ together instead of just $\\boldsymbol{\\theta}(\\mathbf{u})$.\n:::\n\n## GP vs HSGP spatial intercept posterior mean {.midi}\n\n![](./images/22/intercept_p_m.png){fig-align=\"center\" height=\"600\"}\n\n## GP vs HSGP spatial intercept posterior SD {.midi}\n\n![](./images/22/intercept_p_sd.png){fig-align=\"center\" height=\"600\"}\n\n## GP vs HSGP parameter posterior density {.midi}\n\n![](./images/22/density.png){fig-align=\"center\" height=\"550\"}\n\n## GP vs HSGP correlation function {.midi}\n\n![](./images/22/correlation.png){fig-align=\"center\" height=\"550\"}\n\n## GP vs HSGP effective sample size {.midi}\n\n![](./images/22/ESS.png){fig-align=\"center\" height=\"550\"}\n\n## Prepare for next class {.midi}\n\n1.  Work on HW 05 which is due Apr 8\n2.  Complete reading to prepare for Tuesday's lecture\n3.  Tuesday's lecture: Bayesian Clustering\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "22-scalable-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}