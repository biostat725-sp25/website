{
  "hash": "d53ba959a07b24396a2cb9254882c804",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scalable Gaussian Processes #1\"\nauthor: \"Christine Shen\"\ndate: \"2025-04-01\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\n# logo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n# bibliography: ../../doc/reference.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Review of the last lecture\n\nTBU\n\n## Motivating dataset {.midi}\n\nRecall we worked with a dataset on women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. \n\n- There are ~8,600 women nested in ~500 survey clusters. \n\n- Variables are:\n\n  - `loc_id`: location id (i.e. survey cluster).\n\n  - `hemoglobin`: hemoglobin level (g/dL).\n\n  - `anemia`: anemia classifications.\n  \n  - `age`: age in years.\n\n  - `urban`: urban vs. rural.\n\n  - `LATNUM`: latitude.\n\n  - `LONGNUM`: longitude.\n\n## Motivating dataset {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  loc_id hemoglobin     anemia age urban   LATNUM  LONGNUM\n1      1       12.5 not anemic  28 rural 0.220128 21.79508\n2      1       12.6 not anemic  42 rural 0.220128 21.79508\n3      1       13.3 not anemic  15 rural 0.220128 21.79508\n4      1       12.9 not anemic  28 rural 0.220128 21.79508\n5      1       10.4       mild  32 rural 0.220128 21.79508\n6      1       12.2 not anemic  42 rural 0.220128 21.79508\n```\n\n\n:::\n:::\n\n\n\n::: callout-important\n## Modeling goals:\n\n  - Learn the associations between age and urbanality and hemoglobin, accounting for unmeasured spatial confounders.\n  \n  - Create a predicted map of hemoglobin across the spatial surface controlling for age and urbanality, with uncertainty quantification.\n  \n:::\n\n## Map of Sud-Kivu state {.midi}\n\nLast time, we focused on one state with ~500 women at ~30 locations.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Map of the DRC{.midi}\n\nToday we will extend the analysis to the full dataset.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Modeling{.midi}\n\n\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Data Objects:**\n\n- $i \\in \\{1,\\dots,n\\}$ indexes unique locations.\n\n- $j \\in \\{1,\\dots,n_i\\}$ indexes individuals at each location.\n\n- $Y_j(\\mathbf{u}_i)$ denotes the observation of individual $j$ at location $\\mathbf{u}_i$.\n\n- $\\mathbf{x}_j(\\mathbf{u}_i) \\in \\mathbb{R}^{1 \\times p}$, where $p=3$ is the number of predictors (excluding intercept).\n\n## Modeling{.midi}\n\n\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Population Parameters:**\n\n- $\\alpha \\in \\mathbb{R}$ is the intercept.\n\n- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the regression coefficients.\n\n- $\\sigma^2 \\in \\mathbb{R}^+$ is the overall residual error (nugget).\n\n**Location-specific Parameters:**\n\n- $\\mathbf{u}_i = (\\text{latitude}_i, \\text{longitude}_i) \\in \\mathbb{R}^2$ denotes coordinates of location $i$.\n\n- $\\theta(\\mathbf{u}_i)$ denotes the spatial intercept at location $\\mathbf{u}_i$.\n\n\n## Location-specific Notation\n\n$$\\mathbf{Y}(\\mathbf{u}_i) = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}(\\mathbf{u}_i)$$\n\n- $\\mathbf{Y}(\\mathbf{u}_i) = (Y_1(\\mathbf{u}_i),\\ldots,Y_{n_i}(\\mathbf{u}_i))^\\top$\n\n- $\\mathbf{X}(\\mathbf{u}_i)$ is an $n_i \\times p$ dimensional matrix with rows $\\mathbf{x}_j(\\mathbf{u}_i)$.\n\n- $\\boldsymbol{\\epsilon}(\\mathbf{u}_i) = (\\epsilon_i(\\mathbf{u}_i),\\ldots,\\epsilon_{n_i}(\\mathbf{u}_i))^\\top$, where $\\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)$.\n\n## Full data notation\n\n$$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}$$\n\n- $\\mathbf{Y} = (\\mathbf{Y}(\\mathbf{u}_1)^\\top,\\ldots,\\mathbf{Y}(\\mathbf{u}_{n})^\\top)^\\top \\in \\mathbb{R}^N$, with $N = \\sum_{i=1}^n n_i$.\n\n- $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$ that stacks $\\mathbf{X}(\\mathbf{u}_i)$.\n\n- $\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top \\in \\mathbb{R}^n$.\n\n- $\\mathbf{Z}$ is $N \\times n$ dimensional binary matrix. Each row contains a single 1 in column $i$ that corresponds to the location of $Y_j(\\mathbf{u}_i)$.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Analysis of the full Anemia dataset\n\nConsider the following model:\n\n\\begin{align*}\n  Y_{ij} = \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0,\\sigma^2).\n\\end{align*}\n\n**Data Objects:**\n\n- $i \\in \\{1,\\dots,n\\}$ indexes unique locations.\n\n- $j \\in \\{1,\\dots,n_i\\}$ indexes individuals at each location.\n\n- $Y_{ij}$ denotes the hemoglobin level of individual $j$ at location $i$.\n\n- $\\mathbf{x}_{ij} \\in \\mathbb{R}^p$, $p=3$, are covariates including an intercept, age (years), and urban (binary).\n\n## Analysis of the full Anemia dataset\n\nConsider the following model:\n\n\\begin{align*}\n  Y_{ij} = \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0,\\sigma^2).\n\\end{align*}\n\n**Population Parameters:**\n\n- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the regression coefficients.\n\n- $\\sigma^2 \\in \\mathbb{R}^+$ is the overall residual error (nugget).\n\n## Analysis of the full Anemia dataset\n\nConsider the following model:\n\n\\begin{align*}\n  Y_{ij} = \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_{ij}, \\quad \\epsilon_{ij} \\sim N(0,\\sigma^2).\n\\end{align*}\n\n**Location-specific Parameters:**\n\n- $\\theta(\\mathbf{u}_i)$ denotes the spatial intercept at location $\\mathbf{u}_i$.\n\n- $\\mathbf{u}_i \\in \\mathbb{R}^d$ denotes the spatial location of location $i$. For example, $\\mathbf{u}_i = (\\text{latitude}_i, \\text{longitude}_i)$, so that $d = 2$.\n\n## Analysis of the full Anemia dataset {.midi}\n\nRewriting the model in a vectorized form (as a LMM):\n\n$$\\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\theta}(\\mathbf{u}_i) + \\boldsymbol{\\epsilon}_i, \\quad \\boldsymbol{\\epsilon}_i \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i}).$$\n\n-   $\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})$ are location-specific observations.\n\n-   $\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i})$, such that $\\epsilon_{ij} \\stackrel{iid}{\\sim} N(0,\\sigma^2)$.\n\n\n\\begin{align*}\n\\mathbf{Y}_i = \\mathbf{X}_i\\boldsymbol{\\beta} + \\mathbf{Z}_i\\boldsymbol{\\theta}(\\mathbf{u}) + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N(0,\\sigma^2 \\mathbf{I}),\n\\end{align*}\n\nwhere\n\n- $\\mathbf{y} \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, and $\\boldsymbol{\\theta}(\\mathbf{u}) \\in \\mathbb{R}^{q}$\n- $W \\in \\mathbb{R}^{n \\times q}$ assigns the spatial intercepts to each individual\n\nConsider a Bayesian analysis with \n\n- $\\boldsymbol{\\theta}(u) \\sim GP(0,K(\\psi))$ for some covariance kernel $K$ with parameter $\\psi$\n- $\\boldsymbol{\\beta} \\sim N(0,\\tau^2)$\n- and appropriate priors for $\\sigma^2$, $\\tau^2$, and $\\psi$\n\n\n\n## Analysis of the full Anemia dataset\n\nRewriting the model in a vectorized form:\n$$\n\\begin{align}\n  \\mathbf{y} = X\\boldsymbol{\\beta} + W\\boldsymbol{\\theta}(\\mathbf{u}) + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N(0,\\sigma^2 \\mathbf{I}),\n\\end{align}\n$$\nwhere\n\n- $\\mathbf{y} \\in \\mathbb{R}^{n}$, $X \\in \\mathbb{R}^{n \\times p}$, and $\\boldsymbol{\\theta}(\\mathbf{u}) \\in \\mathbb{R}^{q}$\n- $W \\in \\mathbb{R}^{n \\times q}$ assigns the spatial intercepts to each individual\n\nConsider a Bayesian analysis with \n\n- $\\boldsymbol{\\theta}(u) \\sim GP(0,K(\\psi))$ for some covariance kernel $K$ with parameter $\\psi$\n- $\\boldsymbol{\\beta} \\sim N(0,\\tau^2)$\n- and appropriate priors for $\\sigma^2$, $\\tau^2$, and $\\psi$\n\n## Inference goals\n\nOur goal is to [TBU]:\n\n1. study the associations between the outcome and the covariates\n2. learn the spatial intercept surface\n\nTo achieve these goals, we need to\n\n1. perform Bayesian model fitting to obtain posterior samples of $\\boldsymbol{\\beta}$, $\\boldsymbol{\\theta}(\\mathbf{u})$ and other parameters\n2. make out-of-sample predictions for $\\boldsymbol{\\theta}(\\mathbf{u}^*)$, spatial intercepts at $q^*$ new locations $\\mathbf{u}^*=(u_1^*,\\dots,u_{q^*})$\n\n## Computational issues with GP\n\nEffectively, the prior for $\\boldsymbol{\\theta}(\\mathbf{u})$ is \n$$\n\\begin{align}\n  \\boldsymbol{\\theta}(\\mathbf{u}) = \\begin{pmatrix}\n    \\theta(u_1) \\\\\n    \\vdots \\\\\n    \\theta(u_q)\n  \\end{pmatrix} \\sim N(0, \\Sigma), \\quad \\text{where } \\Sigma \\in \\mathbb{R}^{q \\times q}, \\quad \\Sigma_{ij} = K(u_i,u_j|\\psi).\n\\end{align}\n$$\n\nThis is not scalable because of the need to invert a $q \\times q$ covariance matrix for each MCMC iteration, which requires $\\mathcal{O}(q^3)$ floating point operations (flops), and $\\mathcal{O}(q^2)$ memory.\n\n## Scalable GP methods overview\n\nExisting scalable GP methods broadly fall under two categories.\n\n1. Sparsity methods\n\n  - sparsity in $\\Sigma$, e.g., covariance tapering (@furrer2006covariance)\n  - sparsity in $\\Sigma^{-1}$, e.g., Vecchia approximation (@vecchia1988estimation) and nearest-neighbor GP (@datta2016hierarchical)\n  \n2. Low-rank methods\n\n  - approximate $\\Sigma$ on a low-dimensional subspace\n  - e.g., process convolution (@higdon2002space), inducing point method(@snelson2005sparse)\n\n\n## Hilbert space method for GP\n\n@solin2020hilbert introduced a Hilbert space method for reduced-rank Gaussian process regression (HSGP).\n\n@riutort2023practical discussed how to practically implement HSGP.\n\nTutorial codes are available in different probabilistic programming languages:\n\n- [stan](https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper)\n- [NumPyro](https://num.pyro.ai/en/0.15.2/examples/hsgp.html)\n- [pyMC](https://juanitorduz.github.io/hsgp_intro/)\n\n## Outline\n\nLecture today:\n\n - What is HSGP?\n - Theory behind HSGP\n - Why HSGP is scalable?\n - How to use HSGP to make out-of-sample predictions?\n \nLecture on Thursday:\n\n - Parameter tuning for HSGP\n - How to implement HSGP in `stan`\n - Codes demo for the Anemia dataset\n - Application exercise\n\n## Key idea of HSGP\n\nHSGP approximates $\\Sigma$ with\n\n$$\\Sigma \\approx \\Phi(\\mathbf{u}) \\Lambda(\\psi) \\Phi(\\mathbf{u})^T,$$\nwhere \n\n- let $m$ denote the number of basis functions\n- $\\Phi(\\mathbf{u}) \\in \\mathbb{R}^{q \\times m}$ only depends on the observed locations\n- $\\Lambda(\\psi) \\in \\mathbb{R}^{m \\times m}$ is diagonal\n\nComputation cost for the GP in each MCMC iteration reduces to $\\mathcal{O}(qm + m)$.\n\n\n# Theory behind HSGP\n\n\n## Definitions\n\nLet $\\mathcal{U} \\in \\mathbb{R}^d$, $d=2$ be the geographical area of interest. For any locations $u,u' \\in \\mathcal{U}$, let $r=u-u'$. A covariance function $K$ is\n\n- **stationary** (translation invariant) if \n\n$$K(u,u') = K(r)$$\n\n- **isotropic** (rotation invariant) if \n\n$$K(u,u')=K(\\|r\\|)$$\n\n## Spectral density\n\nBy the Bochner's theorem, a bounded stationary covariance function $K$ can be represented as\n\n$$K(r) = \\frac 1{(2\\pi)^d} \\int \\exp(i w^Tr) \\mu(dw)$$\n\nfor some positive finite measure $\\mu$. If $\\mu$ has a density $S(w)$ with respect to the Lebesgue measure, it is called the **spectral density**. \n\nBy the Wiener-Khintchin theorem, $K(r)$ and $S(w)$ are Fourier duals:\n$$\n\\begin{align}\n  K(r) &= \\frac 1{(2\\pi)^d} \\int \\exp(iw^Tr) S(w) dw \\\\\n  S(w) &= \\int K(r) \\exp(-i w^Tr) dr.\n\\end{align}\n$$\n\n## Polynomial expansion of the spectral density\n\nFor an isotropic covariance function $K(\\|r\\|)$, the corresponding spectral density $S(w)=S(\\|w\\|)$. \n\nFor regular covariance functions, $S$ admits a closed form polynomial expansion of $\\|w\\|^2$:\n\n$$S(\\|w\\|) = a_0 + a_1(\\|w\\|^2) + a_2(\\|w\\|^2)^2 + \\dots \\tag{1}$$\n\n\nNote that $-\\|w\\|^2$ is the transfer function for the Laplace operator $\\nabla^2$. Let $\\mathcal{F}$ denote the Fourier transform operator, $f$ denote a twice-differentiable real-valued function,\n\n$$[\\mathcal{F}(\\nabla^2 f)](w) = -\\|w\\|^2 [\\mathcal{F}(f)](w).$$\n\n## Connection with the Laplacian\n\nHence applying inverse Fourier transform on both sides of equation (1), we have\n\n$$\\mathcal{K} = a_0 + a_1(-\\nabla^2) + a_2(-\\nabla^2)^2 + \\dots \\tag{2}$$\n\nwhere $\\mathcal{K}$ is the covariance operator such that \n\n$$\\mathcal{K}f(u) = \\int K(u,u')f(u')du'.$$\n\nEquation (2) shows the connection between $\\nabla^2$ and $\\mathcal{K}$. If we are able to approximate $\\nabla^2$, we can approximate $\\mathcal{K}$.\n\n## Eigenfunction expansion of the Laplacian\n\nOn a compact set $\\Omega \\subset \\mathbb{R}^d$, $-\\nabla^2$ has a discrete spectrum. Consider the eigenvalue problem for $-\\nabla^2$ with Dirichlet boundary conditions:\n$$\n\\begin{align}\n  \\begin{cases}\n    -\\nabla^2 \\phi_j(x) = \\lambda_j \\phi_j(x), & x \\in \\Omega \\\\\n    \\phi_j(x) = 0, & x \\in \\partial \\Omega\n  \\end{cases}\n\\end{align}\n$$\nWith sufficiently smooth boundary $\\partial \\Omega$, eigenvalues and eigenfunctions exist. And because $-\\nabla^2$ is positive definite Hermitian,\n\n- all the eigenvalues $\\lambda_j$'s are real and positive\n- the eigenfunctions $\\phi_j$'s are orthonormal\n\n$$\\int_\\Omega \\phi_i(u) \\phi_j(u) du = 1_{(i=j)}.$$\n\n## Eigenfunction expansion of the Laplacian\n\nTherefore on $\\Omega$, for sufficiently smooth function $f$,\n\n$$-\\nabla^2 f(u) = \\int L(u,u')f(u')du',$$\nwhere the kernel\n$$L(u,u') = \\sum_{j=1}^\\infty \\lambda_j \\phi_j(u) \\phi_j(u').$$\n\nBy orthonormality,\n\n$$L^s(u,u')=\\sum_{j=1}^\\infty \\lambda_j^s \\phi_j(u) \\phi_j(u').$$\n\n## Low rank approximation\n\nTherefore on $\\Omega$ under the boundary conditions, \n\n$$\n\\begin{align}\n  \\mathcal{K}f(u) &= [a_0 + a_1(-\\nabla^2) + a_2(-\\nabla^2)^2 + \\dots]f(u) \\\\\n  &= \\int [a_0 + a_1 L(u,u') + a_2L^2(u,u') + \\dots]f(u')du'.\n\\end{align}\n$$\nRecall $\\mathcal{K}f(u)= \\int K(u,u') f(u') du'$. Hence\n\n$$\n\\begin{align}\n  K(u,u') &= a_0 + a_1 L(u,u') + a_2L^2(u,u') + \\dots \\\\\n  &= \\sum_{j=1}^\\infty (a_0 + a_1 \\lambda_j + a_2 \\lambda_j^2 + \\dots) \\phi_j(u) \\phi_j(u') \\\\\n  &= \\sum_{j=1}^\\infty S(\\sqrt{\\lambda_j})\\phi_j(u) \\phi_j(u').\n\\end{align}\n$$\n\n## Low rank approximation\n\nOverall, we can approximate the covariance function with\n\n$$\n\\begin{align}\n  K(u,u') &\\approx \\sum_{j=1}^\\infty S(\\sqrt{\\lambda_j})\\phi_j(u) \\phi_j(u') \\\\\n  &\\approx \\sum_{j=1}^m S(\\sqrt{\\lambda_j})\\phi_j(u) \\phi_j(u').\n\\end{align}\n$$\n\n- the spectral density $S$ depends on the covariance function $K$\n- the eigenvalue $\\lambda_j$'s and eigenfunction $\\phi_j$'s only depend on the chosen compact domain $\\Omega$ and are independent of $K$\n- even with an infinite sum, this is still an approximation as the eigenexpansion is restricted to $\\Omega$.\n\n## Key takeaways\n\nHSGP approximates the GP covariance function via an eigenfunction expansion of the Laplace operator in a compact set $\\Omega$. \n\nHSGP **only** works:\n\n1. for covariance functions which admits a power spectral density, e.g., the Matern family.\n2. under a user-specified compact domain $\\Omega$.\n3. under a user-specified number of basis functions $m$. The approximation can be made arbitrarily accurate as $m$ and $\\Omega$ increase.\n4. for $d \\le 3$, at most $4$, because given an accuracy level, $m$ scales exponentially in $d$.\n\n## HSGP Summary\n\nRecall, we want to model a spatial intercept\n$$\\boldsymbol{\\theta}(\\mathbf{u}) \\sim N(0,\\Sigma), \\quad \\Sigma \\in \\mathbb{R}^{q \\times q}, \\quad \\Sigma_{ij}=K(u_i,u_j|\\psi).$$\n\nFor isotropic and *nice* covariance function $K$, we can use HSGP to approximate $\\Sigma$ as\n\n$$\n\\begin{align}\n  &\\Sigma_{ik} \\approx \\sum_{j=1}^m S_{K,\\psi}(\\sqrt{\\lambda_j})\\phi_j(u_i) \\phi_j(u_k) \\\\\n  \\implies & \\Sigma \\approx \\Phi(\\mathbf{u})\\Lambda(\\psi)\\Phi(\\mathbf{u})^T, \\quad \\text{where}\n\\end{align}\n$$\n\n- $\\Lambda(\\psi) =$ diag$(S_{K,\\psi}(\\sqrt{\\lambda_1}), \\dots, S_{K,\\psi}(\\sqrt{\\lambda_m}))$\n- $\\Phi(\\mathbf{u})$ is a $q \\times m$ matrix with $\\Phi(\\mathbf{u})_{ij} = \\phi_j(\\mathbf{u}_i)$\n\n## Why HSGP is scalable\n\nThe HSGP approximation\n$$\\Sigma \\approx \\Phi(\\mathbf{u})\\Lambda(\\psi)\\Phi(\\mathbf{u})^T$$\n\nis scalable because \n\n- no matrix inversion\n- each MCMC iteration requires $\\mathcal{O}(qm + m)$ flops, vs $\\mathcal{O}(q^3)$ for full GP\n- ideally $m \\ll q$, but can be faster even for $m>q$\n\n## Model reparameterization\n\nUnder HSGP,\n$$\\boldsymbol{\\theta}(\\mathbf{u}) \\overset{d}{\\approx} \\Phi(\\mathbf{u}) \\Lambda(\\psi)^{1/2}\\mathbf{z}, \\quad \\text{for } \\mathbf{z} \\sim N(0,\\mathbf{I}).$$\n\nTherefore we can reparameterize the model as \n\n$$\n\\begin{align}\n  \\mathbf{y} &= X\\boldsymbol{\\beta} + W\\boldsymbol{\\theta}(\\mathbf{u}) + \\boldsymbol{\\epsilon} \\\\\n  &\\approx X\\boldsymbol{\\beta} + \\underbrace{W\\Phi(\\mathbf{u}) \\Lambda(\\psi)^{1/2}}_{A(\\psi)}\\mathbf{z} + \\boldsymbol{\\epsilon}\n\\end{align}\n$$\n\nNote the resemblance to linear regression:\n\n- $A(\\psi) \\in \\mathbb{R}^{n \\times m}$ is a known design matrix given GP parameters $\\psi$\n- $\\mathbf{z}$ is an unknown parameter vector with prior $N(0,\\mathbf{I})$\n\n## HSGP model in `stan`\n\nSimilarly, we can use the reparameterized model in `stan`. \n\n`Stan` documentation calls it the [*non-centered parameterization*](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#hierarchical-models-and-the-non-centered-parameterization), and suggests it's usually more computationally efficient.\n\n\n\n::: {.cell output.var='model_in_stan'}\n\n```{.stan .cell-code}\ntransformed data {\n  matrix[q,m] PHI = ...;\n}\nparameters {\n  vector[m] z; // standard normal\n}\nmodel {\n  vector[q] theta = PHI * sqrt_Lambda .* z;\n  \n  target += normal_lupdf(y | X * beta + W * theta, sigma);\n  target += normal_lupdf(z | 0, 1);\n  ...\n}\n```\n:::\n\n\n\n## Out-of-sample prediction {.small}\n\nWe are also interested in $\\boldsymbol{\\theta}(\\mathbf{u}^*)$, the spatial intercepts at $q^*$ new locations $u^*_1, \\dots u^*_{q^*}$. Recall under the GP,\n$$\n\\begin{align}\n  \\begin{pmatrix}\n    \\boldsymbol{\\theta}(\\mathbf{u}) \\\\\n    \\boldsymbol{\\theta}(\\mathbf{u}^*)\n  \\end{pmatrix} \\sim N \\left(0,\n  \\begin{pmatrix}\n    \\Sigma & C \\\\\n    C^T & \\Sigma^*\n  \\end{pmatrix} \\right),\n\\end{align}\n$$\nwhere $\\Sigma^* \\in \\mathbb{R}^{q^* \\times q^*}$ is the covariance matrix for $\\boldsymbol{\\theta}(\\mathbf{u}^*)$, and $C$ is the cross covariance matrix between $\\boldsymbol{\\theta}(\\mathbf{u})$ and $\\boldsymbol{\\theta}(\\mathbf{u}^*)$.\n\nThe conditional distribution is \n$$\\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u}) \\sim N(C^T \\Sigma^{-1}\\boldsymbol{\\theta}(\\mathbf{u}), \\Sigma^*-C^T\\Sigma^{-1}C).$$\nFor each posterior sample $\\boldsymbol{\\theta}(\\mathbf{u})^{(s)}$, we obtain a posterior predictive sample $\\boldsymbol{\\theta}(\\mathbf{u}^*)^{(s)}$ by drawing from $\\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u})^{(s)}$.\n\n## Out-of-sample prediction under HSGP {.small}\n\nUnder HSGP, approximately\n$$\n\\begin{align}\n  \\begin{pmatrix}\n    \\boldsymbol{\\theta}(\\mathbf{u}) \\\\\n    \\boldsymbol{\\theta}(\\mathbf{u}^*)\n  \\end{pmatrix} \\sim N \\left(0,\n  \\begin{pmatrix}\n    \\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T & \\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u}^*)^T \\\\\n    \\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^T & \\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u}^*)^T\n  \\end{pmatrix} \\right).\n\\end{align}\n$$\n\n1. if $m \\ge q$, use the same approach\n2. if $m < q$ so that $\\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T$ is not invertible, \n$$\\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u}) = (\\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^T) \\Sigma^+\\boldsymbol{\\theta}(\\mathbf{u})$$\nwhere $\\Sigma^+$ is the pseudo inverse of $\\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T$ such that $\\Phi(\\mathbf{u})\\Lambda(\\psi) \\Phi(\\mathbf{u})^T\\Sigma^+ = \\mathbf{I}$.\n\n\n## Out-of-sample prediction under HSGP\n\nUnder the reparamterized model,\n$$\n\\begin{align}\n  \\boldsymbol{\\theta}(\\mathbf{u}^*) \\mid \\boldsymbol{\\theta}(\\mathbf{u}) &= (\\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^{\\top})   \\Sigma^+\\boldsymbol{\\theta}(\\mathbf{u}) \\\\\n  &= (\\Phi(\\mathbf{u}^*)\\Lambda(\\psi) \\Phi(\\mathbf{u})^T) \\Sigma^+\\Phi(\\mathbf{u}) \\Lambda(\\psi)^{1/2}\\mathbf{z} \\\\\n  &= \\Phi(\\mathbf{u}^*)\\Lambda(\\psi)^{1/2}\\mathbf{z}.\n\\end{align}\n$$\n\nTherefore for each posterior sample of $\\mathbf{z}$ and $\\psi$, we can obtain a posterior predictive sample for $\\boldsymbol{\\theta}(\\mathbf{u}^*)$ as\n$$\\boldsymbol{\\theta}(\\mathbf{u}^*)^{(s)} = \\Phi(\\mathbf{u}^*)\\Lambda(\\psi^{(s)})^{1/2}\\mathbf{z}^{(s)}.$$\n\n## HSGP prediction in `stan`\n\nEasy to implement in `stan`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntransformed data {\n  matrix[q_pred,m] PHI_pred = ...;\n}\ngenerated quantities {\n  vector[q_pred] theta_pred = PHI_pred * sqrt_Lambda .* z;\n}\n```\n:::\n\n\n\n## Recap\n\n1. Goal: Bayesian spatial analysis on the full Anemia dataset with $n>8,000$ observations at $q \\approx 500$ locations\n2. Encountered computational issues with GP, thus need to explore scalable GP methods\n3. HSGP is a low-rank approximation method based on eigenfunction expansion of the Laplace operator in a compact domain $\\Omega$\n4. HSGP can be implemented in `stan` for both model fitting and out-of-sample prediction\n\n## Next lecture\n\n1. How to choose the compact domain $\\Omega$?\n2. How to choose number of basis function $m$?\n3. Which covariance functions can we use HSGP for?\n4. How to implement HSGP?\n\n## References\n::: {#refs}\n:::\n",
    "supporting": [
      "21-scalable-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}