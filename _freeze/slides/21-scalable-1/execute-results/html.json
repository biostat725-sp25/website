{
  "hash": "0cd0397a7242180c9a6020cd63fd2972",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scalable Gaussian Processes #1\"\nauthor: \"Christine Shen\"\ndate: \"2025-04-01\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of previous lectures {.midi}\n\nTwo weeks ago, we learned about:\n\n1.  Gaussian processes, and\n\n2.  How to use Gaussian processes for\n\n    -   longitudinal data\n    -   geospatial data\n\n## Motivating dataset {.midi}\n\nRecall we worked with a dataset on women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. Variables are:\n\n-   `loc_id`: location id (i.e. survey cluster).\n\n-   `hemoglobin`: hemoglobin level (g/dL).\n\n-   `anemia`: anemia classifications.\n\n-   `age`: age in years.\n\n-   `urban`: urban vs. rural.\n\n-   `LATNUM`: latitude.\n\n-   `LONGNUM`: longitude.\n\n## Motivating dataset {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  loc_id hemoglobin     anemia age urban   LATNUM  LONGNUM\n1      1       12.5 not anemic  28 rural 0.220128 21.79508\n2      1       12.6 not anemic  42 rural 0.220128 21.79508\n3      1       13.3 not anemic  15 rural 0.220128 21.79508\n4      1       12.9 not anemic  28 rural 0.220128 21.79508\n5      1       10.4       mild  32 rural 0.220128 21.79508\n6      1       12.2 not anemic  42 rural 0.220128 21.79508\n```\n\n\n:::\n:::\n\n\n\n::: callout-important\n## Modeling goals:\n\n-   Learn the associations between age and urbanicity and hemoglobin, accounting for unmeasured spatial confounders.\n\n-   Create a predicted map of hemoglobin across the spatial surface controlling for age and urbanicity, with uncertainty quantification.\n:::\n\n## Map of the Sud-Kivu state {.midi}\n\nLast time, we focused on one state with \\~500 observations at \\~30 locations.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Prediction for the Sud-Kivu state {.midi}\n\nAnd we created a $20 \\times 20$ grid for prediction of the spatial intercept surface over the Sud-Kivu state.\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=480}\n:::\n\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-4-2.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Map of the DRC {.midi}\n\nToday we will extend the analysis to the full dataset with \\~8,600 observations at \\~500 locations.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Prediction for the DRC {.midi}\n\nAnd we will make predictions on a $30 \\times 30$ grid over the DRC.\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=480}\n:::\n\n::: {.cell-output-display}\n![](21-scalable-1_files/figure-revealjs/unnamed-chunk-6-2.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Modeling {.midi}\n\n\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Data objects:**\n\n-   $i \\in \\{1,\\dots,n\\}$ indexes unique locations.\n\n-   $j \\in \\{1,\\dots,n_i\\}$ indexes individuals at each location.\n\n-   $Y_j(\\mathbf{u}_i)$ denotes the hemoglobin level of individual $j$ at location $\\mathbf{u}_i$.\n\n-   $\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10,\\text{urban}_i) \\in \\mathbb{R}^{1 \\times p}$, where $p=2$ is the number of predictors (excluding intercept).\n\n## Modeling {.midi}\n\n\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Population parameters:**\n\n-   $\\alpha \\in \\mathbb{R}$ is the intercept.\n\n-   $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the regression coefficients.\n\n-   $\\sigma^2 \\in \\mathbb{R}^+$ is the overall residual error (nugget).\n\n**Location-specific parameters:**\n\n-   $\\mathbf{u}_i = (\\text{longitude}_i,\\text{latitude}_i) \\in \\mathbb{R}^2$ denotes coordinates of location $i$.\n\n-   $\\theta(\\mathbf{u}_i)$ denotes the spatial intercept at location $\\mathbf{u}_i$.\n\n## Location-specific notation {.midi}\n\n$$\\mathbf{Y}(\\mathbf{u}_i) = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}(\\mathbf{u}_i), \\quad \\boldsymbol{\\epsilon}(\\mathbf{u}_i) \\sim N_{n_i}(\\mathbf{0},\\sigma^2\\mathbf{I})$$\n\n-   $\\mathbf{Y}(\\mathbf{u}_i) = (Y_1(\\mathbf{u}_i),\\ldots,Y_{n_i}(\\mathbf{u}_i))^\\top$.\n\n-   $\\mathbf{X}(\\mathbf{u}_i)$ is an $n_i \\times p$ dimensional matrix with rows $\\mathbf{x}_j(\\mathbf{u}_i)$.\n\n-   $\\boldsymbol{\\epsilon}(\\mathbf{u}_i) = (\\epsilon_i(\\mathbf{u}_i),\\ldots,\\epsilon_{n_i}(\\mathbf{u}_i))^\\top$.\n\n## Full data notation {.midi}\n\n$$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})$$\n\n-   $\\mathbf{Y} = (\\mathbf{Y}(\\mathbf{u}_1)^\\top,\\ldots,\\mathbf{Y}(\\mathbf{u}_{n})^\\top)^\\top \\in \\mathbb{R}^N$, with $N = \\sum_{i=1}^n n_i$.\n\n-   $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$ stacks $\\mathbf{X}(\\mathbf{u}_i)$.\n\n-   $\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top \\in \\mathbb{R}^n$.\n\n-   $\\mathbf{Z}$ is an $N \\times n$ dimensional block diagonal binary matrix. Each row contains a single 1 in column $i$ that corresponds to the location of $Y_j(\\mathbf{u}_i)$. $$\n    \\begin{align}\n    \\mathbf{Z} = \\begin{bmatrix}\n    \\mathbf{1}_{n_1} & \\mathbf{0} & \\dots & \\mathbf{0} \\\\\n    \\mathbf{0} & \\mathbf{1}_{n_2} & \\dots & \\mathbf{0}  \\\\\n    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    \\mathbf{0} & \\dots & \\mathbf{0} & \\mathbf{1}_{n_n}\n    \\end{bmatrix}.\n    \\end{align}\n    $$\n\n## Modeling {.midi}\n\nWe specify the following model: $$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})$$ with priors\n\n-   $\\boldsymbol{\\theta}(\\mathbf{u}) | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))$, where $C$ is the MatÃ©rn 3/2 covariance function with magnitude $\\tau$ and length scale $\\rho$.\n-   $\\alpha^* \\sim N(0,4^2)$. This is the intercept after centering $\\mathbf{X}$.\n-   $\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)$, $j \\in \\{1,\\dots,p\\}$\n-   $\\sigma \\sim \\text{Half-Normal}(0, 2^2)$\n-   $\\tau \\sim \\text{Half-Normal}(0, 4^2)$\n-   $\\rho \\sim \\text{Inv-Gamma}(5, 5)$\n-   $\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)$\n\n## Computational issues with GP {.midi}\n\nEffectively, the prior for $\\boldsymbol{\\theta}$ is $$\\boldsymbol{\\theta} | \\tau,\\rho \\sim N_n(\\mathbf{0},\\mathbf{C}), \\quad \\mathbf{C} \\in \\mathbb{R}^{n \\times n}.$$ MatÃ©rn 3/2 is an isotropic covariance function, $C(\\mathbf{u}_i, \\mathbf{u}_j) = C(\\|\\mathbf{u}_i-\\mathbf{u}_j\\|)$.\n\n$$\\mathbf{C} = \\begin{bmatrix}\nC(\\mathbf{0}) & C(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & \\cdots & C(\\|\\mathbf{u}_1 - \\mathbf{u}_n\\|)\\\\\nC(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & C(\\mathbf{0}) & \\cdots & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(\\|\\mathbf{u}_{1} - \\mathbf{u}_n\\|) & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|) & \\cdots & C(\\mathbf{0})\\\\\n\\end{bmatrix}.$$\n\nThis is not scalable because we need to invert an $n \\times n$ dense covariance matrix for each MCMC iteration, which requires $\\mathcal{O}(n^3)$ floating point operations (flops), and $\\mathcal{O}(n^2)$ memory.\n\n## Scalable GP methods overview {.midi}\n\nThe computational issues motivated exploration in scalable GP methods. Existing scalable methods broadly fall under two categories.\n\n::: incremental\n1.  Sparsity methods\n\n    -   sparsity in $\\mathbf{C}$, e.g., covariance tapering (@furrer2006covariance).\n    -   sparsity in $\\mathbf{C}^{-1}$, e.g., Vecchia approximation (@vecchia1988estimation) and nearest-neighbor GP (@datta2016hierarchical).\n\n2.  Low-rank methods\n\n    -   approximate $\\mathbf{C}$ on a low-dimensional subspace.\n    -   e.g., process convolution (@higdon2002space), inducing point method(@snelson2005sparse).\n:::\n\n## Hilbert space method for GP {.midi}\n\n-   @solin2020hilbert introduced a Hilbert space method for reduced-rank Gaussian process regression (HSGP).\n\n-   @riutort2023practical discussed how to practically implement HSGP.\n\n-   Tutorial codes are available in different probabilistic programming languages:\n\n    -   [stan](https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper)\n    -   [NumPyro](https://num.pyro.ai/en/0.15.2/examples/hsgp.html)\n    -   [pyMC](https://juanitorduz.github.io/hsgp_intro/)\n\n## Lecture plan {.midi}\n\nToday:\n\n-   How does HSGP work\n-   Why HSGP is scalable\n-   How to use HSGP for Bayesian geospatial model fitting and posterior predictive sampling\n\n::: fragment\nThursday:\n\n-   Parameter tuning for HSGP\n-   How to implement HSGP in `stan`\n:::\n\n## HSGP approximation {.midi}\n\nGiven:\n\n-   an isotropic covariance function $C$ which admits a *power spectral density*, e.g., the MatÃ©rn family, and\n-   a compact domain $\\boldsymbol{\\Theta} \\in \\mathbb{R}^d$ with *smooth* boundaries. For our purposes, we only consider *boxes*, e.g., $[-1,1] \\times [-1,1]$.\n\nHSGP approximates the $(i,j)$ element of the corresponding $n \\times n$ covariance matrix $\\mathbf{C}$ as $$\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).$$\n\n## HSGP approximation {.midi}\n\n$$\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).$$\n\n-   $s_k \\in \\mathbb{R}^+$ are positive scalars which depends on the covariance function $C$ and its parameters $\\tau$ and $\\rho$.\n-   $\\phi_k: \\boldsymbol{\\Theta} \\to \\mathbb{R}$ are *basis functions* which only depends on $\\boldsymbol{\\Theta}$.\n-   $m$ is the number of basis functions. Note: even with an infinite sum (i.e., $m \\to \\infty$), this remains an approximation (see @solin2020hilbert).\n\n## HSGP approximation {.midi}\n\nIn matrix notation,\n\n$$\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.$$\n\n-   $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}$ is a *feature matrix*. Only depends on $\\boldsymbol{\\Theta}$ and the observed locations.\n-   $\\mathbf{S} \\in \\mathbb{R}^{m \\times m}$ is diagonal. Depends on the covariance function $C$ and parameters $\\tau$ and $\\rho$.\n\n$$\n\\begin{align}\n  \\boldsymbol{\\Phi} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_1) & \\dots & \\phi_m(\\mathbf{u}_1) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_n) & \\dots & \\phi_m(\\mathbf{u}_n)\n  \\end{bmatrix}, \\quad \n  \\mathbf{S} = \\begin{bmatrix}\n  s_1 &  &  \\\\\n  & \\ddots &  \\\\\n  &  & s_m\n  \\end{bmatrix}.\n\\end{align}\n$$\n\n## Why HSGP is scalable {.midi}\n\n$$\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.$$\n\n-   $\\boldsymbol{\\Phi}$ only depends on $\\boldsymbol{\\Theta}$ and the observed locations, can be pre-calculated.\n-   No matrix inversion.\n-   Each MCMC iteration requires $\\mathcal{O}(nm + m)$ flops, vs $\\mathcal{O}(n^3)$ for a full GP.\n-   Ideally $m \\ll n$, but HSGP can be faster even for $m>n$.\n\n## Model reparameterization {.midi}\n\nUnder HSGP, approximately $$\\boldsymbol{\\theta} \\overset{d}{=} \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}, \\quad \\mathbf{b} \\sim N_m(0,\\mathbf{I}).$$\n\nTherefore we can reparameterize the model as\n\n$$\n\\begin{align}\n  \\mathbf{Y} &= \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon} \\\\\n  &\\approx \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\underbrace{\\mathbf{Z}\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}}_{\\mathbf{W}}\\mathbf{b} + \\boldsymbol{\\epsilon}\n\\end{align}\n$$\n\nNote the resemblance to linear regression:\n\n-   $\\mathbf{W} \\in \\mathbb{R}^{n \\times m}$ is a known design matrix given parameters $\\tau$ and $\\rho$.\n-   $\\mathbf{b}$ is an unknown parameter vector with prior $N_m(0,\\mathbf{I})$.\n\n## HSGP in `stan` {.small}\n\nSimilarly, we can use the reparameterized model in `stan`.\n\nThis is called the [*non-centered parameterization*](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#hierarchical-models-and-the-non-centered-parameterization) in `stan` documentation. It's recommended for computational efficiency for hierarchical models.\n\n\n\n::: {.cell output.var='model_in_stan'}\n\n```{.stan .cell-code}\ntransformed data {\n  matrix[n,m] PHI;\n  matrix[N,m] Z;\n  matrix[N,p] X_centered;\n}\nparameters {\n  real alpha_star;\n  real<lower=0> sigma;\n  vector[p] beta;\n  vector[m] b;\n  vector<lower=0>[m] sqrt_S;\n  ...\n}\nmodel {\n  vector[n] theta = PHI * (sqrt_S .* b);\n  target += normal_lupdf(y | alpha_star + X_centered * beta + Z * theta, sigma);\n  target += normal_lupdf(b | 0, 1);\n  ...\n}\n```\n:::\n\n\n\n## Posterior predictive distribution {.midi}\n\nWe want to make predictions for $\\mathbf{Y}^* = (Y(\\mathbf{u}_{n+1}),\\ldots, Y(\\mathbf{u}_{n+q}))^\\top$, observations at $q$ new locations. Define $\\boldsymbol{\\theta}^* = (\\theta(\\mathbf{u}_{n+1}),\\ldots,\\theta(\\mathbf{u}_{n+q}))^\\top$, $\\boldsymbol{\\Omega} = (\\alpha,\\boldsymbol{\\beta},\\sigma,\\tau,\\rho)$. Recall:\n\n\\begin{align*}\n  f(\\mathbf{Y}^* | \\mathbf{Y}) &= \\int f(\\mathbf{Y}^*, \\boldsymbol{\\theta}^*, \\boldsymbol{\\theta}, \\boldsymbol{\\Omega} | \\mathbf{Y}) d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n  &= \\int \\underbrace{f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})}_{(1)} \\underbrace{f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})}_{(2)} \\underbrace{f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})}_{(3)} d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n\\end{align*}\n\n(1) Likelihood: $f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})$ [-- remains the same as for GP]{.fragment data-fragment-index=\"1\" style=\"color: #a50f15;\"}\n\n(2) Kriging: $f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})$ [-- we will focus on this next]{.fragment data-fragment-index=\"2\" style=\"color: #a50f15;\"}\n\n(3) Posterior distribution: $f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})$ [-- we have just discussed]{.fragment data-fragment-index=\"0\" style=\"color: #a50f15;\"}\n\n## Kriging {.midi}\n\nRecall under the GP prior,\n\n$$\\begin{bmatrix}\n    \\boldsymbol{\\theta}\\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega} \\sim N_{n+q}\\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\mathbf{C} & \\mathbf{C}_{+}\\\\\n    \\mathbf{C}_{+}^\\top & \\mathbf{C}^*\n  \\end{bmatrix}\\right),$$\n\nwhere $\\mathbf{C}$ is the covariance of $\\boldsymbol{\\theta}$, $\\mathbf{C}^*$ is the covariance of $\\boldsymbol{\\theta}^*$, and $\\mathbf{C}_{+}$ is the cross covariance matrix between $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}^*$.\n\nTherefore by properties of multivariate normal, $$\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*},\\mathbb{V}_{\\boldsymbol{\\theta}^*}), \\quad \\text{where}$$ $$\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}^* - \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\mathbf{C}_+.\n\\end{align}\n$$\n\n## Kriging under HSGP {.midi}\n\nUnder HSGP, $\\mathbf{C}^* \\approx \\boldsymbol{\\Phi}^* \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}$, $\\mathbf{C}_+ \\approx \\boldsymbol{\\Phi} \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}$, where $$\n\\begin{align}\n  \\boldsymbol{\\Phi}^* \\in \\mathbb{R}^{q \\times m} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_{n+1}) & \\dots & \\phi_m(\\mathbf{u}_{n+1}) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_{n+q}) & \\dots & \\phi_m(\\mathbf{u}_{n+q})\n  \\end{bmatrix}\n\\end{align}\n$$ is the feature matrix for the new locations. Therefore approximately $$\n\\begin{align}\n  \\begin{bmatrix}\n    \\boldsymbol{\\theta} \\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega} \\sim N_{n\n  +q} \\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix},\n  \\begin{bmatrix}\n    \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top} \\\\\n    \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\n  \\end{bmatrix} \\right).\n\\end{align}\n$$\n\n## Kriging under HSGP {.midi}\n\nAgain by properties of multivariate normal, $$\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\overset{?}{\\sim} N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS},\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS}),$$\n\n$$\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{-1} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}) - (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{-1}(\\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top}).\n\\end{align}\n$$\n\n::: incremental\n-   If $m \\ge n$, $(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)$ is invertible, this is the kriging distribution under HSGP.\n-   But what if $m < n$?\n:::\n\n## Kriging under HSGP {.midi}\n\nIf $m \\le n$, claim $\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta},$ where $\\mathbf{A}^\\dagger$ denotes a generalized inverse of matrix $\\mathbf{A}$ such that $\\mathbf{A}\\mathbf{A}^{\\dagger}\\mathbf{A} = \\mathbf{A}$. Sketch proof below, see details in class.\n\n1.  By properties of multivariate normal, $\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS},\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS})$, $$\n    \\begin{align}\n      \\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}\\\\\n      \\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}) - (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger \\top}(\\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top}).\n    \\end{align}\n    $$\n\n2.  Show if $\\boldsymbol{\\Phi}$ has full column rank, which is true under HSGP, then $$\n    \\begin{align}\n      \\mathbf{S} \\boldsymbol{\\Phi}^\\top(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger \\top}\\boldsymbol{\\Phi}\\mathbf{S} = \\mathbf{S} \\tag{1} \\\\\n      \\mathbf{S} \\boldsymbol{\\Phi}^\\top(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}\\boldsymbol{\\Phi}\\mathbf{S} = \\mathbf{S} \\tag{2}.\n    \\end{align}\n    $$ Equation (1) is sufficient to show $\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} \\equiv \\mathbf{0}$.\n\n## Kriging under HSGP {.midi}\n\nUnder the reparameterized model, $\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}$, for $\\mathbf{b} \\sim N_m(0,\\mathbf{I}).$ Therefore $$\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta} \\\\\n  &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}(\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}) \\\\\n  &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}. \\quad (\\text{by equation (2) in the last slide})\n\\end{align}\n$$\n\nDuring MCMC sampling, we can obtain posterior predictive samples for $\\boldsymbol{\\theta}^*$ through posterior samples of $\\mathbf{b}$ and $\\mathbf{S}$. Let superscript $(s)$ denote the $s$th posterior sample:\n\n$$\\boldsymbol{\\theta}^{*(s)} = \\boldsymbol{\\Phi}^* \\mathbf{S}^{(s) 1/2} \\mathbf{b}^{(s)}.$$\n\n## Kriging under HSGP -- alternative view {.midi}\n\nUnder the reparameterized model, there is another (perhaps more intuitive) way to recognize the kriging distribution under HSGP when $m \\le n$.\n\nWe model $\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}$, where $\\mathbf{b}$ is treated as the unknown parameter. Therefore for kriging: $$\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b} \\mid (\\mathbf{b},\\mathbf{S},\\boldsymbol{\\Omega}) \\\\\n  &=\\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}.\n\\end{align}\n$$\n\n## HSGP kriging in `stan` {.midi}\n\nIf $m \\le n$, kriging under HSGP can be easily implemented in `stan`.\n\n\n\n::: {.cell output.var='model_in_stan'}\n\n```{.stan .cell-code}\ntransformed data {\n  matrix[q,m] PHI_new;\n  ...\n}\nparameters {\n  vector[m] b;\n  vector<lower=0>[m] sqrt_S;\n  ...\n}\nmodel {\n  ...\n}\ngenerated quantities {\n  vector[q] theta_new = PHI_new * (sqrt_S .* b);\n}\n\n```\n:::\n\n\n\n::: fragment\nIf $m>n$, we need to invert an $n \\times n$ matrix $(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)$ for kriging, which could be computationally prohibitive.\n:::\n\n## Recap {.midi}\n\nHSGP is a low rank approximation method for GP.\n\n$$\n\\begin{align}\n  \\mathbf{C}_{ij} \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j), \\quad \\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top,\n\\end{align}\n$$\n\n-   for covariance function $C$ which admits a power spectral density.\n-   on a box $\\boldsymbol{\\Theta} \\subset \\mathbb{R}^d$.\n-   with $m$ number of basis functions.\n\nWe have talked about:\n\n-   why HSGP is scalable.\n-   how to do posterior sampling and posterior predictive sampling in `stan`.\n\n## HSGP parameters {.midi}\n\n@solin2020hilbert showed that HSGP approximation can be made arbitrarily accurate as $\\boldsymbol{\\Theta}$ and $m$ increase.\n\n::: fragment\nBut how to choose:\n\n-   size of the box $\\boldsymbol{\\Theta}$.\n-   number of basis functions $m$.\n:::\n\n:::: fragment\n::: callout-important\n## Our goal:\n\nMinimize the run time while maintaining reasonable approximation accuracy.\n:::\n::::\n\n::: fragment\n*Note: we treat estimation of the GP magnitude parameter* $\\tau$ as a separate problem, and only consider approximation accuracy of HSGP in terms of the correlation function.\n:::\n\n## Prepare for next class {.midi}\n\n1.  Work on HW 05 which is due Apr 8\n\n2.  Complete reading to prepare for Thursday's lecture\n\n3.  Thursday's lecture:\n\n    -   Parameter tuning for HSGP\n    -   How to implement HSGP in `stan`\n\n## References\n\n::: {#refs}\n:::\n",
    "supporting": [
      "21-scalable-1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}