{
  "hash": "80a0d2dc3b82584aa3d8d312b7dadc75",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Disease Mapping\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-03-25\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last week\n\n-   Last week, we learned about Gaussian processes.\n\n-   We learned how to apply Gaussian processes to longitudinal (or time-series) and geospatial data.\n\n-   Focused on making predictions at new locations across the spatial surface.\n\n-   Today we will focus on areal spatial data, which has different goals associated with it than point-referenced spatial data.\n\n## Lattice Data (Areal Data)\n\n-   Data observed at the level of an areal unit\n\n    -   County Level Sudden Infant Death Syndrome Counts\n\n![](./images/18/lattice_data.png){fig-align=\"center\" height=\"450\"}\n\n## Lattice Data (Areal Data)\n\n-   Birmingham Tract Level Poverty Levels\n\n![](./images/18/Birmingham_Poverty.png){fig-align=\"center\" height=\"500\"}\n\n## Goals of Areal Spatial Data Analysis\n\nThe goal of **areal spatial data analysis** is to understand how spatial patterns (e.g., mortality rates, disease incidence) vary across different geographic areas (e.g., counties, neighborhoods).\n\nIt helps us identify:\n\n-   **Clusters**: Areas with similar characteristics (e.g., high mortality, disease prevalence).\n\n-   **Outliers**: Areas that deviate significantly from the overall pattern (e.g., unexpectedly high mortality rates).\n\n-   **Spatial Dependence**: Whether values in one area are correlated with values in nearby areas (e.g., neighboring counties with similar health outcomes).\n\n## Why We Care About Spatial Patterns\n\n-   **Local Insights**: Spatial analysis helps identify **local variations** in health outcomes that may not be apparent when analyzing data at a higher (e.g., state or national) level.\n\n-   **Targeted Interventions**: Understanding spatial patterns allows for **targeted public health interventions** tailored to regions that need attention (e.g., areas with unusually high mortality rates).\n\n-   **Identifying Spatial Clusters**: By recognizing **clusters of high or low rates**, we can investigate potential **common causes** (e.g., environmental factors, access to healthcare, socioeconomic conditions).\n\n## Motivating Data {.midi}\n\nToday, we will motivate areal spatial data analysis and disease mapping by studying 2020 COVID mortality at the county-level in North Carolina. The data object `covid_nc_2020` is an `sf` object.\n\n-   Variables are:\n\n    -   `name`: county name.\n\n    -   `population`: 2020 population.\n\n    -   `obs_deaths`: observed number of COVID-related deaths in 2020.\n\n    -   `est_deaths`: estimated number of COVID-related deaths in 2020.\n\n    -   `smr`: standardized mortality ratio.\n\n    -   `age`: precentage of residents over 60 years of age.\n\n    -   `poverty`: percentage of residents below the poverty line.\n\n    -   `geometry`: contains centroid and boundary information for each county.\n\n## COVID Mortality\n\n<!-- - Account for noise due to spatial variability in order to provide smoothed estimates across space -->\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Introduction to Disease Mapping\n\n-   Disease mapping is a way of visualizing and analyzing geographic variations in health outcomes, such as mortality or disease incidence, across different regions (e.g., counties or neighborhoods).\n\n-   It helps us identify regions with unusually high or low health outcomes, which could be indicative of underlying health disparities.\n\n## The Challenge with Observed Data\n\nImagine you want to compare the number of deaths across counties in a state, like North Carolina. If we simply look at **observed death counts**, we might be misled:\n\n-   Larger counties with more people may have more deaths simply due to their larger population.\n\n-   Smaller counties may appear \"healthier\" simply because they have fewer people, not because they have lower mortality rates.\n\nThus, observed death counts are **not enough** to draw meaningful comparisons.\n\n## The Challenge with Observed Data\n\n-   To make fair comparisons between regions of different sizes, we need to **adjust for population size** (and sometimes demographics).\n\n-   Without these adjustments, it's hard to determine if a county's high death count is due to its population size or if there's something unique about the county (e.g., healthcare access, environmental factors) that increases the risk of mortality.\n\n-   This is where we need more **nuanced measures** to adjust for population size and allow for better comparisons.\n\n-   Today we will talk about the standardized mortality ratio (SMR).\n\n## Standardized Mortality Ratio\n\n-   SMR is a way of comparing the observed number of deaths in a population to the number of deaths we would expect, given the population's characteristics (such as population size).\n\n-   It adjusts for differences in population, allowing us to identify areas where deaths are higher or lower than we would expect.\n\n$$\\text{SMR} = \\frac{\\text{Observed Deaths}}{\\text{Expected Deaths}}$$\n\n-   **Expected Deaths** is calculated by multiplying the total deaths across the state by the proportion of the population in that county.\n\n## Example Data {.midi}\n\n| County   | Observed Deaths | Population | Population Proportion |\n|----------|-----------------|------------|-----------------------|\n| County A | 10              | 30,000     | 0.3                   |\n| County B | 15              | 50,000     | 0.5                   |\n| County C | 5               | 20,000     | 0.2                   |\n| Total    | 30              | 100,000    | 1.0                   |\n\n## Step 1 - Calculate Expected Deaths\n\nThe **Expected Deaths** for each county are calculated by multiplying the **total deaths** by the **population proportion** for that county:\n\n\\begin{align*}\n\\text{Expected Deaths for County A} = 30 \\times 0.3 &= 9\\\\\n\\text{Expected Deaths for County B} = 30 \\times 0.5 &= 15\\\\\n\\text{Expected Deaths for County C} = 30 \\times 0.2 &= 6\n\\end{align*}\n\n## Step 2 - Compute SMR\n\nNow, we calculate the **SMR** by dividing the **observed deaths** by the **expected deaths**:\n\n\\begin{align*}\n\\text{SMR for County A} &= \\frac{10}{9} = 1.11\\\\\n\\text{SMR for County B} &= \\frac{15}{15} = 1\\\\\n\\text{SMR for County C} &= \\frac{5}{6} = 0.83\n\\end{align*}\n\nWhat do these numbers mean?\n\n## Interpreting SMR\n\n-   **SMR = 1**: The observed number of deaths matches the expected number of deaths.\n\n-   **SMR \\> 1**: More deaths than expected (excess mortality).\n\n-   **SMR \\< 1**: Fewer deaths than expected (lower mortality).\n\nIn our example:\n\n-   **County A** has excess mortality, with SMR of **1.11**.\n\n-   **County B** has as many deaths as expected, with SMR of **1**.\n\n-   **County C** has fewer deaths than expected, with SMR of **0.83**.\n\n## Why Use SMR in Disease Mapping?\n\n**SMR** allows us to:\n\n-   Make meaningful comparisons across counties of different sizes.\n\n-   Identify areas with **excess mortality** (SMR \\> 1) and areas with **lower-than-expected mortality** (SMR \\< 1).\n\nIn disease mapping, **SMR** helps us better understand **spatial health disparities** and identify regions that may need targeted public health interventions.\n\n## Standardized Mortality Ratios\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Writing down a model for SMR\n\nDefine $Y_i$ and $E_i$ as the observed and expected mortality counts at county $i$ ($i = 1\\ldots,n$). We can model the observed counts as follows:\n\n$$Y_i | \\lambda_i \\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i).$$\n\n-   Recall that for a random variable $Y \\sim \\text{Poisson}(\\lambda)$, $\\mathbb{E}[Y] = \\lambda$ and $\\mathbb{V}(Y) = \\lambda$.\n\n-   We have: $\\mathbb{E}[Y_i | \\lambda_i] = E_i \\lambda_i \\implies \\mathbb{E}\\left[(Y_i / E_i) | \\lambda_i\\right] = \\lambda_i.$\n\n    -   Under this parameterization $\\lambda_i$ is the SMR.\n\n## Disease Mapping Model\n\nThe parameter $\\lambda_i$, sometimes also called **relative risk**, is modeled as follows:\n\n\\begin{align*}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i, \\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\nwhere $\\mathbf{x}_i \\in \\mathbb{R}^{p \\times 1}$ contains county-level predictors.\n\n**Population parameters**:\n\n-   $\\alpha \\in \\mathbb{R}$ is a population intercept.\n\n-   $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is a vector of population coefficients.\n\n-   $\\sigma \\in \\mathbb{R}^+$ is a residual error term.\n\n## Disease Mapping Model\n\nThe parameter $\\lambda_i$, sometimes also called **relative risk**, is modeled as follows:\n\n\\begin{align*}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Spatial Error Term**:\n\n-   $\\theta_i \\in \\mathbb{R}$ is a location-specific parameter that smooths data according to a neighborhood structure. \n\n  -   $\\theta_i$ induces spatial correlation, such that $\\lambda_i$ in neighboring areas will be more similar.\n\n## Spatial Correlation: Areal Data\n\n-   How to induce spatial correlation between areal units?\n\n    -   Distances between centroids (possibly population weighted); may be inappropriate for oddly shaped regions of varying sizes (great for equal sized grid though).\n\n    -   Neighborhood structure of your spatial region; are two regions neighbors?\n\n-   Correlation introduced through spatial random effects.\n\n-   The default model for areal data in the Bayesian setting is called the conditionally autoregressive (CAR) model.\n\n## Adjacency Matrix\n\n- We will define the matrix $\\mathbf{W} \\in \\mathbb{R}^{n \\times n}$ as the **adjacency matrix**. \n\n  - This is sometimes called a **proximity matrix** or **neighborhood matrix**. \n  \n- Each entry ($w_{ij} = [\\mathbf{W}]_{ij}$) is given by: $$w_{ij} = 1(i \\sim j) = \\left\\{ \\begin{array}{ll}\n         1 & \\mbox{if $i$ and $j$ share a border};\\\\\n         0 & \\mbox{otherwise}.\\end{array} \\right.$$\n\n- In some cases, $w_{ij}$ can be generalized to be non-binary. \n\n## Compute Adjacency Matrix\n\nTo compute the adjacency matrix of an `sf` data object we can use the `spdep` library.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneighbors <- spdep::poly2nb(covid_nc_2020) # computes the neighborhood structure\nW <- spdep::nb2mat(neighbors, style = \"B\", zero.policy = TRUE) # converts to an n x n matrix\n```\n:::\n\n\n\n- `style = \"B\"` specifies binary encoding (1 if neighbors, 0 if not).\n\n- `zero.policy = TRUE` ensures the function works even if some counties do not have neighbors.\n\n## Visualzing the Adjacency Matrix\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Visualzing the Adjacency Matrix\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## ICAR Model\n\nToday, we will look at the intrinsic CAR (ICAR) process for a vector $\\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_n)^\\top$, $\\boldsymbol{\\theta} | \\tau^2 \\sim \\text{ICAR}\\left(\\tau^2\\right)$. Under this specification, the following joint distribution is given:\n\n$$f(\\boldsymbol{\\theta} | \\tau^2) \\propto \\exp\\left\\{-\\frac{1}{2\\tau^2}\\boldsymbol{\\theta}^\\top \\left(\\mathbf{D}_w - \\mathbf{W}\\right) \\boldsymbol{\\theta}\\right\\},$$\n\nwhere $\\mathbf{D}_w$ is diagonal with $[\\mathbf{D}_w]_{ii} = w_{i+}$ and $w_{i+} = \\sum_{j=1}^n w_{ij}$ (i.e., $w_{i+}$ is the number of neighbors for locations $i$).\n\n- $\\left(\\mathbf{D}_w - \\mathbf{W}\\right)$ is singular, so $\\left(\\mathbf{D}_w - \\mathbf{W}\\right)^{-1}$ does not exist and this distribution is improper. \n\n- We can still use this as a prior for $\\boldsymbol{\\theta}$ and get a proper posterior!\n\n## ICAR Model: Conditional Distributinos\n\nThe joint distribution on the previous slide can be written as a $n$ conditional distributions: \n\n$$\\theta_{i} | \\boldsymbol{\\theta}_{-i}, \\tau^2 \\sim  N \\left({\\frac{\\sum_{j=1}^n w_{ij}\\theta_{j}}{w_{i+}}},\\frac{\\tau^2}{w_{i+}}\\right)$$\n\n-   $\\boldsymbol\\theta_{-j}$: Vector of $\\theta_{i}$ parameters with $\\theta_{j}$ removed.\n\n-   The mean is an average of the neighbors values.\n\n-   The variance shrinks as a function of the number of neighbors. \n\n## Another Equivalent Specification\n\nPairwise difference specification: \n\n$$f(\\boldsymbol{\\theta} | \\tau^2) \\propto \\exp\\left\\{-\\frac{1}{2\\tau^2}\\sum_{i \\sim j} w_{ij} (\\theta_i - \\theta_j)^2\\right\\},$$\n\n- The impropriety of the distribution can also be seen here, because we can add any constant to all $\\theta_i$ and the distribution is unaffected. \n\n- A constraint such as $\\sum_{i=1}^n \\theta_i = 0$ would provide the needed centering. \n\n- We will use this specification in Stan.\n\n## Full Disease Mapping Model\n\nThe full model can be written as:\n\n\\begin{align*}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n\\boldsymbol{\\theta} | \\tau &\\sim \\text{ICAR}\\left(\\tau^2\\right)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\\\\\n\\end{align*}\n\nwhere $\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta}, \\sigma, \\tau)$.\n\n- $\\mu_i = \\exp\\{\\log E_i + \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\theta_i + \\epsilon_i\\}$.\n\n## Posterior Distribution\n\nDefine $\\mathbf{Y} = (Y_1,\\ldots,Y_n)$. The posterior can be written as:\n\n\\begin{align*}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega},\\boldsymbol{\\theta})\\\\\n&\\propto f(\\mathbf{Y} | \\boldsymbol{\\Omega},\\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega})\\\\\n&\\propto f(\\boldsymbol{\\Omega}) \\prod_{i=1}^n f({Y}_i | \\lambda_i) f(\\theta_i | \\tau^2) .\n\\end{align*}\n\n## Adding the ICAR prior to Stan\n\nWe will use the pairwise differences specification, so we need the unique pairs of neighbors. We will define $n_{edges}$ as the number of non-zero edges. The following is added to the Stan data code chunk.\n\n\n\n::: {.cell output.var='icar'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> n;\n  int<lower=0> n_edges;\n  array[n_edges] int<lower = 1, upper = n> node1; // node1[i] adjacent to node2[i]\n  array[n_edges] int<lower = 1, upper = n> node2; // and node1[i] < node2[i]\n  ...\n}\n```\n:::\n\n\n\n## Extracting non-zero edges for Stan\n\nOur goal is to get the row-column pairs from $\\mathbf{W}$ where the $w_{ij} = 1$. This will return all non-zero indices in the adjacency matrix.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneighbor_pairs <- which(W == 1, arr.ind = TRUE)\n```\n:::\n\n\n\nSince $\\mathbf{W}$ is symmetric, we only need to keep the edges from above the diagonal to avoid repeating edges.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nneighbor_pairs_lower <- neighbor_pairs[neighbor_pairs[, 1] < neighbor_pairs[, 2], ]\nn_edges <- nrow(neighbor_pairs_lower)\nnode1 <- neighbor_pairs_lower[, 1]\nnode2 <- neighbor_pairs_lower[, 2]\n```\n:::\n\n\n\n## Adding the ICAR prior to Stan\n\nWe can then add the following to the parameters and model Stan code chunks, where we leverage Stan's ability to perform multi-indexing and vectorization!\n\n\n\n::: {.cell output.var='icar'}\n\n```{.stan .cell-code}\nparameters {\n  vector[n] z;\n}\ntransformed parameters {\n  vector[n] theta = tau * z;\n}\nmodel {\n  target += -0.5 * dot_self(z[node1] - z[node2]);\n  // soft sum-to-zero constraint on z,\n  // equivalent to mean(z) ~ normal(0,0.01)\n  sum(z) ~ normal(0, 0.01 * n);\n}\n```\n:::\n\n\n\n## Modeling {.midi}\n\nWe specify the following model:\n\n\\begin{align*}\nY_i | \\lambda_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i,\\quad \\epsilon_i \\stackrel{iid}{\\sim} N(0,\\sigma^2)\\\\\n\\boldsymbol{\\theta} | \\tau &\\sim \\text{ICAR}\\left(\\tau^2\\right)\\\\\n\\alpha^* &\\sim N(0,3^2)\\\\\n\\beta_j &\\sim N(0,3^2), \\quad j = 1,\\ldots,p\\\\\n\\sigma &\\sim \\text{Half-Normal}(0, 3^2)\\\\\n\\tau &\\sim \\text{Half-Normal}(0, 3^2)\\\\\n\\end{align*}\n\nWhere $n = 100$, $\\mathbf{x}_i = (\\text{age}_{i}, \\text{poverty}_i)$.\n\n## Full Stan Model for ICAR\n\n\n\n::: {.cell output.var='full_icar'}\n\n```{.stan .cell-code}\n// saved in icar.stan\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int<lower = 0> n_edges;\n  array[n_edges] int<lower = 1, upper = n> node1; // node1[i] adjacent to node2[i]\n  array[n_edges] int<lower = 1, upper = n> node2; // and node1[i] < node2[i]\n  array[n] int<lower = 0> Y;\n  vector<lower = 0>[n] E;\n  matrix[n, p] X;\n}\ntransformed data {\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n  vector[n] logE = log(E);\n}\nparameters {\n  real alpha_star;\n  vector[p] beta;\n  real<lower = 0> sigma; // precision of heterogeneous effects\n  real<lower = 0> tau; // precision of spatial effects\n  vector[n] z1; // spatial effects\n  vector[n] z2; // heterogeneous effects\n}\ntransformed parameters {\n  vector[n] theta = tau * z1;     // spatial effects\n  vector[n] epsilon = sigma * z2; // heterogeneous effects\n}\nmodel {\n  Y ~ poisson_log(logE + alpha_star + X_centered * beta + theta + epsilon);\n  // the following computes the ICAR prior on theta (through the standardized version z1)\n  target += -0.5 * dot_self(z1[node1] - z1[node2]);\n  // soft sum-to-zero constraint on theta)\n  sum(z1) ~ normal(0, 0.001 * n); // equivalent to mean(z1) ~ normal(0, 0.001)\n  // heterogeneous effects\n  z2 ~ std_normal();\n  // population parameters\n  alpha_star ~ normal(0, 3);\n  beta ~ normal(0, 3);\n  sigma ~ normal(0, 3);\n  tau ~ normal(0, 3);\n}\ngenerated quantities {\n  real alpha = alpha_star - X_bar * beta;\n  vector[n] log_mu = logE + alpha_star + X_centered * beta + theta + epsilon;\n  vector[n] lambda = exp(log_mu - logE);\n  vector[n] mu = exp(log_mu);\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = poisson_log_rng(log_mu[i]);\n    log_lik[i] = poisson_log_lpmf(Y[i] | log_mu[i]);\n  }\n}\n```\n:::\n\n\n\n## Fit the Stan Model\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(~ age + poverty, data = covid_nc_2020)[, -1]\nstan_data <- list(\n  n = nrow(covid_nc_2020),\n  p = ncol(X),\n  n_edges = nrow(neighbor_pairs_lower),\n  node1 = neighbor_pairs_lower[, 1],\n  node2 = neighbor_pairs_lower[, 2],\n  Y = covid_nc_2020$obs_deaths,\n  E = covid_nc_2020$est_deaths,\n  X = X\n)\nicar <- stan_model(\"icar.stan\")\nfit_icar <- sampling(icar, stan_data, pars = c(\"z1\", \"z2\", \"epsilon\", \"log_mu\", \"lp__\"), include = FALSE, iter = 10000)\n```\n:::\n\n\n\n## Examine model summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit_icar, pars = c(\"alpha\", \"alpha_star\", \"beta\", \"sigma\", \"tau\"))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nalpha      -0.99    0.01 0.42 -1.81 -1.27 -0.99 -0.71 -0.17  6567    1\nalpha_star  0.08    0.00 0.05 -0.01  0.05  0.08  0.12  0.18  6187    1\nbeta[1]     0.99    0.01 1.05 -1.06  0.28  0.99  1.70  3.06  5966    1\nbeta[2]     4.66    0.01 1.13  2.40  3.92  4.67  5.43  6.86  6293    1\nsigma       0.42    0.00 0.06  0.30  0.39  0.43  0.46  0.53  1087    1\ntau         0.25    0.01 0.17  0.01  0.11  0.22  0.36  0.64   550    1\n\nSamples were drawn using NUTS(diag_e) at Fri Mar 21 15:37:56 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Examine traceplots\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrstan::traceplot(fit_icar, pars = c(\"alpha\", \"alpha_star\", \"beta\", \"sigma\", \"tau\"))\n```\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-17-1.png){width=960}\n:::\n:::\n\n\n\n## Looking at SMR observed versus $\\lambda_i$\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Posterior SMR Across North Carolina\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-19-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-19-2.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-19-3.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-19-4.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Mapping $P(\\lambda_i > 1 | \\mathbf{Y})$\n\n\n\n::: {.cell layout-nrow=\"1\" layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-20-1.png){fig-align='center' width=1056}\n:::\n:::\n\n\n\n## Mapping $P(\\lambda_i > 1 | \\mathbf{Y})$\n\nBinary indicator of $P(\\lambda_i > 1 | \\mathbf{Y}) > 0.95$:\n\n\n\n::: {.cell layout-nrow=\"1\" layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-21-1.png){fig-align='center' width=1056}\n:::\n:::\n\n\n\n## Prepare for next class\n\n-   Work on HW 05, which is due April 8.\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Guest lecture by Prof. Hwanhee Hong on Bayesian Meta-Analysis\n",
    "supporting": [
      "19-disease-mapping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}