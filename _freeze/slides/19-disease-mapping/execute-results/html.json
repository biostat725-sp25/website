{
  "hash": "125c9340c6d2eba1936657d34961891c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Disease Mapping\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-03-25\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   During our last lecture, we learned about Gaussian processes.\n\n-   We learned how to apply Gaussian processes to longitudinal (or time-series) data.\n\n-   The longitudinal setting is one-dimensional (i.e., time). Today we will learn about applying Gaussian processes in two-dimensions (i.e. space).\n\n## Lattice Data (Areal Data)\n\n-   Data observed at the level of an areal unit\n\n    -   County Level Sudden Infant Death Syndrome Counts\n\n![](./images/18/lattice_data.png){fig-align=\"center\" height=\"450\"}\n\n## Lattice Data (Areal Data)\n\n-   Birmingham Tract Level Poverty Levels\n\n![](./images/18/Birmingham_Poverty.png){fig-align=\"center\" height=\"500\"}\n\n## Goals of Areal Spatial Data Analysis\n\nThe goal of **areal spatial data analysis** is to understand how spatial patterns (e.g., mortality rates, disease incidence) vary across different geographic areas (e.g., counties, neighborhoods).\n\nIt helps us identify:\n\n- **Clusters**: Areas with similar characteristics (e.g., high mortality, disease prevalence).\n\n- **Outliers**: Areas that deviate significantly from the overall pattern (e.g., unexpectedly high mortality rates).\n\n- **Spatial Dependence**: Whether values in one area are correlated with values in nearby areas (e.g., neighboring counties with similar health outcomes).\n\n## Why We Care About Spatial Patterns\n\n- **Local Insights**: Spatial analysis helps identify **local variations** in health outcomes that may not be apparent when analyzing data at a higher (e.g., state or national) level.\n\n- **Targeted Interventions**: Understanding spatial patterns allows for **targeted public health interventions** tailored to regions that need attention (e.g., areas with unusually high mortality rates).\n\n- **Identifying Spatial Clusters**: By recognizing **clusters of high or low rates**, we can investigate potential **common causes** (e.g., environmental factors, access to healthcare, socioeconomic conditions).\n\n## Motivating Data {.midi}\n\nToday, we will motivate areal spatial data analysis and disease mapping by studying 2020 COVID mortality at the county-level in North Carolina. The data object `covid_nc_2020` is an `sf` object.\n\n-   Variables are:\n\n    -   `name`: county name.\n\n    -   `population`: 2020 population.\n\n    -   `obs_deaths`: observed number of COVID-related deaths in 2020.\n\n    -   `est_deaths`: estimated number of COVID-related deaths in 2020.\n\n    -   `smr`: standardized mortality ratio.\n\n    -   `age`: precentage of residents over 60 years of age.\n\n    -   `poverty`: percentage of residents below the poverty line.\n    \n    -   `geometry`: contains centroid and boundary information for each county.\n\n## COVID Mortality\n\n<!-- - Account for noise due to spatial variability in order to provide smoothed estimates across space -->\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Introduction to Disease Mapping\n\n- Disease mapping is a way of visualizing and analyzing geographic variations in health outcomes, such as mortality or disease incidence, across different regions (e.g., counties or neighborhoods). \n\n- It helps us identify regions with unusually high or low health outcomes, which could be indicative of underlying health disparities.\n\n## The Challenge with Observed Data\n\nImagine you want to compare the number of deaths across counties in a state, like North Carolina. If we simply look at **observed death counts**, we might be misled:\n\n- Larger counties with more people may have more deaths simply due to their larger population.\n\n- Smaller counties may appear \"healthier\" simply because they have fewer people, not because they have lower mortality rates.\n\nThus, observed death counts are **not enough** to draw meaningful comparisons.\n\n## The Challenge with Observed Data\n\n- To make fair comparisons between regions of different sizes, we need to **adjust for population size** (and sometimes demographics). \n\n- Without these adjustments, it's hard to determine if a county's high death count is due to its population size or if there's something unique about the county (e.g., healthcare access, environmental factors) that increases the risk of mortality.\n\n- This is where we need more **nuanced measures** to adjust for population size and allow for better comparisons.\n\n- Today we will talk about the standardized mortality ratio (SMR).\n\n## Standardized Mortality Ratio\n\n- SMR is a way of comparing the observed number of deaths in a population to the number of deaths we would expect, given the population's characteristics (such as population size).\n\n- It adjusts for differences in population, allowing us to identify areas where deaths are higher or lower than we would expect.\n\n$$\\text{SMR} = \\frac{\\text{Observed Deaths}}{\\text{Expected Deaths}}$$\n\n- **Expected Deaths** is calculated by multiplying the total deaths across the state by the proportion of the population in that county.\n\n## Example Data {.midi}\n\n| County     | Observed Deaths | Population | Population Proportion |\n|------------|-----------------|------------|-----------------------|\n| County A   | 10              | 30,000     | 0.3                   |\n| County B   | 15              | 50,000     | 0.5                   |\n| County C   | 5               | 20,000     | 0.2                   |\n| Total      | 30              | 100,000    | 1.0                   |\n\n## Step 1 - Calculate Expected Deaths\n\nThe **Expected Deaths** for each county are calculated by multiplying the **total deaths** by the **population proportion** for that county:\n\n\\begin{align*}\n\\text{Expected Deaths for County A} = 30 \\times 0.3 &= 9\\\\\n\\text{Expected Deaths for County B} = 30 \\times 0.5 &= 15\\\\\n\\text{Expected Deaths for County C} = 30 \\times 0.2 &= 6\n\\end{align*}\n\n## Step 2 - Compute SMR\n\nNow, we calculate the **SMR** by dividing the **observed deaths** by the **expected deaths**:\n\n\\begin{align*}\n\\text{SMR for County A} &= \\frac{10}{9} = 1.11\\\\\n\\text{SMR for County B} &= \\frac{15}{15} = 1\\\\\n\\text{SMR for County C} &= \\frac{5}{6} = 0.83\n\\end{align*}\n\nWhat do these numbers mean?\n\n## Interpreting SMR\n\n- **SMR = 1**: The observed number of deaths matches the expected number of deaths.\n\n- **SMR > 1**: More deaths than expected (excess mortality).\n\n- **SMR < 1**: Fewer deaths than expected (lower mortality).\n\nIn our example:\n\n- **County A** has excess mortality, with SMR of **1.11**.\n\n- **County B** has as many deaths as expected, with SMR of **1**.\n\n- **County C** has fewer deaths than expected, with SMR of **0.83**.\n\n## Why Use SMR in Disease Mapping?\n\n**SMR** allows us to:\n\n- Make meaningful comparisons across counties of different sizes.\n\n- Identify areas with **excess mortality** (SMR > 1) and areas with **lower-than-expected mortality** (SMR < 1).\n\nIn disease mapping, **SMR** helps us better understand **spatial health disparities** and identify regions that may need targeted public health interventions.\n\n## Standardized Mortality Ratios\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Writing down a model for SMR\n\nDefine $Y_i$ and $E_i$ as the observed and expected mortality counts at county $i$ ($i = 1\\ldots,n$). We can model the observed counts as follows:\n\n$$Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i).$$\n\n- Recall that for a random variable $Y \\sim \\text{Poisson}(\\lambda)$, $\\mathbb{E}[Y] = \\lambda$ and $\\mathbb{V}(Y) = \\lambda$.\n\n- For our model, we have: $\\mathbb{E}[Y_i] = E_i \\lambda_i \\implies \\mathbb{E}\\left[\\frac{Y_i}{E_i}\\right] = \\lambda_i$.\n\n  - Under this parameterization $\\lambda_i$ is the SMR. \n\n## Disease Mapping Model\n\nThe parameter $\\lambda_i$, sometimes also called relative risk, is modeled as follows: \n\n\\begin{align*}\nY_i &\\stackrel{ind}{\\sim} \\text{Poisson}(E_i \\lambda_i)\\\\\n\\log \\lambda_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\theta_i + \\epsilon_i\n\\end{align*}\n\n- $\\mathbf{x}_i \\in \\mathbb{R}^{p \\times 1}$ contains county-level predictors.\n\n- $\\alpha \\in \\mathbb{R}$ is a population intercept.\n\n- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is a vector of population coefficients.\n\nBayesian hierarchical models enable to obtain smoothed disease relative risks by including covariates and random effects to borrow information from neighboring areas. Spatial disease risk models are commonly specified using a Poisson distribution for the observed number of cases ($Y_i$) with mean equal to the expected number of cases ($E_i$) times the relative risk ($\\lambda_i$) corresponding to area $i$, $i = 1,\\ldots,n$,\n\nHere, the logarithm of $\\lambda_i$ is expressed as a sum of fixed effects to quantify the effects of the covariates on the disease risk, and random effects that represent residual variation that is not explained by the available covariates. The fixed effects are expressed using a vector of intercept and \n\nSpatial random effects $\\theta_i$ that smooth data according to a neighborhood structure are included to acknowledge that data may be spatially correlated, and relative risks in neighboring areas may be more similar than relative risks in areas that are further away Unstructured exchangeable components $\\epsilon_i$ are also included to model uncorrelated noise.\n\nThe relative risk $\\lambda_i$ quantifies whether an area \n$i$ has higher ($\\lambda_i > 1$) or lower ($\\lambda_i < 1$) risk than the average risk in the standard population (e.g., the whole population of the study region). For example, $\\lambda_i = 2$ indicates the risk of area $i$ is two times the average risk in the standard population.\n\n## Spatial Correlation: Areal Data\n\n- How to induce spatial correlation between areal units? \n\t\n  - Distances between centroids (possibly population weighted); may be inappropriate for oddly shaped regions of varying sizes (great for equal sized grid though).\n\n  - Neighborhood structure of your spatial region; are two regions neighbors?\n  \n- Correlation introduced through spatial random effects.\n\n## Bayesian Model Fitting\n\n- CAR models are preferred in the Bayesian setting due to the conditional definition of the model\n\n- $\\theta_i$ parameters are typically updated individually.\n\n- Depending on the likelihood, conjugacy may be available, useful for Gibbs sampling.\n\n- Today, CAR models can be implemented using probabilistic programming languages (e.g., Stan).\n\n## Inducing Spatial Dependency\n\nThe most common approach for inducing spatial dependency in an areal data setting using the intrinsic conditional autoregressive (ICAR) process,\n\n$$\\boldsymbol{\\theta} | \\tau^2 \\sim \\text{ICAR}\\left(\\tau^2\\right)$$\n\n$$\\theta_{i} | \\boldsymbol{\\theta}_{-i}, \\tau^2 \\sim \\mathcal N \\left({\\frac{\\sum_{j=1}^n w_{ij}\\theta_{j}}{\\sum_{j=1}^n w_{ij}}},\\frac{\\tau^2}{\\sum_{j=1}^n w_{ij}}\\right)$$\n\n- $\\boldsymbol\\theta_{-j}$: Vector of $\\theta_{i}$ parameters with $\\theta_{j}$ removed\n\n- $\\boldsymbol{\\beta}_0 | \\tau_0^2 \\sim \\text{ICAR}\\left(\\tau_0^2\\right),\\quad \\boldsymbol{\\beta}_1 | \\tau_1^2 \\sim \\text{ICAR}\\left(\\tau_1^2\\right)$\n\n- Not a proper prior distribution (implications for MCMC algorithms!)\n\n## A Proper CAR Process\n\n$$\\boldsymbol{\\theta} = \\left\\{\\theta_1, \\ldots, \\theta_n\\right\\}^{\\text{T}};\\  \\boldsymbol{\\theta} | \\rho, \\tau^2 \\sim \\text{Leroux CAR}\\left(\\rho,\\tau^2\\right)$$\n\n$$\\theta_i | \\boldsymbol{\\theta}_{-i}, \\rho, \\tau^2 \\sim \\mathcal N \\left({\\frac{\\rho \\sum_{j=1}^n w_{ij} \\theta_j}{\\rho \\sum_{j=1}^n w_{ij} + 1 - \\rho}}, \\frac{\\tau^2}{\\rho \\sum_{j=1}^n w_{ij} + 1 - \\rho}\\right)$$\n\n- Proper for $\\rho \\in [0,1)$\n\n- $\\rho=1$ gives us the ICAR model\n\n  - $\\rho$ is given a prior distribution and estimated by the data\n  \n  - $\\text{COR}(\\theta_k,\\theta_j|\\boldsymbol{\\theta}_{-kj}) = \\frac{\\rho w_{kj}}{\\sqrt{(\\rho \\sum_{i=1}^n w_{ki} + 1 - \\rho)(\\rho \\sum_{i=1}^n w_{ji} + 1-\\rho)}}$\n\n## Compute Adjacency Matrix\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Get adjacency matrix\nneighbors <- spdep::poly2nb(covid_nc_2020)\nadj_matrix <- spdep::nb2mat(neighbors, style = \"B\", zero.policy = TRUE)\n# style = \"B\" specifies binary encoding (1 if neighbors, 0 if not).\n# zero.policy = TRUE ensures the function works even if some counties do not have neighbors.\n# Convert adjacency matrix to a long format for ggplot\nadj_matrix_long <- melt(adj_matrix)\n\n# Plot the adjacency matrix\n# ggplot(adj_matrix_long, aes(x = Var1, y = Var2, fill = value)) +\n#  geom_tile() +\n#  scale_fill_gradient(low = \"white\", high = \"blue\") +\n#  labs(title = \"Adjacency Matrix Heatmap\", x = \"County\", y = \"County\")\n```\n:::\n\n\n\n## Better Adjacency Plot\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Prepare Adjacnecy for Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the row-column pairs where the adjacency matrix is 1\n# This will return all non-zero indices in the adjacency matrix\nneighbor_pairs <- which(adj_matrix == 1, arr.ind = TRUE)\n\n# Filter out the upper triangle (to avoid repeating edges)\nneighbor_pairs_lower <- neighbor_pairs[neighbor_pairs[, 1] < neighbor_pairs[, 2], ]\n```\n:::\n\n\n\n## Fit the Stan Model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n###Try to fit ICAR model\nicar <- stan_model(\"/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/data/covid/icar.stan\")\nX <- model.matrix(~ age + poverty, data = covid_nc_2020)[, -1]\nstan_data <- list(\n  n = nrow(covid_nc_2020),\n  p = ncol(X),\n  n_edges = nrow(neighbor_pairs_lower),\n  node1 = neighbor_pairs_lower[, 1],\n  node2 = neighbor_pairs_lower[, 2],\n  Y = covid_nc_2020$DEATHS,\n  E = covid_nc_2020$ESTDEATHS,\n  X = X\n)\nfit_icar <- sampling(icar, stan_data, pars = c(\"z\", \"log_mu\", \"lp__\"), include = FALSE, iter = 10000)\nprint(fit_icar, pars = c(\"alpha\", \"alpha_star\", \"beta\", \"sigma\", \"tau\"))\nrstan::traceplot(fit_icar, pars = c(\"alpha\", \"alpha_star\", \"beta\", \"sigma\", \"tau\"))\nrstan::traceplot(fit_icar, pars = \"theta[1]\")\nsaveRDS(fit_icar, file = \"/Users/sib2/Box Sync/Faculty/Education/biostat725-sp25/course-material/r-objects/19-fit-icar.rds\")\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=10000; warmup=5000; thin=1; \npost-warmup draws per chain=5000, total post-warmup draws=20000.\n\n            mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nalpha      -0.99    0.00 0.42 -1.80 -1.27 -0.99 -0.71 -0.16  6969 1.00\nalpha_star  0.09    0.00 0.05 -0.01  0.05  0.09  0.12  0.18  6498 1.00\nbeta[1]     0.98    0.01 1.05 -1.09  0.29  0.99  1.70  3.03  6469 1.00\nbeta[2]     4.68    0.01 1.13  2.43  3.93  4.69  5.44  6.86  6770 1.00\nsigma       0.42    0.00 0.05  0.31  0.39  0.43  0.46  0.52  1337 1.00\ntau         0.24    0.01 0.17  0.01  0.11  0.21  0.35  0.62   643 1.01\n\nSamples were drawn using NUTS(diag_e) at Mon Mar 17 11:37:56 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Compute PPD\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY_pred <- rstan::extract(fit_icar, pars = \"Y_pred\")$Y_pred\nppd_oe <- sweep(Y_pred, 2, stan_data$E, \"/\")\nppd_mean <- apply(ppd_oe, 2, mean)\nppd_sd <- apply(ppd_oe, 2, sd)\ncovid_nc_2020$ppd_mean <- ppd_mean\ncovid_nc_2020$ppd_sd <- ppd_sd\n```\n:::\n\n\n\n## Looking at SMR observed versus PPD\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Final Map\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-11-2.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-11-3.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](19-disease-mapping_files/figure-revealjs/unnamed-chunk-11-4.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## Create a figure with $P(\\lambda_i > 1)$\n\n## Prepare for next class\n\n-   Work on HW 04, which is due before class on Tuesday.\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Disease mapping\n",
    "supporting": [
      "19-disease-mapping_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}