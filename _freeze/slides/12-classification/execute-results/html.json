{
  "hash": "85a27e95463a5bccabebe173477da381",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-18\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   On Thursday, we learned about Bayesian approaches to regularization.\n\n    -   Global-local shrinage priors\n\n-   Today, we will focus on classification: logistic regression.\n\n## Generalized linear models\n\n-   Previously, we have focused on linear regression. Other forms of regression follow naturally from linear regression.\n\n## Models for binary outcomes\n\n-   Suppose we have a binary outcome (e.g., $Y = 1$ if a condition is satisfied and $Y = 0$ if not) and predictors on a variety of scales.\n\n-   If the predictors are discrete and the binary outcomes are independent, we can use the Bernoulli distribution for individual 0-1 data or the binomial distribution for grouped data that are counts of successes in each group.\n\n## Models for binary outcomes\n\n-   Let's suppose we want to model $P(Y = 1)$.\n\n-   One strategy might be to simply fit a linear regression model to the probabilities.\n\n-   For example,\n\n\\begin{align*}\nP(Y_i = 1) &= \\alpha + \\beta_1x_{i1} + \\cdots + \\beta_px_{ip} + \\epsilon_i\\\\\n&= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.\n\\end{align*}\n\n## Primary biliary cirrhosis\n\n-   The Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\n\n-   Researchers are interested in predicting whether a patient died based on the following variables:\n\n    -   ascites: whether the patient had ascites (1 = yes, 0 = no)\n\n    -   bilirubin: serum bilirubin in mg/dL\n\n    -   stage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)\n\n## What can go wrong?\n\n-   Suppose we fit the following model:\n\n\\begin{align*}\nP(Y_i = 1) &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{align*}\n\n. . .\n\n**What can go wrong?**\n\n## What can go wrong?\n\n![](images/11/resid.png){fig-alt=\"resid\" fig-align=\"center\" height=\"5in\"}\n\n## What can go wrong?\n\n-   Additionally, as a probability, $P(Y_i = 1)$ must be in the interval \\[0, 1\\], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!\n\n## From probabilities to log-odds\n\n-   Suppose the **probability** of an event is $\\pi$.\n\n-   Then the **odds** that the event occurs is $\\frac{\\pi}{1 - \\pi}$.\n\n-   Taking the (natural) log of the odds, we have the **logit** of $\\pi$: the **log-odds**:\n\n$$\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).$$\n\n-   Note that although $\\pi$ is constrained to lie between 0 and 1, the logit of $\\pi$ is unconstrained - it can be anything from $-\\infty$ to $\\infty$.\n\n## Logistic regression model\n\n-   Let's create a model for the logit of $\\pi$: $\\text{logit}(\\pi_i)= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.$\n\n-   This is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n$$\\pi_i = \\frac{\\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})}{1 + \\exp(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta})} = \\text{expit}(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}).$$\n\n-   The expression on the right is called a **logistic function** and cannot yield a value that is negative or a value that is \\>1. Fitting a model of this form is known as **logistic regression**.\n\n## Logistic regression\n\n$$\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}$$\n\n-   Negative logits represent probabilities less than one-half, and positive logits represent probabilities above one-half.\n\n## Interpreting parameters in logistic regression\n\n-   Typically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: $\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta x_{i},$ we note that the probability that $Y_i = 1$ when $X_i = 0$ is\n\n$$P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.$$\n\n## Interpreting parameters in logistic regression\n\n-   Suppose that $X$ is a binary (0/1) variable (e.g., $X$ = 1 for males and 0 for non-males).\n\n    -   In this case, we interpret $\\exp(\\beta)$ as the **odds ratio** (OR) of the response for the two possible levels of $X$.\n\n    -   For $X$ on other scales, $\\exp(\\beta)$ is interpreted as the odds ratio of the response comparing two values of $X$ one unit apart.\n\n-   Why?\n\n## Interpreting parameters in logistic regression\n\n-   The log odds of response for $X = 1$ is given by $\\alpha + \\beta$, and the log odds of response for $X = 0$ is $\\alpha$.\n\n-   So the odds ratio of response comparing $X = 1$ to $X = 0$ is given by $\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)$.\n\n-   In a \\emph{multivariable logistic regression} model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them).\n\n## Bayesian logistic regression\n\n-   We start with observations $Y_i \\in \\{0,1\\}$ for $i = 1,\\ldots,n$, where $Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)$, $\\pi_i = P(Y_i = 1)$.\n\n-   The log-odds are modeled as $\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}$.\n\n-   To complete the Bayesian model specification, we must place priors on $\\alpha$ and $\\boldsymbol{\\beta}$.\n\n    -   All priors we have discussed up-to-this point apply!\n\n-   Historically, this was a difficult model to fit, but can be easily implemented in Stan.\n\n## Logistic regression in Stan\n\n\n\n::: {.cell output.var='log_reg'}\n\n```{.stan .cell-code}\n// Saved in logistic_regression.stan\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X * beta);\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    in_sample[i] = bernoulli_logit_rng(alpha + height_c[i] * beta);\n    log_lik[i] = bernoulli_logit_lmpf(Y[i] | alpha + X[i, ] * beta);\n  }\n}\n```\n:::\n\n\n\n## Primary biliary cirrhosis\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pbc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0\n```\n\n\n:::\n:::\n\n\n\n## Prepare data for Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY <- pbc$outcome\nstan_data <- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0\n```\n\n\n:::\n:::\n\n\n\n## Logistic regression in Stan\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\ncompiled_model <- stan_model(file = \"logistic_regression.stan\")\nfit <- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\"), probs = c(0.025, 0.5, 0.975))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n         mean se_mean   sd  2.5%   50% 97.5% n_eff Rhat\nalpha   -3.35    0.04 1.21 -6.13 -3.24 -1.41   791    1\nbeta[1]  2.24    0.04 1.32  0.18  2.05  5.48  1252    1\nbeta[2]  0.38    0.00 0.08  0.24  0.38  0.54  2005    1\nbeta[3]  1.71    0.04 1.25 -0.34  1.59  4.55   783    1\nbeta[4]  2.19    0.04 1.22  0.20  2.07  4.96   806    1\nbeta[5]  2.61    0.04 1.23  0.59  2.48  5.46   798    1\n\nSamples were drawn using NUTS(diag_e) at Mon Dec 30 14:36:11 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Convergence diagnostics\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Convergence diagnostics\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Back to the PBC data\n\n-   Fitting a logistic regression model, we obtain\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|        |variable   |  mean|   sd|  2.5%| 97.5%|\n|:-------|:----------|-----:|----:|-----:|-----:|\n|alpha   |intercept  | -3.35| 0.04| -6.13| -1.41|\n|beta[1] |ascites    |  2.24| 0.04|  0.18|  5.48|\n|beta[2] |bilirubin  |  0.38| 0.00|  0.24|  0.54|\n|beta[3] |stage == 2 |  1.71| 0.04| -0.34|  4.55|\n|beta[4] |stage == 3 |  2.19| 0.04|  0.20|  4.96|\n|beta[5] |stage == 4 |  2.61| 0.04|  0.59|  5.46|\n\n\n:::\n:::\n\n\n\n-   How might we interpret these coefficients as odds ratios?\n\n## Back to the PBC data {.small}\n\n-   Remember, we are interested in the probability that a patient died during follow-up (a \"success\"). We are predicting the log-odds of this event happening.\n\n    -   The posterior mean for ascites was 2.24. Thus, the odds ratio for dying is $\\exp(2.24) \\approx 9.4$. That is, patients with ascites have 9.4 times the odds of dying compared to patients that do not, holding all other variables constant.\n\n    -   The posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is $\\exp(0.38) \\approx 1.46$, holding all other variables constant.\n\n    -   The baseline stage was 1. The posterior mean for stage 3 was 2.19. Thus, patients in stage 3 have approximately 8.93 times the odds of dying compared to patients that do not, holding all other variables constant.\n\n## Predicted probabilities\n\n-   There is a one-to-one relationship between $\\pi$ and $\\text{logit}(\\pi)$. So, if we predict $\\text{logit}(\\pi)$, we can \"back-transform\" to get back to a predicted probability.\n\n-   For instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_i <- matrix(c(0, 5, 1, 0, 0), ncol = 1)\npars <- rstan::extract(fit, pars = c(\"alpha\", \"beta\"))\nlog_odds <- pars$alpha + as.numeric(pars$beta %*% x_i)\npi <- exp(log_odds) / (1 + exp(log_odds))\n```\n:::\n\n\n\n## Predicted probabilities\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n-   Posterior mean of the predicted probabilities is 0.56.\n\n## Posterior predictive checks\n\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\ny_pred <- rstan::extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-14-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Posterior predictive checks\n\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\nppc_bars(stan_data$Y, y_pred[1:100, ])\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Posterior predictive checks\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=384}\n:::\n\n```{.r .cell-code}\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-16-2.png){fig-align='center' width=384}\n:::\n\n```{.r .cell-code}\nq025 <- function(y) quantile(y, 0.025)\nq975 <- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-16-3.png){fig-align='center' width=384}\n:::\n\n```{.r .cell-code}\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-16-4.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Model comparison\n\n-   Comparing our model to a baseline that removed stage.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(loo)\nlog_lik <- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline <- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model <- loo::waic(log_lik)\nwaic_model_baseline <- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic <- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -155.94      9.51         7.69    2.09    311.88\nbaseline   -1.69      3.09 -157.63      9.36         4.86    1.98    315.25\n         se_waic\nfull       19.01\nbaseline   18.73\n```\n\n\n:::\n:::\n\n\n\n## Other models for binary data\n\n-   Other transformations (also called \\emph{link functions}) can be used to ensure the probabilities lie in \\[0, 1\\], including the Probit (popular in Bayesian statistics).\n\n## Steps to selecting a Bayesian GLM\n\n1.  Identify the support of the response distribution.\n\n2.  Select the likelihood by picking a parametric family of distributions with this support.\n\n3.  Choose a link function $g$ that transforms the range of parameters to the whole real line.\n\n4.  Specify a linear model on the transformed parameters.\n\n5.  Select priors for the regression coefficients.\n\n## Example of selecting a Bayesian GLM\n\n1.  Support: $Y_i \\in \\{0, 1, 2, \\ldots\\}$.\n\n2.  Likelihood family: $Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)$.\n\n3.  Link: $g(\\lambda_i) = \\log(\\lambda_i) \\in (âˆ’\\infty, \\infty)$.\n\n4.  Regression model: $\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}$.\n\n5.  Priors: $\\alpha, \\beta_j \\sim N(0, 10^2)$.\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Multiclass classification\n",
    "supporting": [
      "12-classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}