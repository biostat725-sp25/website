{
  "hash": "762c1ee463d41420d91abc00a08494bf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-18\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   Last week, we learned about Bayesian approaches to robust regression and regularization.\n\n    -   Global-local shrinkage priors.\n\n-   This week, we will focus on classification models.\n\n    -   Today: Binary classification (logistic regression).\n    \n    -   Thursday: Multiclass classification (multinomial, ordinal regression).\n\n## Models for binary outcomes\n\n- **Bernoulli** random variable: Used for binary outcomes (success/failure), e.g., whether a patient responds to a treatment (yes/no).\n\n- **Binomial** random variable: Used when there are multiple trials (e.g., 10 patients), and you want to model the number of successes (e.g., how many out of 10 patients experience a treatment response).\n\n## Bernoulli random variable example\n\nA **Bernoulli random variable** represents a random variable with two possible outcomes: 0 or 1. \n\n**Scenario:**\n\nImagine a medical study on a new drug for hypertension (high blood pressure). You want to model whether a patient responds positively to the treatment.\n\n- **Success (1)**: The patientâ€™s blood pressure decreases significantly (e.g., more than 10% reduction).\n\n- **Failure (0)**: The patient does not experience a significant decrease in blood pressure.\n\n## Binomial random variable example\n\nA **Binomial random variable** represents the number of successes in a fixed number of independent Bernoulli trials.\n\n**Scenario:**\n\nA clinical trial is conducted where 10 patients are given a new drug for diabetes. You want to model how many of these 10 patients experience a significant reduction in their blood sugar levels (e.g., a decrease by at least 20%).\n\n- Each patientâ€™s outcome is a Bernoulli random variable: success (1) if their blood sugar level decreases, failure (0) if it does not.\n\n- The total number of successes (patients who experience a reduction) is modeled as a **Binomial random variable**.\n\n## Models for binary outcomes\n\n-   Suppose $Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)$ for $i = 1,\\ldots,n$. The pmf is,\n\n$$f(Y_i) = P(Y_i = y) = \\pi_i^y (1 - \\pi_i)^{1 - y}, \\quad y \\in\\{0,1\\}.$$\n\n-   We only need to specify $\\pi_i = P(Y_i = 1)$.\n\n-   One strategy might be to simply fit a linear regression model,\n\n$$Y_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad\\epsilon_i \\sim N(0, \\sigma^2).$$\n\n  - We can set $P(Y_i = 1) = \\hat{Y}_i$.\n    \n## Primary biliary cirrhosis\n\n-   The Mayo Clinic conducted a trial for primary biliary cirrhosis, comparing the drug D-penicillamine vs. placebo. Patients were followed for a specified duration, and their status at the end of follow-up (whether they died) was recorded.\n\n-   Researchers are interested in predicting whether a patient died based on the following variables:\n\n    -   ascites: whether the patient had ascites (1 = yes, 0 = no)\n\n    -   bilirubin: serum bilirubin in mg/dL\n\n    -   stage: histologic stage of disease (ordinal categorical variable with stages 1, 2, 3, and 4)\n\n## What can go wrong?\n\n-   Suppose we fit the following model:\n\n\\begin{align*}\nY_i &= \\alpha + \\beta_1(ascites)_i + \\beta_2(bilirubin)_i\\\\\n&\\quad+\\beta_3(stage = 2)_i + \\beta_4(stage = 3)_i\\\\\n&\\quad+\\beta_5(stage = 4)_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\n\\end{align*}\n\n. . .\n\n**What can go wrong?**\n\n## What can go wrong?\n\n![](images/11/resid.png){fig-alt=\"resid\" fig-align=\"center\" height=\"5in\"}\n\n## What can go wrong?\n\n-   Additionally, as a probability, $P(Y_i = 1)$ must be in the interval \\[0, 1\\], but there is nothing in the model that enforces this constraint, so that you could be estimating probabilities that are negative or that are greater than 1!\n\n## From probabilities to log-odds\n\n-   Suppose the **probability** of an event is $\\pi$.\n\n-   Then the **odds** that the event occurs is $\\frac{\\pi}{1 - \\pi}$.\n\n-   Taking the (natural) log of the odds, we have the **logit** of $\\pi$: the **log-odds**:\n\n$$\\text{logit}(\\pi) = \\log\\left(\\frac{\\pi}{1-\\pi}\\right).$$\n\n-   Note that although $\\pi$ is constrained to lie between 0 and 1, the logit of $\\pi$ is unconstrained - it can be anything from $-\\infty$ to $\\infty$.\n\n## Logistic regression model\n\n-   Let's create a model for the logit of $\\pi$: $\\text{logit}(\\pi_i)= \\eta_i$, where $\\eta_i = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}.$\n\n-   This is a linear model for a transformation of the outcome of interest, and is also equivalent to,\n\n$$\\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\text{expit}(\\eta_i).$$\n\n-   The expression on the right is called a **logistic function** and cannot yield a value that is negative or a value that is \\>1. Fitting a model of this form is known as **logistic regression**.\n\n## Logistic regression\n\n$$\\text{logit}(\\pi_i) = \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\eta_i$$\n\n-   Negative logits represent probabilities less than one-half.\n\n    -   $\\eta_i < 0 \\implies \\pi_i < 0.5$.\n\n-   Positive logits represent probabilities greater than one-half. \n\n    -   $\\eta_i > 0 \\implies \\pi_i > 0.5$.\n\n## Interpreting parameters in logistic regression\n\nTypically we interpret functions of parameters in logistic regression rather than the parameters themselves.\n\nFor the simple model: $\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\alpha + \\beta X_{i},$ we note that the probability that $Y_i = 1$ when $X_i = 0$ is\n\n$$P(Y_i = 1 | X_{i} = 0) = \\frac{\\exp(\\alpha)}{1 + \\exp(\\alpha)}.$$\n\n## Interpreting parameters in logistic regression\n\n-   Suppose that $X$ is a binary (0/1) variable (e.g., $X = 1$ for males and 0 for non-males).\n\n    -   In this case, we interpret $\\exp(\\beta)$ as the **odds ratio** (OR) of the response for the two possible levels of $X$.\n\n    -   For $X$ on other scales, $\\exp(\\beta)$ is interpreted as the odds ratio of the response comparing two values of $X$ one unit apart.\n\n-   Why?\n\n## Interpreting parameters in logistic regression\n\n-   The log odds of response for $X = 1$ is given by $\\alpha + \\beta$, and the log odds of response for $X = 0$ is $\\alpha$.\n\n-   So the odds ratio of response comparing $X = 1$ to $X = 0$ is given by $\\frac{\\exp(\\alpha + \\beta)}{\\exp(\\alpha)} = \\exp(\\beta)$.\n\n-   In a \\emph{multivariable logistic regression} model with more than one predictor, this OR is interpreted conditionally on values of other variables (i.e., controlling for them).\n\n## Bayesian logistic regression\n\n-   We start with observations $Y_i \\in \\{0,1\\}$ for $i = 1,\\ldots,n$, where $Y_i \\stackrel{ind}{\\sim} \\text{Bernoulli}(\\pi_i)$, $\\pi_i = P(Y_i = 1)$.\n\n-   The log-odds are modeled as $\\text{logit}(\\pi_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} = \\eta_i$.\n\n-   To complete the Bayesian model specification, we must place priors on $\\alpha$ and $\\boldsymbol{\\beta}$.\n\n    -   All priors we have discussed up-to-this point apply!\n\n-   Historically, this was a difficult model to fit, but can be easily implemented in Stan.\n\n## Logistic regression in Stan\n\n\n\n::: {.cell output.var='log_reg'}\n\n```{.stan .cell-code}\n// Saved in logistic_regression.stan\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  int Y[n];                              // Y is now type int\n  matrix[n, p] X;\n}\ntransformed data {\n  matrix[n, p] X_centered;               // We are only centering X!\n  row_vector[p] X_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n}\nmodel {\n  target += bernoulli_logit_lpmf(Y | alpha + X_centered * beta); // bernoulli likelihood parameterized in logits\n  target += normal_lpdf(alpha | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n}\ngenerated quantities {\n  real pi_average = exp(alpha) / (1 + exp(alpha));\n  vector[n] Y_pred;\n  vector[n] log_lik;\n  for (i in 1:n) {\n    Y_pred[i] = bernoulli_logit_rng(alpha + X_centered[i, ] * beta);\n    log_lik[i] = bernoulli_logit_lpmf(Y[i] | alpha + X_centered[i, ] * beta);\n  }\n}\n```\n:::\n\n\n\n[bernoulli_logit_lpmf](https://mc-stan.org/docs/functions-reference/binary_distributions.html#bernoulli-logit-distribution)\n\n## Primary biliary cirrhosis\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(pbc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id trt      age sex ascites hepato spiders edema bili chol albumin copper\n1  1   1 58.76523   f       1      1       1   1.0 14.5  261    2.60    156\n2  2   1 56.44627   f       0      1       1   0.0  1.1  302    4.14     54\n3  3   1 70.07255   m       0      0       0   0.5  1.4  176    3.48    210\n4  4   1 54.74059   f       0      1       1   0.5  1.8  244    2.54     64\n5  5   2 38.10541   f       0      1       1   0.0  3.4  279    3.53    143\n6  7   2 55.53457   f       0      1       0   0.0  1.0  322    4.09     52\n  alk.phos    ast trig platelet protime stage outcome\n1   1718.0 137.95  172      190    12.2     4       1\n2   7394.8 113.52   88      221    10.6     3       0\n3    516.0  96.10   55      151    12.0     4       1\n4   6121.8  60.63   92      183    10.3     4       1\n5    671.0 113.15   72      136    10.9     3       1\n6    824.0  60.45  213      204     9.7     3       0\n```\n\n\n:::\n:::\n\n\n\n## Prepare data for Stan\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix(object = ~ ascites + bili + as.factor(stage), data = pbc)[, -1]\nY <- pbc$outcome\nstan_data <- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X)\nhead(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  ascites bili as.factor(stage)2 as.factor(stage)3 as.factor(stage)4\n1       1 14.5                 0                 0                 1\n2       0  1.1                 0                 1                 0\n3       0  1.4                 0                 0                 1\n4       0  1.8                 0                 0                 1\n5       0  3.4                 0                 1                 0\n6       0  1.0                 0                 1                 0\n```\n\n\n:::\n:::\n\n\n\n## Logistic regression in Stan\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\ncompiled_model <- stan_model(file = \"logistic_regression.stan\")\nfit <- sampling(compiled_model, data = stan_data)\nprint(fit, pars = c(\"alpha\", \"beta\", \"pi_average\"), probs = c(0.025, 0.5, 0.975))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n           mean se_mean   sd  2.5%  50% 97.5% n_eff Rhat\nalpha      0.21    0.00 0.19 -0.14 0.20  0.60  1819 1.00\nbeta[1]    2.24    0.03 1.32  0.12 2.12  5.30  2328 1.00\nbeta[2]    0.38    0.00 0.08  0.24 0.38  0.55  2027 1.00\nbeta[3]    1.79    0.04 1.31 -0.43 1.65  4.86  1097 1.01\nbeta[4]    2.26    0.04 1.30  0.10 2.14  5.27  1073 1.01\nbeta[5]    2.69    0.04 1.31  0.52 2.54  5.74  1069 1.01\npi_average 0.55    0.00 0.05  0.47 0.55  0.65  1826 1.00\n\nSamples were drawn using NUTS(diag_e) at Mon Feb 17 09:17:32 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Convergence diagnostics\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-9-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Convergence diagnostics\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Back to the PBC data\n\n-   Fitting a logistic regression model, we obtain\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|        |variable   | mean|   sd|  2.5%| 97.5%|\n|:-------|:----------|----:|----:|-----:|-----:|\n|alpha   |intercept  | 0.21| 0.00| -0.14|  0.60|\n|beta[1] |ascites    | 2.24| 0.03|  0.12|  5.30|\n|beta[2] |bilirubin  | 0.38| 0.00|  0.24|  0.55|\n|beta[3] |stage == 2 | 1.79| 0.04| -0.43|  4.86|\n|beta[4] |stage == 3 | 2.26| 0.04|  0.10|  5.27|\n|beta[5] |stage == 4 | 2.69| 0.04|  0.52|  5.74|\n\n\n:::\n:::\n\n\n\n-   How might we interpret these coefficients as odds ratios?\n\n## Back to the PBC data {.small}\n\n-   Remember, we are interested in the probability that a patient died during follow-up (a \"success\"). We are predicting the log-odds of this event happening.\n\n    -   The posterior mean for ascites was 2.24. Thus, the odds ratio for dying is $\\exp(2.24) \\approx 9.40$. That is, patients with ascites have 9 times the odds of dying compared to patients that do not, holding all other variables constant.\n\n    -   The posterior mean for bilirubin was 0.38. Thus, the odds ratio for dying for a patient with 1 additional mg/dL serum bilirubin compared to another is $\\exp(0.38) \\approx 1.46$, holding all other variables constant.\n\n    -   The baseline stage was 1. The posterior mean for stage 3 was 2.26. Thus, patients in stage 3 have approximately 9.58 times the odds of dying compared to patients that do not, holding all other variables constant.\n\n## Predicted probabilities\n\n-   There is a one-to-one relationship between $\\pi$ and $\\text{logit}(\\pi)$. So, if we predict $\\text{logit}(\\pi)$, we can \"back-transform\" to get back to a predicted probability.\n\n\n\n::: {.cell output.var='pred'}\n\n```{.stan .cell-code}\n// stored in logistic_regression_new.stan\ndata {\n  row_vector[p] X_new;\n}\ngenerated quantities {\n  real eta_new = (alpha + (X_new - X_bar) * beta);\n  real pi_new = inv_logit(eta_new); // expit function\n  real Y_new = bernoulli_logit_rng(eta_new); // posterior predictive distribution\n}\n```\n:::\n\n\n\n## Predicted probabilities\n\n-   For instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompiled_model <- stan_model(file = \"logistic_regression_new.stan\")\nstan_data <- list(n = nrow(pbc),\n                  p = ncol(X),\n                  Y = Y,\n                  X = X,\n                  X_new = c(0, 5, 1, 0, 0))\nfit <- sampling(compiled_model, data = stan_data)\n```\n:::\n\n\n\n## Predicted probabilities\n\n-   For instance, suppose a patient does not have ascites, has a bilirubin level of 5 mg/dL, and is a stage 2 patient.\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n        mean se_mean   sd  2.5%  50% 97.5% n_eff Rhat\neta_new 0.28    0.01 0.40 -0.48 0.27  1.07  2754    1\npi_new  0.57    0.00 0.09  0.38 0.57  0.75  2780    1\nY_new   0.57    0.01 0.49  0.00 1.00  1.00  4013    1\n\nSamples were drawn using NUTS(diag_e) at Mon Feb 17 09:17:32 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Predicted probabilities\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-15-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n-   Posterior mean of the predicted probabilities is 0.57.\n\n## Posterior predictive checks\n\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\ny_pred <- rstan::extract(fit, pars = \"Y_pred\")$Y_pred\nppc_dens_overlay(stan_data$Y, y_pred[1:100, ])\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-16-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Posterior predictive checks\n\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\nppc_bars(stan_data$Y, y_pred[1:100, ])\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-17-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Posterior predictive checks\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nppc_stat(stan_data$Y, y_pred, stat = \"mean\") # from bayesplot\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-18-1.png){fig-align='center' width=384}\n:::\n\n```{.r .cell-code}\nppc_stat(stan_data$Y, y_pred, stat = \"sd\")\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-18-2.png){fig-align='center' width=384}\n:::\n\n```{.r .cell-code}\nq025 <- function(y) quantile(y, 0.025)\nq975 <- function(y) quantile(y, 0.975)\nppc_stat(stan_data$Y, y_pred, stat = \"q025\")\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-18-3.png){fig-align='center' width=384}\n:::\n\n```{.r .cell-code}\nppc_stat(stan_data$Y, y_pred, stat = \"q975\")\n```\n\n::: {.cell-output-display}\n![](12-classification_files/figure-revealjs/unnamed-chunk-18-4.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Model comparison\n\n-   Comparing our model to a baseline that removed stage.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(loo)\nlog_lik <- loo::extract_log_lik(fit, parameter_name = \"log_lik\", merge_chains = TRUE)\nlog_lik_baseline <- loo::extract_log_lik(fit_baseline, parameter_name = \"log_lik\", merge_chains = TRUE)\nwaic_model <- loo::waic(log_lik)\nwaic_model_baseline <- loo::waic(log_lik_baseline)\n\n###Make a comparison\ncomp_waic <- loo::loo_compare(list(\"full\" = waic_model, \"baseline\" = waic_model_baseline))\nprint(comp_waic, digits = 2, simplify = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         elpd_diff se_diff elpd_waic se_elpd_waic p_waic  se_p_waic waic   \nfull        0.00      0.00 -156.08      9.60         7.83    2.20    312.17\nbaseline   -1.38      3.15 -157.46      9.29         4.68    1.82    314.92\n         se_waic\nfull       19.21\nbaseline   18.58\n```\n\n\n:::\n:::\n\n\n\n## Other models for binary data {.midi}\n\nAn alternative approach is **Probit regression**, where we use the CDF of the standard normal distribution instead of the logit link: $\\Phi^{-1}(\\pi) = \\alpha + \\beta X$\n\nWhere $\\Phi^{-1}$ is the inverse normal CDF (also called the **probit link function**).\n\n\n\n::: {.cell output.var='probit'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;               // number of observations\n  int<lower = 1> p;               // number of predictors\n  int<lower = 0, upper = 1> Y[n]; // binary outcome (0 or 1)\n  matrix[n, p] X;                 // design matrix (predictors)\n}\nparameters {\n  real alpha;               // intercept\n  vector[p] beta;           // coefficients\n}\n\nmodel {\n  target += bernoulli_lpmf(Phi(alpha + X * beta)); // Probit model\n}\n```\n:::\n\n\n\n\n## Steps to selecting a Bayesian GLM\n\n1.  Identify the support of the response distribution.\n\n2.  Select the likelihood by picking a parametric family of distributions with this support.\n\n3.  Choose a link function $g$ that transforms the range of parameters to the whole real line.\n\n4.  Specify a linear model on the transformed parameters.\n\n5.  Select priors for the regression coefficients.\n\n## Example of selecting a Bayesian GLM\n\n1.  Support: $Y_i \\in \\{0, 1, 2, \\ldots\\}$.\n\n2.  Likelihood family: $Y_i \\stackrel{ind}{\\sim} \\text{Poisson}(\\lambda_i)$.\n\n3.  Link: $g(\\lambda_i) = \\log(\\lambda_i) \\in (âˆ’\\infty, \\infty)$.\n\n4.  Regression model: $\\log(\\lambda_i) = \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}$.\n\n5.  Priors: $\\alpha, \\beta_j \\sim N(0, 10^2)$.\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Multiclass classification\n",
    "supporting": [
      "12-classification_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}