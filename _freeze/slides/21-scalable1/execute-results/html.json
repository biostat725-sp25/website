{
  "hash": "0ef6fc19178223fb8b0f40dbd7ed259a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Scalable Gaussian Processes #1\"\nauthor: \"Christine Shen\"\ndate: \"2025-04-01\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\n# bibliography: ../../doc/reference.bib\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Review of the last lecture\n\nTBU\n\n## Motivating dataset {.midi}\n\nRecall we worked with a dataset on women aged 15-49 sampled from the 2013-14 Democratic Republic of Congo (DRC) Demographic and Health Survey. Variables are:\n\n  - `loc_id`: location id (i.e. survey cluster).\n\n  - `hemoglobin`: hemoglobin level (g/dL).\n\n  - `anemia`: anemia classifications.\n  \n  - `age`: age in years.\n\n  - `urban`: urban vs. rural.\n\n  - `LATNUM`: latitude.\n\n  - `LONGNUM`: longitude.\n\n## Motivating dataset {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n  loc_id hemoglobin     anemia age urban   LATNUM  LONGNUM\n1      1       12.5 not anemic  28 rural 0.220128 21.79508\n2      1       12.6 not anemic  42 rural 0.220128 21.79508\n3      1       13.3 not anemic  15 rural 0.220128 21.79508\n4      1       12.9 not anemic  28 rural 0.220128 21.79508\n5      1       10.4       mild  32 rural 0.220128 21.79508\n6      1       12.2 not anemic  42 rural 0.220128 21.79508\n```\n\n\n:::\n:::\n\n\n\n::: callout-important\n## Modeling goals:\n\n  - Learn the associations between age and urbanality and hemoglobin, accounting for unmeasured spatial confounders.\n  \n  - Create a predicted map of hemoglobin across the spatial surface controlling for age and urbanality, with uncertainty quantification.\n  \n:::\n\n## Map of Sud-Kivu state {.midi}\n\nLast time, we focused on one state with ~500 observations at ~30 locations.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable1_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Map of the DRC{.midi}\n\nToday we will extend the analysis to the full dataset with ~8,600 observations at ~500 locations. \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](21-scalable1_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Modeling{.midi}\n\n\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Data objects:**\n\n- $i \\in \\{1,\\dots,n\\}$ indexes unique locations.\n\n- $j \\in \\{1,\\dots,n_i\\}$ indexes individuals at each location.\n\n- $Y_j(\\mathbf{u}_i)$ denotes the observation of individual $j$ at location $\\mathbf{u}_i$.\n\n- $\\mathbf{x}_j(\\mathbf{u}_i) = (\\text{age}_{ij}/10,\\text{urban}_i) \\in \\mathbb{R}^{1 \\times p}$, where $p=2$ is the number of predictors (excluding intercept).\n\n## Modeling{.midi}\n\n\\begin{align*}\n  Y_j(\\mathbf{u}_i) &= \\alpha + \\mathbf{x}_j(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i) + \\epsilon_j(\\mathbf{u}_i), \\quad \\epsilon_j(\\mathbf{u}_i) \\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\n**Population parameters:**\n\n- $\\alpha \\in \\mathbb{R}$ is the intercept.\n\n- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ is the regression coefficients.\n\n- $\\sigma^2 \\in \\mathbb{R}^+$ is the overall residual error (nugget).\n\n**Location-specific parameters:**\n\n- $\\mathbf{u}_i = (\\text{latitude}_i, \\text{longitude}_i) \\in \\mathbb{R}^2$ denotes coordinates of location $i$.\n\n- $\\theta(\\mathbf{u}_i)$ denotes the spatial intercept at location $\\mathbf{u}_i$.\n\n\n## Location-specific notation{.midi}\n\n$$\\mathbf{Y}(\\mathbf{u}_i) = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}(\\mathbf{u}_i) \\boldsymbol{\\beta} + \\theta(\\mathbf{u}_i)\\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}(\\mathbf{u}_i), \\quad \\boldsymbol{\\epsilon}(\\mathbf{u}_i) \\sim N_{n_i}(\\mathbf{0},\\sigma^2\\mathbf{I})$$\n\n- $\\mathbf{Y}(\\mathbf{u}_i) = (Y_1(\\mathbf{u}_i),\\ldots,Y_{n_i}(\\mathbf{u}_i))^\\top$.\n\n- $\\mathbf{X}(\\mathbf{u}_i)$ is an $n_i \\times p$ dimensional matrix with rows $\\mathbf{x}_j(\\mathbf{u}_i)$.\n\n- $\\boldsymbol{\\epsilon}(\\mathbf{u}_i) = (\\epsilon_i(\\mathbf{u}_i),\\ldots,\\epsilon_{n_i}(\\mathbf{u}_i))^\\top$.\n\n## Full data notation{.midi}\n\n$$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})$$\n\n- $\\mathbf{Y} = (\\mathbf{Y}(\\mathbf{u}_1)^\\top,\\ldots,\\mathbf{Y}(\\mathbf{u}_{n})^\\top)^\\top \\in \\mathbb{R}^N$, with $N = \\sum_{i=1}^n n_i$.\n\n- $\\mathbf{X} \\in \\mathbb{R}^{N \\times p}$ stacks $\\mathbf{X}(\\mathbf{u}_i)$.\n\n- $\\boldsymbol{\\theta} = (\\theta(\\mathbf{u}_1),\\ldots,\\theta(\\mathbf{u}_n))^\\top \\in \\mathbb{R}^n$.\n\n- $\\mathbf{Z}$ is an $N \\times n$ dimensional block diagonal binary matrix. Each row contains a single 1 in column $i$ that corresponds to the location of $Y_j(\\mathbf{u}_i)$.\n$$\n\\begin{align}\n  \\mathbf{Z} = \\begin{bmatrix}\n  \\mathbf{1}_{n_1} & \\mathbf{0} & \\dots & \\mathbf{0} \\\\\n  \\mathbf{0} & \\mathbf{1}_{n_2} & \\dots & \\mathbf{0}  \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  \\mathbf{0} & \\dots & \\mathbf{0} & \\mathbf{1}_{n_n}\n  \\end{bmatrix}.\n\\end{align}\n$$\n\n## Modeling{.midi}\n\nWe specify the following model: \n$$\\mathbf{Y} = \\alpha \\mathbf{1}_{N} + \\mathbf{X} \\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\quad \\boldsymbol{\\epsilon} \\sim N_N(\\mathbf{0},\\sigma^2\\mathbf{I})$$\nwith priors\n\n - $\\boldsymbol{\\theta}(\\mathbf{u}) | \\tau,\\rho \\sim GP(\\mathbf{0},C(\\cdot,\\cdot))$, where $C$ is the MatÃ©rn 3/2 covariance function with magnitude $\\tau$ and length scale $\\rho$\n - $\\alpha^* \\sim N(0,4^2)$. This is the intercept after centering $\\mathbf{X}$.\n - $\\beta_j | \\sigma_{\\beta} \\sim N(0,\\sigma_{\\beta}^2)$, $j \\in \\{1,\\dots,p\\}$\n - $\\sigma \\sim \\text{Half-Normal}(0, 2^2)$\n - $\\tau \\sim \\text{Half-Normal}(0, 4^2)$\n - $\\rho \\sim \\text{Inv-Gamma}(5, 5)$\n - $\\sigma_{\\beta} \\sim \\text{Half-Normal}(0, 2^2)$\n\n## Computational issues with GP{.midi}\n\nEffectively, the prior for $\\boldsymbol{\\theta}$ is \n$$\\boldsymbol{\\theta} | \\tau,\\rho \\sim N_n(\\mathbf{0},\\mathbf{C}), \\quad \\mathbf{C} \\in \\mathbb{R}^{n \\times n}.$$\nMatÃ©rn 3/2 is an isotropic covariance function, $C(\\mathbf{u}_i, \\mathbf{u}_j) = C(\\|\\mathbf{u}_i-\\mathbf{u}_j\\|)$.\n\n$$\\mathbf{C} = \\begin{bmatrix}\nC(0) & C(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & \\cdots & C(\\|\\mathbf{u}_1 - \\mathbf{u}_n\\|)\\\\\nC(\\|\\mathbf{u}_1 - \\mathbf{u}_2\\|) & C(0) & \\cdots & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\nC(\\|\\mathbf{u}_{1} - \\mathbf{u}_n\\|) & C(\\|\\mathbf{u}_2 - \\mathbf{u}_n\\|) & \\cdots & C(0)\\\\\n\\end{bmatrix}.$$\n\nThis is not scalable because we need to invert an $n \\times n$ dense covariance matrix for each MCMC iteration, which requires $\\mathcal{O}(n^3)$ floating point operations (flops), and $\\mathcal{O}(n^2)$ memory.\n\n## Scalable GP methods overview{.midi}\n\nThe computational issues motivated exploration in scalable GP methods. Existing scalable methods broadly fall under two categories.\n\n1. Sparsity methods\n\n  - sparsity in $\\mathbf{C}$, e.g., covariance tapering (@furrer2006covariance).\n  - sparsity in $\\mathbf{C}^{-1}$, e.g., Vecchia approximation (@vecchia1988estimation) and nearest-neighbor GP (@datta2016hierarchical).\n  \n2. Low-rank methods\n\n  - approximate $\\mathbf{C}$ on a low-dimensional subspace.\n  - e.g., process convolution (@higdon2002space), inducing point method(@snelson2005sparse).\n\n\n## Hilbert space method for GP{.midi}\n\n - @solin2020hilbert introduced a Hilbert space method for reduced-rank Gaussian process regression (HSGP).\n\n - @riutort2023practical discussed how to practically implement HSGP.\n\n - Tutorial codes are available in different probabilistic programming languages:\n\n    - [stan](https://github.com/gabriuma/basis_functions_approach_to_GP/tree/master/Paper)\n    - [NumPyro](https://num.pyro.ai/en/0.15.2/examples/hsgp.html)\n    - [pyMC](https://juanitorduz.github.io/hsgp_intro/)\n\n## Lecture plan{.midi}\n\nToday:\n\n - How does HSGP work\n - Why HSGP is scalable\n - How to use HSGP for Bayesian geospatial model fitting and posterior predictive sampling\n \nThursday:\n\n - Parameter tuning for HSGP\n - How to implement HSGP in `stan`\n\n\n## HSGP approximation{.midi}\n\nGiven:\n\n - an isotropic covariance function $C$ which admits a *power spectral density*, e.g., the MatÃ©rn family, and \n - a compact domain $\\boldsymbol{\\Theta} \\in \\mathbb{R}^d$ with *smooth* boundaries. For our purposes, we only consider *boxes*, e.g., $[-1,1] \\times [-1,1]$.\n\nHSGP approximates the $(i,j)$ element of the corresponding $n \\times n$ covariance matrix $\\mathbf{C}$ as\n$$\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).$$\n\n## HSGP approximation{.midi}\n\n$$\\mathbf{C}_{ij}=C(\\|\\mathbf{u}_i - \\mathbf{u}_j\\|) \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j).$$\n\n - $s_k \\in \\mathbb{R}^+$ are positive scalars which depends on the covariance function $C$ and its parameters $\\tau$ and $\\rho$.\n - $\\phi_k: \\boldsymbol{\\Theta} \\to \\mathbb{R}$ are *basis functions* which only depends on $\\boldsymbol{\\Theta}$.\n - $m$ is the number of basis functions. Note: even with an infinite sum ($m \\to \\infty$), this remains an approximation (see @solin2020hilbert).\n\n\n## HSGP approximation{.midi}\n\nIn matrix notation,\n\n$$\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.$$\n \n - $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{n \\times m}$ is a *feature matrix*. Only depends on $\\boldsymbol{\\Theta}$ and the observed locations.\n - $\\mathbf{S} \\in \\mathbb{R}^{m \\times m}$ is diagonal. Depends on the covariance function $C$ and parameters $\\tau$ and $\\rho$.\n \n$$\n\\begin{align}\n  \\boldsymbol{\\Phi} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_1) & \\dots & \\phi_m(\\mathbf{u}_1) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_n) & \\dots & \\phi_m(\\mathbf{u}_n)\n  \\end{bmatrix}, \\quad \n  \\mathbf{S} = \\begin{bmatrix}\n  s_1 &  &  \\\\\n  & \\ddots &  \\\\\n  &  & s_m\n  \\end{bmatrix}.\n\\end{align}\n$$\n\n## Why HSGP is scalable{.midi}\n\n$$\\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top.$$\n \n - $\\boldsymbol{\\Phi}$ only depends on $\\boldsymbol{\\Theta}$ and the observed locations, can be pre-calculated.\n - No matrix inversion.\n - Each MCMC iteration requires $\\mathcal{O}(nm + m)$ flops, vs $\\mathcal{O}(n^3)$ for a full GP.\n - Ideally $m \\ll n$, but HSGP can be faster even for $m>n$.\n\n\n## Model reparameterization{.midi}\n\nUnder HSGP, approximately\n$$\\boldsymbol{\\theta} \\overset{d}{=} \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}, \\quad \\mathbf{b} \\sim N_m(0,\\mathbf{I}).$$\n\nTherefore we can reparameterize the model as \n\n$$\n\\begin{align}\n  \\mathbf{Y} &= \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{\\theta} + \\boldsymbol{\\epsilon} \\\\\n  &\\approx \\alpha \\mathbf{1}_{N} + X\\boldsymbol{\\beta} + \\underbrace{\\mathbf{Z}\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}}_{\\mathbf{W}}\\mathbf{b} + \\boldsymbol{\\epsilon}\n\\end{align}\n$$\n\nNote the resemblance to linear regression:\n\n- $\\mathbf{W} \\in \\mathbb{R}^{n \\times m}$ is a known design matrix given parameters $\\tau$ and $\\rho$.\n- $\\mathbf{b}$ is an unknown parameter vector with prior $N_m(0,\\mathbf{I})$.\n\n## HSGP in `stan` {.midi}\n\nSimilarly, we can use the reparameterized model in `stan`. \n\nThis is called the [*non-centered parameterization*](https://mc-stan.org/docs/stan-users-guide/efficiency-tuning.html#hierarchical-models-and-the-non-centered-parameterization) in `stan` documentation. It's recommended for computational efficiency for hierarchical models.\n\n\n\n::: {.cell output.var='model_in_stan'}\n\n```{.stan .cell-code}\ntransformed data {\n  matrix[n,m] PHI;\n  matrix[N,m] Z;\n}\nparameters {\n  real alpha;\n  real<lower=0> sigma;\n  vector[p] beta;\n  vector[m] b;\n  vector<lower=0>[m] sqrt_S;\n}\nmodel {\n  vector[n] theta = PHI * (sqrt_S .* b);\n  target += normal_lupdf(y | alpha + X * beta + Z * theta, sigma);\n  target += normal_lupdf(b | 0, 1);\n  ...\n}\n```\n:::\n\n\n\n## Posterior predictive distribution {.midi}\n\nWe want to make predictions for $\\mathbf{Y}^* = (Y(\\mathbf{u}_{n+1}),\\ldots, Y(\\mathbf{u}_{n+q}))^\\top$, observations at $q$ new locations. Define $\\boldsymbol{\\theta}^* = (\\theta(\\mathbf{u}_{n+1}),\\ldots,\\theta(\\mathbf{u}_{n+q}))^\\top$, $\\boldsymbol{\\Omega} = (\\alpha,\\boldsymbol{\\beta},\\sigma,\\tau,\\rho)$. Recall:\n\n\\begin{align*}\n  f(\\mathbf{Y}^* | \\mathbf{Y}) &= \\int f(\\mathbf{Y}^*, \\boldsymbol{\\theta}^*, \\boldsymbol{\\theta}, \\boldsymbol{\\Omega} | \\mathbf{Y}) d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n  &= \\int \\underbrace{f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})}_{(1)} \\underbrace{f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})}_{(2)} \\underbrace{f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})}_{(3)} d\\boldsymbol{\\theta}^* d\\boldsymbol{\\theta} d\\boldsymbol{\\Omega}\\\\\n\\end{align*}\n\n(1) Likelihood: $f(\\mathbf{Y}^* | \\boldsymbol{\\theta}^*, \\boldsymbol{\\Omega})$ <span class=\"fragment\" data-fragment-index=\"1\" style=\"color: #a50f15;\"> -- remains the same as for GP </span>\n\n(2) Kriging: $f(\\boldsymbol{\\theta}^* | \\boldsymbol{\\theta}, \\boldsymbol{\\Omega})$ <span class=\"fragment\" data-fragment-index=\"2\" style=\"color: #a50f15;\"> -- we will focus on this next </span>\n\n(3) Posterior distribution: $f(\\boldsymbol{\\theta},\\boldsymbol{\\Omega} | \\mathbf{Y})$ <span class=\"fragment\" data-fragment-index=\"0\" style=\"color: #a50f15;\"> -- we have just discussed </span>\n\n## Kriging {.midi}\n\nRecall under the GP prior,\n\n$$\\begin{bmatrix}\n    \\boldsymbol{\\theta}\\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega} \\sim N_{n+q}\\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\mathbf{C} & \\mathbf{C}_{+}\\\\\n    \\mathbf{C}_{+}^\\top & \\mathbf{C}^*\n  \\end{bmatrix}\\right),$$\n\nwhere $\\mathbf{C}$ is the covariance of $\\boldsymbol{\\theta}$, $\\mathbf{C}^*$ is the covariance of $\\boldsymbol{\\theta}^*$, and $\\mathbf{C}_{+}$ is the cross covariance matrix between $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}^*$.\n\nTherefore by properties of multivariate normal,\n$$\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*},\\mathbb{V}_{\\boldsymbol{\\theta}^*}), \\quad \\text{where}$$\n$$\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*} &= \\mathbf{C}^* - \\mathbf{C}_+^\\top \\mathbf{C}^{-1} \\mathbf{C}_+.\n\\end{align}\n$$\n\n## Kriging under HSGP {.midi}\n\nUnder HSGP, $\\mathbf{C}^* \\approx \\boldsymbol{\\Phi}^* \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}$, $\\mathbf{C}_+ \\approx \\boldsymbol{\\Phi} \\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}$, where\n$$\n\\begin{align}\n  \\boldsymbol{\\Phi}^* \\in \\mathbb{R}^{q \\times m} = \\begin{bmatrix}\n  \\phi_1(\\mathbf{u}_{n+1}) & \\dots & \\phi_m(\\mathbf{u}_{n+1}) \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  \\phi_1(\\mathbf{u}_{n+q}) & \\dots & \\phi_m(\\mathbf{u}_{n+q})\n  \\end{bmatrix}\n\\end{align}\n$$\nis the feature matrix for the new locations. Therefore approximately\n$$\n\\begin{align}\n  \\begin{bmatrix}\n    \\boldsymbol{\\theta} \\\\\n    \\boldsymbol{\\theta}^*\n  \\end{bmatrix} \\sim N_{n\n  +q} \\left(\\begin{bmatrix}\n    \\mathbf{0}_n \\\\\n    \\mathbf{0}_q\n  \\end{bmatrix},\n  \\begin{bmatrix}\n    \\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top} \\\\\n    \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top & \\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}\n  \\end{bmatrix} \\right).\n\\end{align}\n$$\nClaim $\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}$, where $\\mathbf{A}^\\dagger$ denotes a generalized inverse of matrix $\\mathbf{A}$ such that $\\mathbf{A}\\mathbf{A}^\\dagger = \\mathbf{I}$.\n\n## Kriging under HSGP {.midi}\n\nClaim $\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) = (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}.$\n\nSketch proof below, see details in class:\n\n1. By properties of multivariate normal, $\\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta}, \\boldsymbol{\\Omega}) \\sim N_q(\\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS},\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS})$,\n$$\n\\begin{align}\n  \\mathbb{E}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta}\\\\\n  \\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^{*\\top}) - (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}(\\boldsymbol{\\Phi}\\mathbf{S} \\boldsymbol{\\Phi}^{*\\top}).\n\\end{align}\n$$\n2. Show if $\\boldsymbol{\\Phi}$ has full column rank, which is true under HSGP, then for any matrix $\\mathbf{A}$ of proper dimension, \n$$\\mathbf{S} \\boldsymbol{\\Phi}^\\top(\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}\\boldsymbol{\\Phi}\\mathbf{A} = \\mathbf{A} \\tag{1}$$\nTherefore $\\mathbb{V}_{\\boldsymbol{\\theta}^*}^{HS} \\equiv \\mathbf{0}$.\n\n## Kriging under HSGP {.midi}\n\nUnder the reparameterized model, $\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}$, for $\\mathbf{b} \\sim N_m(0,\\mathbf{I}).$ Therefore\n$$\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger} \\boldsymbol{\\theta} \\\\\n  &= (\\boldsymbol{\\Phi}^*\\mathbf{S}\\boldsymbol{\\Phi}^\\top) (\\boldsymbol{\\Phi}\\mathbf{S}\\boldsymbol{\\Phi}^\\top)^{\\dagger}(\\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}) \\\\\n  &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}. \\quad (\\text{by equation (1) in the last slide})\n\\end{align}\n$$\n\nDuring MCMC sampling, we can obtain posterior predictive samples for $\\boldsymbol{\\theta}^*$ through posterior samples of $\\mathbf{b}$ and $\\mathbf{S}$. Let superscript $(s)$ denote the $s$th posterior sample:\n\n$$\\boldsymbol{\\theta}^{*(s)} = \\boldsymbol{\\Phi}^* \\mathbf{S}^{(s) 1/2} \\mathbf{b}^{(s)}.$$\n\n## Kriging under HSGP -- alternative view {.midi}\n\nUnder the reparameterized model, there is another (perhaps more intuitive) way to recognize the kriging distribution under HSGP.\n\nWe model $\\boldsymbol{\\theta} = \\boldsymbol{\\Phi} \\mathbf{S}^{1/2}\\mathbf{b}$, where $\\mathbf{b}$ is treated as the unknown parameter. Therefore for kriging:\n$$\n\\begin{align}\n  \\boldsymbol{\\theta}^* \\mid (\\boldsymbol{\\theta},\\boldsymbol{\\Omega}) &= \\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b} \\mid (\\mathbf{b},\\boldsymbol{\\Omega}) \\\\\n  &=\\boldsymbol{\\Phi}^*\\mathbf{S}^{1/2}\\mathbf{b}.\n\\end{align}\n$$\n\n## HSGP kriging in `stan` {.midi}\n\nKriging under HSGP can be easily implemented in `stan`.\n\n\n\n::: {.cell output.var='model_in_stan'}\n\n```{.stan .cell-code}\ntransformed data {\n  matrix[q,m] PHI_new;\n}\nparameters {\n  vector[m] b;\n  vector<lower=0>[m] sqrt_S;\n}\ngenerated quantities {\n  vector[q] theta_new = PHI_new * (sqrt_S .* b);\n}\n\n```\n:::\n\n\n\n\n## Recap {.midi}\n\nHSGP is a low rank approximation method for GP.\n\n$$\n\\begin{align}\n  \\mathbf{C}_{ij} \\approx \\sum_{k=1}^m s_k\\phi_k(\\mathbf{u}_i)\\phi_k(\\mathbf{u}_j), \\quad \\mathbf{C} \\approx \\boldsymbol{\\Phi} \\mathbf{S} \\boldsymbol{\\Phi}^\\top,\n\\end{align}\n$$\n\n - for covariance function $C$ which admits a power spectral density.\n - on a box $\\boldsymbol{\\Theta} \\subset \\mathbb{R}^d$.\n - with $m$ number of basis functions.\n \nWe have talked about:\n\n - why HSGP is scalable.\n - how to do posterior sampling and posterior predictive sampling in `stan`.\n \n## HSGP parameters {.midi}\n\n@solin2020hilbert showed that HSGP approximation can be made arbitrarily accurate as $\\boldsymbol{\\Theta}$ and $m$ increase.\n\nBut how to choose:\n\n1. size of the box $\\boldsymbol{\\Theta}$.\n\n2. number of basis functions $m$.\n\n## HSGP approximation box {.midi}\n\nDue to the design of HSGP, the approximation is less accurate near the boundaries of $\\boldsymbol{\\Theta}$.\n\n - Suppose all the coordinates are centered. Let\n $$S_l = \\max_i |\\mathbf{u}_{il}|, \\quad l=1,\\dots,d, \\quad i= 1, \\dots, (n+q)$$\nsuch that $\\boldsymbol{\\Theta}_S = \\prod_{l=1}^d [-S_l,S_l]$ is the smallest box which contains all observed and prediction locations. We should at least ensure $\\boldsymbol{\\Theta} \\supset \\boldsymbol{\\Theta}_S$.\n - We want the box to be large enough to ensure good boundary accuracy. Let $c_l \\ge 1$ be *boundary factors*, we consider\n$$\\boldsymbol{\\Theta} = \\prod_{l=1}^d [-L_l,L_l], \\quad L_l = c_l S_l.$$\n\n \n## HSGP approximation box {.midi}\n\n![](./images/21/HSGPbox.png){fig-align=\"center\" height=\"350\"}\n\nHow much the approximation accuracy deteriorates towards the boundaries depends on smoothness of the true surface. \n\n - E.g., the larger the length scale $\\rho$, the smoother the surface, a smaller box can be used for the same level of boundary accuracy.\n\n \n## HSGP approximation box {.midi}\n\n![](./images/21/HSGPbox.png){fig-align=\"center\" height=\"350\"}\n\nThe larger the box,\n\n - the more basis functions we need for the same level of overall accuracy,\n - hence higher run time.\n\n## HSGP basis functions {.midi}\n\nThe total number of basis functions $m = \\prod_{l=1}^d m_l$, i.e., we need to decide on $m_l$'s, the number of basis functions for each dimension.\n\n - The higher the $m$, the better the overall approximation accuracy, the higher the runtime.\n - $m$ scales exponentially in $d$, hence the HSGP computation complexity $\\mathcal{O}(mn+m)$ also scales exponentially in $d$. Therefore HSGP is only recommended for $d \\le 3$, at most $4$.\n\n## Relationship between $c$ and $m$ {.midi}\n\n@riutort2023practical used simulations with a squared exponential covariance function to investigate the relationship between $c$ and $m$.\n\n![](./images/21/PHSGP_fig2.png){fig-align=\"center\" height=\"375\"}\n\nNotice that $c$ needs to be above a certain *minimum value* to achieve a reasonable boundary accuracy.\n\n## Relationship between $c$, $m$ and $\\rho$ {.midi}\n\nTo summarize:\n\n1. The boundary factor $c$ needs to be above a minimum value, otherwise HSGP approximation is poor no matter how large $m$ is.\n2. As $\\rho$ increases, the surface is less smooth,\n  - $c$ needs to increase to retain boundary accuracy.\n  - $m$ needs to increase to retain overall accuracy.\n3. As $c$ increases, $m$ needs to increase to retain overall accuracy.\n4. As $m$ increases, run time increases. Hence we want to minimize $m$ and $c$ while maintaining certain accuracy level.  \n\nAnd, we **do not** know $\\rho$. So how to choose $c$ and $m$?\n\n## Prepare for next class\n\n1. Work on HW 05 which is due Apr 8\n2. Complete reading to prepare for Thursday's lecture\n3. Thursday's lecture:\n\n  - Parameter tuning for HSGP\n  - How to implement HSGP in `stan`\n\n\n## References\n::: {#refs}\n:::\n",
    "supporting": [
      "21-scalable1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}