{
  "hash": "978b4aede4a33d9d6abfcd498aaa6076",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model checking\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-01-28\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\nOn Thursday, we learned about\n\n-   Different types of priors that can be specified\n\n-   Posterior summaries: point estimates, intervals, and probabilities\n\n-   Posterior predictive distributions\n\n-   We learned about the **generated quantities** code chunk\n\nToday, we will dive into (1) methods for assessing MCMC convergence, and (2) model performance techniques.\n\n# Assessing MCMC convergence\n\n-   Traceplots\n\n-   Effective sample size ($n_{eff}$)\n\n-   MC standard error\n\n-   $\\hat{R}$\n\n-   Sampling issues\n\n# Traceplots\n\n-   The first few MCMC samples are probably not draws from the posterior distribution.\n\n-   It can take hundreds or even thousands of iterations to move from the initial values to the posterior.\n\n-   When the sampler reaches the posterior this is called **convergence**.\n\n-   Samples before convergence are discard as **burn-in**.\n\n-   After convergence the samples should not converge to a single point!\n\n-   They should be draws from the posterior, and ideally look like a caterpillar or bar code.\n\n## Convergence in a few iterations\n\n![](images/06/con1.png){fig-alt=\"con1\" fig-align=\"center\" height=\"7in\"}\n\n## Convergence in a few hundred iterations\n\n![](images/06/con2.png){fig-alt=\"con2\" fig-align=\"center\" height=\"7in\"}\n\n## This one never converged\n\n![](images/06/con3.png){fig-alt=\"con3\" fig-align=\"center\" height=\"7in\"}\n\n## Convergence is questionable\n\n![](images/06/con2.png){fig-alt=\"con4\" fig-align=\"center\" height=\"7in\"}\n\n## Traceplots for linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rstan)\nfit <- readRDS(file = \"linear_regression_ppd_fit.rds\")\nrstan::traceplot(fit, pars = c(\"beta\", \"sigma\"), \n                 inc_warmup = TRUE)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n\n## Autocorrelation\n\n-   Ideally the samples would be independent across iteration.\n\n-   When using MCMC we are obtaining *dependent* samples from the posterior.\n\n-   The autocorrelation function $\\rho(h)$ is the correlation between samples $h$ iterations apart.\n\n-   Lower values are better, but if the chains are long enough even large values can be OK.\n\n-   Highly correlated samples have less information than independent samples.\n\n## Autocorrelation for linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_ac(fit, pars = c(\"beta\", \"sigma\"), \n        separate_chains = TRUE, lags = 25)\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n\n## Autocorrelation for linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bayesplot)\nmcmc_acf(fit, regex_pars = c(\"beta\", \"sigma\"))\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n\n## Effective sample size\n\nThe **effective samples size** is,\n\n$$n_{eff}=ESS(\\theta_i) = \\frac{mS}{1 + 2 \\sum_{h = 1}^{\\infty} \\rho (h)},$$ where $m$ is the number of chains, $S$ is the number of MCMC samples, and $\\rho(h)$ is the $h$th order autocorrelation for $\\theta_i$.\n\n-   The correlated MCMC sample of length $mS$ has the same information as $n_{eff}$ independent samples.\n\n-   Rule of thumb: $n_{eff}$ should be at least a thousand for all parameters.\n\n## Effective sample size for linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit, pars = c(\"beta\", \"sigma\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Standard errors of posterior mean estimates {.midi}\n\n-   The sample mean from our MCMC draws is a point estimate for the posterior mean.\n\n-   The standard error (SE) of this estimate can be used as a diagnostic.\n\n-   Assuming independence, the Monte Carlo standard error is $\\text{MCSE} = \\frac{s}{\\sqrt{S}},$ where $s$ is the sample standard deviation and $S$ is the number of samples.\n\n-   A more realistic standard error is $\\text{MCSE} = \\frac{s}{\\sqrt{n_{eff}}}.$\n\n-   $\\text{MCSE} \\rightarrow 0$ as $S \\rightarrow \\infty$, whereas the standard deviation of the posterior draws approaches the standard deviation of the posterior distribution.\n\n## Assessing mixing using between- and within-sequence variances {.midi}\n\nFor a scalar parameter, $\\theta$, define the MCMC samples as $\\theta_{ij}$ for chain $j=1,\\ldots,m$ and simulations $i = 1,\\ldots,n$. We can compute the between- and within-sequence variances:\n\n\\begin{align*}\nB &= \\frac{n}{m-1}\\sum_{j=1}^m \\left(\\bar{\\theta}_{\\cdot j} - \\bar{\\theta}_{\\cdot\\cdot}\\right)^2,\\quad \\bar{\\theta}_{\\cdot j} = \\frac{1}{n}\\sum_{i=1}^n \\theta_{ij},\\quad\\bar{\\theta}_{\\cdot \\cdot} = \\frac{1}{m} \\sum_{j=1}^m \\bar{\\theta}_{\\cdot j}\\\\\nW &= \\frac{1}{m}\\sum_{j=1}^m s_j^2,\\quad s_j^2=\\frac{1}{n-1}\\sum_{i=1}^n \\left(\\theta_{ij} - \\bar{\\theta}_{\\cdot j}\\right)^2\n\\end{align*}\n\nThe between-sequence variance, $B$ contains a factor of $n$ because it is based on the variance of the within-sequence means, $\\bar{\\theta}_{\\cdot j}$, each of which is an average of $n$ values $\\theta_{ij}$.\n\n## Convergence metric: $\\widehat{R}$ {.midi}\n\nEstimate a total variance $\\mathbb{V}(\\theta | \\mathbf{Y})$ as a weighted mean of $W$, $B$ $$\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y}) = \\frac{n-1}{n}W + \\frac{1}{n}B$$\n\n-   This overestiamtes marginal posterior variance if starting points are overdispersed.\n\n-   Given finite $n$, $W$ underestimates marginal posterior variance.\n\n    -   Single chains have not yet visited all points in the distribution.\n\n    -   When $n \\rightarrow \\infty$, $\\mathbb{E}[W] \\rightarrow \\mathbb{V}(\\theta | \\mathbf{Y})$\n\n-   As $\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})$ overestimates and $W$ underestimates, we can compute\n\n$$\\widehat{R} = \\sqrt{\\frac{\\widehat{\\text{var}}^+(\\theta | \\mathbf{Y})}{W}}, \\quad \\widehat{R} \\rightarrow 1 \\text{ as }n \\rightarrow \\infty.$$\n\n## $\\widehat{R}$ for linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit, pars = c(\"beta\", \"sigma\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nbeta[1] -1.48       0 0.16 -1.79 -1.58 -1.48 -1.37 -1.16  2002    1\nbeta[2]  3.30       0 0.15  3.01  3.19  3.29  3.40  3.59  1980    1\nsigma    1.55       0 0.11  1.36  1.47  1.54  1.62  1.78  1909    1\n\nSamples were drawn using NUTS(diag_e) at Fri Nov 22 10:44:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\nA good rule of thumb is to want $\\widehat{R} \\leq 1.1$.\n\n## What to do if your MCMC does not converge?\n\nGelmanâ€™s *Folk Theorem:*\n\n<div>\n\n> *When you have computational problems, often thereâ€™s a problem with your model.*\n\nSource: [Andrew Gelman](https://statmodeling.stat.columbia.edu/2008/05/13/the_folk_theore/)\n\n</div>\n\n## Sampling issues: slow convergence\n\nPoor chain mixing is usually due to lack of **parameter identification**.\n\n-   A parameter is identified if it has some unique effect on the data generating process that can be separated from the effect of the other parameters.\n\n*Solution:* use simulated data where you know the true parameter values.\n\n-   Informative as to whether the model is sufficient to estimate a parameterâ€™s value.\n\n## Sampling issues: divergent iterations\n\nYou may get a warning in output from Stan with the number of iterations where the NUTS sampler has terminated prematurely.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nWarning messages:\n1: There were 62 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\n2: There were 8 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See\nhttps://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded \n```\n:::\n\n\n\n**Solution:** If this warning appears, you can try:\n\n1.  Increase `adapt_delta` (double, between 0 and 1, defaults to 0.8).\n\n2.  Increase `max_treedepth` (integer, positive, defaults to 10).\n\n3.  Play with `stepsize` (double, positive, defaults to 1).\n\n## Sampling issues: divergent iterations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- sampling(stan_model, data = stan_data, \n                chains = 4, iter = 1000,\n                control = list(adapt_delta = 0.8, \n                               max_treedepth = 10,\n                               stepsize = 1))\n```\n:::\n\n\n\n-   See `help(stan)` for a full list of options that can be set in `control`.\n\n-   Complete set of recommendations: [runtime warnings and convergence problems](https://mc-stan.org/misc/warnings.html)\n\nIf the above doesnâ€™t help change priors then likelihood!\n\n## Coding errors\n\nStan error messages are generally quite informative, however inevitably there are times when it is less clear why code fails.\n\n-   One option is to debug by print.\n\n\n\n::: {.cell output.var='print'}\n\n```{.stan .cell-code}\nmodel {\n  ...\n  print(theta);\n}\n```\n:::\n\n\n\nIn R this prints (neatly) to the console output.\n\nMore details on printing can be found on Stan: [print statements](https://mc-stan.org/docs/reference-manual/statements.html#print-statements.section)\n\n## Coding errors\n\n**Important:** failing a resolution via the above go to <http://mc-stan.org/> and do:\n\n1.  Look through manual for a solution.\n\n2.  Look through user forum for previous answers to similar problems.\n\n3.  Ask a question; be clear, and thorough - post as simple a model that replicates the issue.\n\nOutside of this, you have an endless source of resources using Google/stackoverflow/stackexchange/ChatGPT. You can also post to Ed Discussion, go to office hours, and ask in lecture!\n\n## What to do when things go wrong: summary\n\n-   Problems with sampling are almost invariably problems with the underlying model not the sampling algorithm.\n\n-   Use fake data with all models to test for parameter identification (and that youâ€™ve coded up correctly).\n\n-   To debug a model that fails read error messages carefully, then try *print* statements.\n\n-   Stan has an active developer and user forum, great documentation, and an extensive answer bank.\n\n# Posterior predictive checks\n\n## Posterior predictive checks\n\n-   Last lecture we learned about posterior predictive distributions.\n\n-   These can be used to check the model fit in our observed data.\n\n-   The goal is to check how well our model can generate data that matches the observed data.\n\n-   If our model is \"good\", it should be able to generate new observations that resemble the observed data.\n\n-   To perform the posterior predictive check, we must include the **generated quantities** code chunk.\n\n## Comparing the PPD to the observed data distribution\n\n\n\n::: {.cell layout-nrow=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rstan)\nlibrary(bayesplot)\nY_in_sample <- extract(fit, pars = \"in_sample\")$in_sample\nppc_dens_overlay(Y, Y_in_sample)\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## PPD test statistics\n\n-   The procedure for carrying out a posterior predictive check requires a test quantity, $T(\\mathbf{Y})$, for our observed data.\n\n-   Suppose that we have samples from the posterior predictive distribution, $\\mathbf{Y}^{(s)} = \\left\\{Y_1^{(s)},\\ldots,Y_n^{(s)}\\right\\}$ for $s = 1,\\ldots,S$.\n\n-   We can compute: $T\\left(\\mathbf{Y}^{(s)}\\right) = \\left\\{T\\left(Y_1^{(s)}\\right),\\ldots,\\left(Y_n^{(s)}\\right)\\right\\}$\n\n-   Our posterior predictive check will then compare the distribution of $T\\left(\\mathbf{Y}^{(s)}\\right)$ to the value from our observed data, $T(\\mathbf{Y})$.\n\n-   $T(\\cdot)$ can be any statistics, including mean, median, etc.\n\n-   When the predictive distribution is not consistent with the observed statistics it indicates poor model fit.\n\n## Visualizing posterior predictive check\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\" layout-align=\"center\"}\n\n```{.r .cell-code}\nppc_stat(Y, Y_in_sample, stat = \"mean\") # from bayesplot\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-12-1.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\nppc_stat(Y, Y_in_sample, stat = \"sd\")\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-12-2.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\nq25 <- function(y) quantile(y, 0.25)\nq75 <- function(y) quantile(y, 0.75)\nppc_stat(Y, Y_in_sample, stat = \"q25\")\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-12-3.png){fig-align='center' width=576}\n:::\n\n```{.r .cell-code}\nppc_stat(Y, Y_in_sample, stat = \"q75\")\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-12-4.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n## Posterior predictive p-values\n\n\n\n::: {.cell layout-nrow=\"1\" layout-ncol=\"1\" layout-align=\"center\"}\n\n```{.r .cell-code}\nplot <- ppc_stat(Y, Y_in_sample, stat = \"median\") # from bayesplot\npvalue <- mean(apply(Y_in_sample, 1, median) > median(Y))\nplot + yaxis_text() + # just so I can see y-axis values for specifying them in annotate() below, but can remove this if you don't want the useless y-axis values displayed \n  annotate(\"text\", x = -0.75, y = 150, label = paste(\"p =\", pvalue))\n```\n\n::: {.cell-output-display}\n![](06-model-checking_files/figure-revealjs/unnamed-chunk-13-1.png){fig-align='center' width=576}\n:::\n:::\n\n\n\n-   Bayesian p-value: $p_B = P\\left(T\\left(\\mathbf{Y}^{(s)}\\right) \\geq T\\left(\\mathbf{Y}\\right) | \\mathbf{Y}\\right)$\n\n## Debugging with `shinystan`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(shinystan)\nmy_sso <- launch_shinystan(fit)\n```\n:::\n\n\n\n## Prepare for next class\n\n-   Work on [HW 01](https://biostat725-sp25.netlify.app/hw/hw-01) which is due before next class\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Bayesian Workflow\n",
    "supporting": [
      "06-model-checking_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}