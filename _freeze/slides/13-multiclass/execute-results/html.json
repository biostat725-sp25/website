{
  "hash": "f7f78f634344834d88a2218853f514ea",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiclass Classification\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-20\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   On Tuesday, we learned about classification using logistic regression.\n\n-   Today, we will focus on multiclass classification: multinomial regression, ordinal regression.\n\n## Multiclass regression\n\n- Often times one encounters an outcome variable that is nominal and has more than two categories.\n\n- If there is no inherent rank or order to the variable, we can use multinomial regression. Examples include: \n\n  - gender (male, female, non-binary), \n  \n  - blood type (A, B, AB, O). \n\n- If there is an order to the variable, we can use ordinal regression. Examples include: \n\n  - stages of cancer (stage I, II, III, IV), \n  \n  - pain level (mild, moderate, severe).\n\n## Multinomial random variable\n\n- Assume an outcome $Y_i \\in \\{1,\\ldots,K\\}$ for $i = 1,\\ldots,n$. \n\n- The likelihood in multinomial regression can be written as the following categorical likelihood, \n\n$$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}},$$\n\n- $\\delta_{ij} = 1(Y_i = j)$ is the Kronecker delta.\n\n- Since $Y_i$ is discrete, we only need to specify $P(Y_i = j)$ for all $i$ and $j$. \n\n## Log-linear regression\n\n- One way to motivate multinomial regression is through a log-linear specification:\n\n$$\\log P(Y_i = j) = \\mathbf{x}_i\\boldsymbol{\\beta}_j - \\log Z.$$\n\n- $\\boldsymbol{\\beta}_j$ is a $j$ specific set of regression parameters.\n\n- $P(Y_i = j) = \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}/Z$.\n\n- $Z$ is a normalizing constant that guarentees that $\\sum_{j=1}^K P(Y_i = j) = 1$.\n\n## Finding the normalizing constant\n\n- We know that,\n\n\\begin{align*}\n1 &= \\sum_{j=1}^K P(Y_i = j) = \\frac{1}{Z}\\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\\\\\n&\\implies Z = \\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\n\\end{align*}\n\n## Multinomial probabilities\n\n- Thus, we have the following,\n\n$$P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.$$\n\n- This function is called the **softmax** function.\n\n- Unfortunately, this specification is not identifiable.\n\n## Identifiability issue\n\n- We can add a vector $\\mathbf{c}$ to all parameters and get the same result,\n\n\\begin{align*}\n\\frac{\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_k + \\mathbf{c})\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_j + \\mathbf{c})\\}} &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{align*}\n\n- A common solution is to set: $\\boldsymbol{\\beta}_K = \\mathbf{0}$.\n\n## Updating the probabilities\n\n- Using the identifiability constraint of $\\boldsymbol{\\beta}_K = \\mathbf{0}$, the probabilities become, \n\\begin{align*}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\},\\\\\nP(Y_i = K) &= \\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{align*}\n\n- How to interpret the $\\boldsymbol{\\beta}_k$?\n\n## Deriving the additive log ratio model\n\n- Using our specification of the probabilities, it can be seen that,\n\n\\begin{align*}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{align*}\n\n$\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}$\n\n## Additive log ratio model\n\n- If outcome $K$ is chosen as reference, the $K âˆ’ 1$ regression equations are:\n\n$$\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}$$\n\n- This formulation is called **additive log ratio**.\n\n- $\\boldsymbol{\\beta}_k$ has a nice interpretation as a **relative risk**.\n\n## Getting back to the likelihood\n\n- The log-likelihood can be written as,\n\n$$\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).$$\n\n- The $P(Y_i = j)$ are given by the additive log ratio model.\n\n- As Bayesians, we only need to specify priors for $\\boldsymbol{\\beta}_k, k \\in \\{1,\\ldots,K-1\\}$.\n\n## Multinomial regression in Stan\n\n- Hard coding the likelihood.\n\n\n\n::: {.cell output.var='alr'}\n\n```{.stan .cell-code}\n// additive_log_ratio.stan\nfunctions {\n  matrix compute_alr_probs(int n, int K, int p, matrix X, matrix beta) {\n    matrix[n, K] probs;\n    matrix[n, K - 1] expXbeta = exp(X * beta);\n    for (i in 1:n) {\n      real sum_i = sum(expXbeta[i, ]);\n      for (j in 1:K) {\n        if (j < K) {\n          probs[i, j] = expXbeta[i, j] / (1 + sum_i);\n        }\n        if (j == K) probs[i, j] = 1 - sum(probs[i, 1:(K - 1)]);\n      }\n    }\n    return probs\n  }\n}\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n  matrix[n, K] delta;\n}\nparameters {\n  matrix[p, K - 1] beta;\n}\nmodel {\n  matrix[n, K] probs = compute_alr_probs(n, K, p, X, beta);\n  for (i in 1:n) {\n    for (j in 1:K) {\n      target += delta[i, j] * log(probs[i, j]);\n    }\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\n```\n:::\n\n\n\n## Multinomial regression in Stan\n\n- Non-identifiable version.\n\n\n\n::: {.cell output.var='multi_bad'}\n\n```{.stan .cell-code}\n// multi_logit_bad.stan\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n}\nparameters {\n  matrix[p, K] beta;\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    Y[i] ~ categorical_logit(Xbeta[i]');\n  }\n  to_vector(beta) ~ normal(0, 10);\n}\n```\n:::\n\n\n\n[categorical_logit](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#categorical-logit-glm)\n\n## Multinomial regression in Stan\n\n- Zero identifiability constraint.\n\n\n\n::: {.cell output.var='multi'}\n\n```{.stan .cell-code}\n// multi_logit.stan\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[p] zeros = rep_vector(0, p);\n}\nparameters {\n  matrix[p, K - 1] beta_raw;\n}\ntransformed parameters {\n  matrix[p, K] beta = append_col(beta_raw, zeros);\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    Y[i] ~ categorical_logit(Xbeta[i]');\n  }\n  to_vector(beta) ~ normal(0, 10);\n}\n```\n:::\n\n\n\n## Ordinal regression\n\nLet $Y_i \\in \\{1,\\ldots,K\\}$ be an ordinal outcome with $K$ categories. \n\n- The likelihood in ordinal regression is identical to the one from multinomial regression,\n\n$$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}}.$$\n\n- We need to add additional constraints that guarentee ordinality. \n\n## Proportional odds assumption\n\n- $P(Y_i \\leq k)$ is the cumulative probability of $Y_i$ less than or equal to a specific category $k=1,\\ldots,K-1$. \n\n- The odds of being less than or equal to a particular category can be defined as, $$\\frac{P(Y\\leq k)}{P(Y > k)} \\text { for } k=1,\\ldots,K-1.$$\n\n- Not defined for $k = K$, since division by zero is not defined.\n\n## Proportional odds regression\n\nThe log odds can then be modeled as follows, $$\\log \\frac{P(Y_i\\leq k)}{P(Y_i > k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}$$\n\n- Why $-\\boldsymbol{\\beta}$?\n\n- $\\boldsymbol{\\beta}$ is a common regression parameter. For a one-unit increase in $x_{ij}$, $\\beta_j$ is the change in log odds of moving to a more severe level of the outcome $Y_i$.\n\n- $\\alpha_k$ for $k = 1,\\ldots,K-1$ are $k$-specific intercepts that corresponds to the log odds of moving from level $k$ to $k+1$. \n\n## Understanding the probabilities\n\n- One can solve for $P(Y_i \\leq k), \\quad k = 1,\\ldots,K-1$),\n\n$$P(Y_i \\leq k) = \\frac{\\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}}{1 + \\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}} = \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}).$$\n\n\n  -   $P(Y_i \\leq K) = 1$.\n\nThe individual probabilities are then given by,\n\n\\begin{align*}\nP(Y_i = k) &= P(Y_i \\leq k) - P(Y_i \\leq k-1)\\\\\n&= \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(\\alpha_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{align*}\n\n## A latent variable representation\n\n- Define a latent variable,\n\n$$Y_i^* = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\text{Logistic}(0, 1).$$\n\n  - $\\mathbb{E}[\\epsilon_i] = 0$.\n\n  - $\\mathbb{V}(\\epsilon_i) = \\pi^2/3$.\n\n  - CDF: $P(\\epsilon_i \\leq x) = \\frac{1}{1 + \\exp\\{-x\\}} = \\frac{\\exp\\{x\\}}{1 + \\exp\\{x\\}} = \\text{expit}(x).$\n\n## A latent variable representation\n\n- Define a set of $K-1$ cut-points, $(c_1,\\ldots,c_{K-1}) \\in \\mathbb{R}^{K-1}$. We also define $c_0 = -\\infty, c_K = \\infty$.  \n\n- Our ordinal random variable can be generated as,\n\n$$Y_i = \\left\\{\n\\begin{matrix*}[l]\n1 & c_0 < Y_i^* \\leq c_1\\\\\n2 & c_1 < Y_i^* \\leq c_2\\\\\n\\vdots & \\\\\nK & c_{K-1} < Y_i^* \\leq c_K\\\\\n\\end{matrix*}\n\\right.$$\n\n## Equivalence of the two specifications\n\n- Probabilities under the latent specification:\n\\begin{align*}\nP(Y_i = k) &= P(c_{k-1} < Y_i^* \\leq c_k)\\\\\n&= P(c_{k-1} < \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n&= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i < c_{k-1})\\\\\n&= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i < c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n&= \\text{expit}(c_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(c_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{align*}\n\n- Equivalency:\n\n  - $\\alpha_k = c_k, \\quad k = 1,\\ldots, K-1$, assuming that $\\alpha_k < \\alpha_{k+1}$.\n  \n## Ordinal regression using Stan\n\n\n\n::: {.cell output.var='ordinal'}\n\n```{.stan .cell-code}\n// ordinal.stan\ndata {\n  int<lower = 2> K;\n  int<lower = 0> n;\n  int<lower = 1> p;\n  int<lower = 1, upper = K> Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}\n```\n:::\n\n\n\n[ordered_logistic_glm_lpmf](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#ordered-logistic-generalized-linear-model-ordinal-regression)\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Missing data\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}