{
  "hash": "dc4a7c8eb7801bfe2b4bc7b0b0d5d9c3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Multiclass Classification\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-20\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Review of last lecture\n\n-   On Tuesday, we learned about classification using logistic regression.\n\n-   Today, we will focus on multiclass classification: multinomial regression, ordinal regression.\n\n## Multiclass regression\n\n-   Often times one encounters an outcome variable that is nominal and has more than two categories.\n\n-   If there is no inherent rank or order to the variable, we can use multinomial regression. Examples include:\n\n    -   gender (male, female, non-binary),\n\n    -   blood type (A, B, AB, O).\n\n-   If there is an order to the variable, we can use ordinal regression. Examples include:\n\n    -   stages of cancer (stage I, II, III, IV),\n\n    -   pain level (mild, moderate, severe).\n\n## Multinomial random variable\n\n-   Assume an outcome $Y_i \\in \\{1,\\ldots,K\\}$ for $i = 1,\\ldots,n$.\n\n-   The likelihood in multinomial regression can be written as the following categorical likelihood,\n\n$$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}},$$\n\n-   $\\delta_{ij} = 1(Y_i = j)$ is the Kronecker delta.\n\n-   Since $Y_i$ is discrete, we only need to specify $P(Y_i = j)$ for all $i$ and $j$.\n\n## Log-linear regression\n\n-   One way to motivate multinomial regression is through a log-linear specification:\n\n$$\\log P(Y_i = j) = \\mathbf{x}_i\\boldsymbol{\\beta}_j - \\log Z.$$\n\n-   $\\boldsymbol{\\beta}_j$ is a $j$ specific set of regression parameters.\n\n-   $P(Y_i = j) = \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}/Z$.\n\n-   $Z$ is a normalizing constant that guarentees that $\\sum_{j=1}^K P(Y_i = j) = 1$.\n\n## Finding the normalizing constant\n\n-   We know that,\n\n\\begin{align*}\n1 &= \\sum_{j=1}^K P(Y_i = j) = \\frac{1}{Z}\\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\\\\\n&\\implies Z = \\sum_{j=1}^K \\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}\n\\end{align*}\n\n## Multinomial probabilities\n\n-   Thus, we have the following,\n\n$$P(Y_i = k) = \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.$$\n\n-   This function is called the **softmax** function.\n\n-   Unfortunately, this specification is not identifiable.\n\n## Identifiability issue\n\n-   We can add a vector $\\mathbf{c}$ to all parameters and get the same result,\n\n\\begin{align*}\n\\frac{\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_k + \\mathbf{c})\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i(\\boldsymbol{\\beta}_j + \\mathbf{c})\\}} &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{\\sum_{j = 1}^K\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n\\end{align*}\n\n-   A common solution is to set: $\\boldsymbol{\\beta}_K = \\mathbf{0}$.\n\n## Updating the probabilities\n\n-   Using the identifiability constraint of $\\boldsymbol{\\beta}_K = \\mathbf{0}$, the probabilities become, \\begin{align*}\n    P(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}, \\quad k \\in \\{1,\\ldots,K-1\\},\\\\\n    P(Y_i = K) &= \\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}.\n    \\end{align*}\n\n-   How to interpret the $\\boldsymbol{\\beta}_k$?\n\n## Deriving the additive log ratio model\n\n-   Using our specification of the probabilities, it can be seen that,\n\n\\begin{align*}\nP(Y_i = k) &= \\frac{\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\\\\n&= \\left[\\frac{1}{1 + \\sum_{j = 1}^{K-1}\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_j\\}}\\right]\\exp\\{\\mathbf{x}_i\\boldsymbol{\\beta}_k\\}\\\\\n&= P(Y_i = K) \\exp\\{\\mathbf{x}_i \\boldsymbol{\\beta}_k\\}.\n\\end{align*}\n\n$\\implies \\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots,K-1\\}$\n\n## Additive log ratio model\n\n-   If outcome $K$ is chosen as reference, the $K âˆ’ 1$ regression equations are:\n\n$$\\log \\frac{P(Y_i = k)}{P(Y_i = K)} = \\mathbf{x}_i \\boldsymbol{\\beta}_k, \\quad k \\in \\{1,\\ldots, K-1\\}$$\n\n-   This formulation is called **additive log ratio**.\n\n-   $\\boldsymbol{\\beta}_k$ has a nice interpretation as a **relative risk**.\n\n## Getting back to the likelihood\n\n-   The log-likelihood can be written as,\n\n$$\\log f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\sum_{i=1}^n \\sum_{j=1}^K \\delta_{ij} \\log P(Y_i = j).$$\n\n-   The $P(Y_i = j)$ are given by the additive log ratio model.\n\n-   As Bayesians, we only need to specify priors for $\\boldsymbol{\\beta}_k, k \\in \\{1,\\ldots,K-1\\}$.\n\n## Multinomial regression in Stan\n\n-   Hard coding the likelihood.\n\n\n\n\n\n\n::: {.cell output.var='alr'}\n\n```{.stan .cell-code}\n// additive_log_ratio.stan\nfunctions {\n  matrix compute_alr_probs(int n, int K, int p, matrix X, matrix beta) {\n    matrix[n, K] probs;\n    matrix[n, K - 1] expXbeta = exp(X * beta);\n    for (i in 1:n) {\n      real sum_i = sum(expXbeta[i, ]);\n      for (j in 1:K) {\n        if (j < K) {\n          probs[i, j] = expXbeta[i, j] / (1 + sum_i);\n        }\n        if (j == K) probs[i, j] = 1 - sum(probs[i, 1:(K - 1)]);\n      }\n    }\n    return probs\n  }\n}\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n  matrix[n, K] delta;\n}\nparameters {\n  matrix[p, K - 1] beta;\n}\nmodel {\n  matrix[n, K] probs = compute_alr_probs(n, K, p, X, beta);\n  for (i in 1:n) {\n    for (j in 1:K) {\n      target += delta[i, j] * log(probs[i, j]);\n    }\n  }\n  target += normal_lpdf(to_vector(beta) | 0, 10);\n}\n```\n:::\n\n\n\n\n\n\n## Multinomial regression in Stan\n\n-   Non-identifiable version.\n\n\n\n\n\n\n::: {.cell output.var='multi_bad'}\n\n```{.stan .cell-code}\n// multi_logit_bad.stan\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n}\nparameters {\n  matrix[p, K] beta;\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    Y[i] ~ categorical_logit(Xbeta[i]');\n  }\n  to_vector(beta) ~ normal(0, 10);\n}\n```\n:::\n\n\n\n\n\n\n[categorical_logit](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#categorical-logit-glm)\n\n## Multinomial regression in Stan\n\n-   Zero identifiability constraint.\n\n\n\n\n\n\n::: {.cell output.var='multi'}\n\n```{.stan .cell-code}\n// multi_logit.stan\ndata {\n  int<lower = 1> K;\n  int<lower = 1> n;\n  int<lower = 1> p;\n  array[n] int<lower = 1, upper = K> Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[p] zeros = rep_vector(0, p);\n}\nparameters {\n  matrix[p, K - 1] beta_raw;\n}\ntransformed parameters {\n  matrix[p, K] beta = append_col(beta_raw, zeros);\n}\nmodel {\n  matrix[n, K] Xbeta = X * beta;\n  for (i in 1:n) {\n    Y[i] ~ categorical_logit(Xbeta[i]');\n  }\n  to_vector(beta) ~ normal(0, 10);\n}\n```\n:::\n\n\n\n\n\n\n## Ordinal regression\n\nLet $Y_i \\in \\{1,\\ldots,K\\}$ be an ordinal outcome with $K$ categories.\n\n-   The likelihood in ordinal regression is identical to the one from multinomial regression,\n\n$$f(\\mathbf{Y} | \\boldsymbol{\\beta}) = \\prod_{i=1}^n \\prod_{j=1}^K P(Y_i = j)^{\\delta_{ij}}.$$\n\n-   We need to add additional constraints that guarentee ordinality.\n\n## Proportional odds assumption\n\n-   $P(Y_i \\leq k)$ is the cumulative probability of $Y_i$ less than or equal to a specific category $k=1,\\ldots,K-1$.\n\n-   The odds of being less than or equal to a particular category can be defined as, $$\\frac{P(Y\\leq k)}{P(Y > k)} \\text { for } k=1,\\ldots,K-1.$$\n\n-   Not defined for $k = K$, since division by zero is not defined.\n\n## Proportional odds regression\n\nThe log odds can then be modeled as follows, $$\\log \\frac{P(Y_i\\leq k)}{P(Y_i > k)} = \\text{logit}P(Y_i\\leq k) = \\alpha_k - \\mathbf{x}_i \\boldsymbol{\\beta}$$\n\n-   Why $-\\boldsymbol{\\beta}$?\n\n-   $\\boldsymbol{\\beta}$ is a common regression parameter. For a one-unit increase in $x_{ij}$, $\\beta_j$ is the change in log odds of moving to a more severe level of the outcome $Y_i$.\n\n-   $\\alpha_k$ for $k = 1,\\ldots,K-1$ are $k$-specific intercepts that corresponds to the log odds of moving from level $k$ to $k+1$.\n\n## Understanding the probabilities\n\n-   One can solve for $P(Y_i \\leq k), \\quad k = 1,\\ldots,K-1$),\n\n$$P(Y_i \\leq k) = \\frac{\\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}}{1 + \\exp\\{\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}\\}} = \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}).$$\n\n-   $P(Y_i \\leq K) = 1$.\n\nThe individual probabilities are then given by,\n\n\\begin{align*}\nP(Y_i = k) &= P(Y_i \\leq k) - P(Y_i \\leq k-1)\\\\\n&= \\text{expit}(\\alpha_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(\\alpha_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n\\end{align*}\n\n## A latent variable representation\n\n-   Define a latent variable,\n\n$$Y_i^* = \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i, \\quad \\epsilon_i \\sim \\text{Logistic}(0, 1).$$\n\n-   $\\mathbb{E}[\\epsilon_i] = 0$.\n\n-   $\\mathbb{V}(\\epsilon_i) = \\pi^2/3$.\n\n-   CDF: $P(\\epsilon_i \\leq x) = \\frac{1}{1 + \\exp\\{-x\\}} = \\frac{\\exp\\{x\\}}{1 + \\exp\\{x\\}} = \\text{expit}(x).$\n\n## A latent variable representation\n\n-   Define a set of $K-1$ cut-points, $(c_1,\\ldots,c_{K-1}) \\in \\mathbb{R}^{K-1}$. We also define $c_0 = -\\infty, c_K = \\infty$.\n\n-   Our ordinal random variable can be generated as,\n\n$$Y_i = \\left\\{\n\\begin{matrix*}[l]\n1 & c_0 < Y_i^* \\leq c_1\\\\\n2 & c_1 < Y_i^* \\leq c_2\\\\\n\\vdots & \\\\\nK & c_{K-1} < Y_i^* \\leq c_K\\\\\n\\end{matrix*}\n\\right.$$\n\n## Equivalence of the two specifications\n\n-   Probabilities under the latent specification: \\begin{align*}\n    P(Y_i = k) &= P(c_{k-1} < Y_i^* \\leq c_k)\\\\\n    &= P(c_{k-1} < \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k)\\\\\n    &= P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i \\leq c_k) - P(\\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i < c_{k-1})\\\\\n    &= P(\\epsilon_i \\leq c_k - \\mathbf{x}_i \\boldsymbol{\\beta}) - P(\\epsilon_i < c_{k-1} - \\mathbf{x}_i \\boldsymbol{\\beta})\\\\\n    &= \\text{expit}(c_k - \\mathbf{x}_i\\boldsymbol{\\beta}) - \\text{expit}(c_{k-1} - \\mathbf{x}_i\\boldsymbol{\\beta}).\n    \\end{align*}\n\n-   Equivalency:\n\n    -   $\\alpha_k = c_k, \\quad k = 1,\\ldots, K-1$, assuming that $\\alpha_k < \\alpha_{k+1}$.\n\n## Ordinal regression using Stan\n\n\n\n\n\n\n::: {.cell output.var='ordinal'}\n\n```{.stan .cell-code}\n// ordinal.stan\ndata {\n  int<lower = 2> K;\n  int<lower = 0> n;\n  int<lower = 1> p;\n  int<lower = 1, upper = K> Y[n];\n  matrix[n, p] X;\n}\nparameters {\n  vector[p] beta;\n  ordered[K - 1] alpha;\n}\nmodel {\n  target += ordered_logistic_glm_lpmf(Y | X, beta, alpha);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(alpha | 0, 10);\n}\n```\n:::\n\n\n\n\n\n\n[ordered_logistic_glm_lpmf](https://mc-stan.org/docs/functions-reference/bounded_discrete_distributions.html#ordered-logistic-generalized-linear-model-ordinal-regression)\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Missing data\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}