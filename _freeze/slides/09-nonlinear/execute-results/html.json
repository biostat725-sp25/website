{
  "hash": "414b1ed62a7718bbf50cf8446f551704",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Nonlinear Regression\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-06\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nLoaded lars 1.3\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:mvtnorm':\n\n    dmvt, logdet, rmvt\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nnimble version 1.2.1 is loaded.\nFor more information on NIMBLE and a User Manual,\nplease visit https://R-nimble.org.\n\nNote for advanced users who have written their own MCMC samplers:\n  As of version 0.13.0, NIMBLE's protocol for handling posterior\n  predictive nodes has changed in a way that could affect user-defined\n  samplers in some situations. Please see Section 15.5.1 of the User Manual.\n\n\nAttaching package: 'nimble'\n\n\nThe following objects are masked from 'package:LaplacesDemon':\n\n    cloglog, dcat, dinvgamma, is.model, logdet, logit, rcat, rinvgamma\n\n\nThe following object is masked from 'package:mvtnorm':\n\n    logdet\n\n\nThe following object is masked from 'package:stats':\n\n    simulate\n\n\nThe following object is masked from 'package:base':\n\n    declare\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Review of last lecture\n\n-   On Tuesday, we put all of our skills together and learned about the Bayesian workflow.\n\n-   We have now learned all the skills needed to perform Bayesian inference.\n\n-   The rest of the course we will introduce new models and data types that are useful for performing biomedical data science.\n\n## Learning objectives today\n\n-   Thus far, we have focused on linear regression models.\n\n-   Today we will focus on two common approaches that use linear regression to build nonlinear associations: polynomial regression and b-splines.\n\n-   Both approaches work by transforming a single predictor variable into several synthetic variables.\n\n-   We will also look at a change point model, that encodes clinical context into a nonlinear framework.\n\n## Linear regression\n\n-   Consider the classic parametric model:\n\n$$Y_i = \\alpha + X_i \\beta + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2).$$\n\n-   Assumptions:\n\n    1.  $\\epsilon_i$ are independent.\n\n    2.  $\\epsilon_i$ are Gaussian.\n\n    3.  The mean of $Y_i$ is linear in $X_i$.\n\n    4.  The residual distribution does not depend on $X_i$.\n\n**Today we will generalize the linearity assumption.**\n\n## Nonlinear regression\n\n-   Define: $\\mu_i = \\mathbb{E}[Y_i] = \\alpha + x_i\\beta.$\n\n-   The mean process can be modeled flexibly, $\\mu_i = g(X_i)$, where $g$ is some function that relates $X_i$ to $\\mathbb{E}[Y_i].$\n\n-   A form of nonlinear regression approximates the function $g$ using a finite basis expansion, $$g(X_i) = \\alpha + \\sum_{j=1}^J B_j(X_i)\\beta_j,$$ where $B_j(X)$ are known basis functions and $\\beta_j$ are unknown parameters that determine the shape of $g$.\n\n## Nonlinear regression\n\n-   Example: Polynomial regression takes $B_j(X_i) = X_i^j$.\n\n-   Example: Gaussian radial basis functions: $$B_j(X_i) = \\exp\\left\\{-\\frac{|X_i - \\nu_j|^2}{l^2}\\right\\},$$ where $\\nu_j$ are centers of the basis functions and $l$ is a common width parameter.\n\n-   The number of of basis functions and the width parameter $l$ controls the scale at which the model can vary as a function of $X_i$.\n\n## Nonlinear regression {.smaller}\n\n-   Example: The cubic B-spline basis function is the following piecewise cubic polynomial:\n\n$$B_j(X_i) = \\left\\{\n\\begin{matrix*}[l]\n\\frac{1}{6}u^3 & \\text{for }X_i \\in (\\nu_j,\\nu_{j+1}), & u = (X_i - \\nu_j) / \\delta\\\\\n\\frac{1}{6}(1 + 3u + 3u^2 - 3u^3) & \\text{for }X_i \\in (\\nu_{j+1},\\nu_{j+2}), & u = (X_i - \\nu_{j+1}) / \\delta\\\\\n\\frac{1}{6}(4 - 6u^2 + 3u^3) & \\text{for }X_i \\in (\\nu_{j+2},\\nu_{j+3}), & u = (X_i - \\nu_{j+2}) / \\delta\\\\\n\\frac{1}{6}(1 - 3u + 3u^2 - u^3) & \\text{for }X_i \\in (\\nu_{j+3},\\nu_{j+4}), & u = (X_i - \\nu_{j+3}) / \\delta\\\\\n0 & \\text{otherwise.}\n\\end{matrix*}\n\\right.$$\n\n-   B-splines are a piecewise continuous function defined conditional on some set of knots.\n\n-   Here we assume a uniform knot locations $\\nu_{j + k} = \\nu_j + \\delta k$.\n\n-   B-splines have compact support, so the design matrix is sparse.\n\n## Nonlinear regression\n\n-   Conditionally on the selected bases $B$, the model is linear in the parameters. Hence we can write, $$Y_i = \\mu(X_i) + \\epsilon_i = \\mathbf{w}_i \\boldsymbol{\\beta} + \\epsilon_i,$$ with $\\mathbf{w}_i = (B_1(X_i),\\ldots,B_J(X_i))$.\n\n-   Model fitting can proceed as in linear regression models, since the resulting model is linear in $\\boldsymbol{\\beta}$.\n\n-   It is often useful to center the basis function model around the linear model, $\\mu(X_i) = \\alpha + X_i \\beta + \\mathbf{w}_i\\boldsymbol{\\beta}$.\n\n## Glaucoma disease progression {.midi}\n\n-   Today we will use data from the [Rotterdam Ophthalmic Data Repository](http://www.rodrep.com/longitudinal-glaucomatous-vf-data---description.html).\n\n-   Glaucoma is the leading cause of irreversible blindness world wide with over 60 million glaucoma patients as of 2012. Since impairment caused by glaucoma is irreversible, early detection of disease progression is crucial for effective treatment.\n\n-   Patients with glaucoma are routinely followed up and administered visual fields, a functional assessment of their vision.\n\n-   After each visual field test their current disease status is reported as a mean deviation (MD) value, measured in decibels (dB). A lower mean deviation indicates worse vision.\n\n-   Central clinical challenges are i) identifying disease progression of MD, and ii) predicting future MD.\n\n## Glaucoma data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Load and process data to obtain data for an example patient\ndat <- read.csv(file = \"LongGlaucVF_20150216/VisualFields.csv\")\ndat <- dat[order(dat$STUDY_ID, dat$SITE), ]\ndat$EYE_ID <- cumsum(!duplicated(dat[, c(\"STUDY_ID\", \"SITE\")]))\ndat_pat <- dat[dat$EYE_ID == \"4\", ] # 4\ndat_pat$time <- (dat_pat$AGE - dat_pat$AGE[1]) / 365\ndat_pat <- dat_pat[, c(\"time\", \"MD\")]\ncolnames(dat_pat) <- c(\"X\", \"Y\")\nglimpse(dat_pat)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 18\nColumns: 2\n$ X <dbl> 0.0000000, 0.6136986, 1.1315068, 1.6520548, 2.1671233, 2.6794521, 3.â€¦\n$ Y <dbl> -2.76, -2.08, -1.91, -2.63, -5.13, -2.14, -1.97, -0.83, -1.75, -1.61â€¦\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## An example patient\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Linear regression\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Linear regression\n\n-   Linear regression is simple.\n\n-   Linear regression is highly interpretable. It encodes disease progression into a slope, which is the amount of MD loss (dB) per year.\n\n    -   Interpretability is important!\n\n-   A linear relationship may be an oversimplification.\n\n-   Often in prediction contexts, a nonlinear approach is preferred.\n\n## Polynomials\n\n-   Model for the mean process becomes nonlinear:\n\n$$\\mathbb{E}[MD_i] = \\alpha + \\beta_1 Time_i + \\cdots + \\beta_p Time_i^p$$\n\n-   $p$ is chosen depending on the degree of non-linearity.\n\n-   When fitting non-linear regression in Bayesian context it is useful to standardize the data.\n\n## Polynomial regression in Stan\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat_poly <- data.frame(\n  Y = scale(dat_pat$Y),\n  X = scale(dat_pat$X)\n)\ndat_poly$X2 <- dat_poly$X^2\nstan_data <- list(\n  n = nrow(dat_pat),\n  p = 2,\n  Y = dat_poly$Y,\n  X = cbind(dat_poly$X, dat_poly$X2),\n)\ncompile_model <- stan_model(file = \"nonlinear_linear.stan\")\nfit_quadratic <- sampling(compile_model, data = stan_data)\n```\n:::\n\n\n\n\n\n\n## Polynomial regression in Stan\n\n\n\n\n\n\n::: {.cell output.var='test'}\n\n```{.stan .cell-code}\n// saved in nonlinear_linear.stan\ndata {\n  int<lower = 1> n; // number of observations\n  int<lower = 1> p; // number of covariates\n  vector[n] Y; // outcome vector\n  matrix[n, p] X; // covariate vector\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n}\nmodel {\n  target += normal_lpdf(Y | X * beta, sigma); // likelihood\n  target += normal_lpdf(alpha | 0, 1); // prior for beta\n  target += normal_lpdf(beta | 0, 1); // prior for beta\n  target += inv_gamma_lpdf(sigma | 3, 1); // prior for sigma\n}\ngenerated quantities {\n  vector[n] in_sample;\n  vector[n] log_lik;\n  vector[n] mu;\n  for (i in 1:n) {\n    mu[i] = alpha + X[i, ] * beta;\n    in_sample[i] = normal_rng(mu[i], sigma);\n    log_lik[i] = normal_lpdf(Y[i] |  mu[i], sigma);\n  }\n}\n```\n:::\n\n\n\n\n\n\n## Quadratic regression\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Extract posterior mean for $\\mu$\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmu <- rstan::extract(fit_quadratic, pars = \"mu\")$mu\nmu <- mu * sd(dat_pat$Y) + mean(dat_pat$Y) # transform to original unstandardized Y_i\nmu_mean <- apply(mu, 2, mean)\nmu_lower <- apply(mu, 2, function(x) quantile(x, probs = 0.025))\nmu_upper <- apply(mu, 2, function(x) quantile(x, probs = 0.975))\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=864}\n:::\n:::\n\n\n\n\n\n\n## Cubic regression\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## B-spline regression with 5 knots\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=480}\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-15-2.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n\n\n## B-spline regression with 10 knots\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-16-1.png){fig-align='center' width=480}\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-16-2.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n\n\n## B-spline regression {.smaller}\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(splines)\nnum_knots <- 5\nknot_list <- quantile(dat_pat$X, probs = seq(from = 0, to = 1, length.out = num_knots))\nB <- bs(dat_pat$X,\n        knots = knot_list[-c(1, num_knots)], \n        degree = 3, \n        intercept = TRUE)\nB\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 1            2            3           4            5\n [1,] 1.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n [2,] 0.3932160225 0.5205689817 0.0833170637 0.002897932 0.0000000000\n [3,] 0.1303337386 0.6136422453 0.2378607351 0.018163281 0.0000000000\n [4,] 0.0220025742 0.5116331668 0.4098320335 0.056532225 0.0000000000\n [5,] 0.0001737807 0.3325401051 0.5396793792 0.127606735 0.0000000000\n [6,] 0.0000000000 0.1814804087 0.5792969517 0.238573820 0.0006488199\n [7,] 0.0000000000 0.0883598392 0.5363295039 0.367729756 0.0075809005\n [8,] 0.0000000000 0.0097143260 0.3289042281 0.593688910 0.0676925363\n [9,] 0.0000000000 0.0003772242 0.1863761908 0.659307134 0.1539394508\n[10,] 0.0000000000 0.0000000000 0.0788259578 0.624941237 0.2955691681\n[11,] 0.0000000000 0.0000000000 0.0357900241 0.542804472 0.4124556582\n[12,] 0.0000000000 0.0000000000 0.0107764123 0.421496598 0.5286019082\n[13,] 0.0000000000 0.0000000000 0.0006952375 0.263269190 0.6110273290\n[14,] 0.0000000000 0.0000000000 0.0000000000 0.145095180 0.5888020313\n[15,] 0.0000000000 0.0000000000 0.0000000000 0.060977201 0.4468755310\n[16,] 0.0000000000 0.0000000000 0.0000000000 0.016061781 0.2349278937\n[17,] 0.0000000000 0.0000000000 0.0000000000 0.002189635 0.0740048857\n[18,] 0.0000000000 0.0000000000 0.0000000000 0.000000000 0.0000000000\n                 6            7\n [1,] 0.0000000000 0.0000000000\n [2,] 0.0000000000 0.0000000000\n [3,] 0.0000000000 0.0000000000\n [4,] 0.0000000000 0.0000000000\n [5,] 0.0000000000 0.0000000000\n [6,] 0.0000000000 0.0000000000\n [7,] 0.0000000000 0.0000000000\n [8,] 0.0000000000 0.0000000000\n [9,] 0.0000000000 0.0000000000\n[10,] 0.0006636368 0.0000000000\n[11,] 0.0089498455 0.0000000000\n[12,] 0.0391250811 0.0000000000\n[13,] 0.1250082431 0.0000000000\n[14,] 0.2659535203 0.0001492682\n[15,] 0.4675827148 0.0245645535\n[16,] 0.5868492773 0.1621610484\n[17,] 0.4743685242 0.4494369547\n[18,] 0.0000000000 1.0000000000\nattr(,\"degree\")\n[1] 3\nattr(,\"knots\")\n     25%      50%      75% \n2.295205 4.965753 6.997945 \nattr(,\"Boundary.knots\")\n[1] 0.000000 9.257534\nattr(,\"intercept\")\n[1] TRUE\nattr(,\"class\")\n[1] \"bs\"     \"basis\"  \"matrix\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## B-spline regression {.smaller}\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <- list(\n  n = nrow(dat_pat),\n  p = ncol(B),\n  Y = dat_poly$Y,\n  X = B\n)\nfit_bspline <- sampling(compile_model, data = stan_data)\n```\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-20-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Model comparison\n\n\n\n\n\n\n::: {.cell layout-nrow=\"2\" layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-21-2.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-21-3.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-21-4.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n\n\n\n## What is the point?\n\n-   Choice of model is highly dependent on the context.\n\n-   As we learned in the model comparison lecture, a better fit to the sample might not actually be a better model.\n\n-   These basis models are difficult to interpret and are not particularly useful for a clinical setting (they may be useful for prediction!).\n\n## Change point motivation\n\n::::: columns\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n-   Progression is defined by slow (or stable) deterioration, followed by a rapid decrease.\n\n-   Flexible modeling of MD across time.\n\n-   Biological representation of progression through the change point.\n\n-   Change points are a framework for inherently parameterizing progression.\n:::\n:::::\n\n## Writing down a model\n\n-   Model for the observed data:\n\n$$Y_i = \\mu(X_i) + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,\\sigma^2).$$\n\n. . .\n\n-   Model for the mean process:\n\n$$\\mu(X_i) =\\left\\{ \\begin{array}{ll}\n        {\\beta}_0 + \\beta_1 X_i & \\text{ } \\mbox{$X_i \\leq \\theta$},\\\\\n        {\\beta}_0 + \\beta_1 \\theta + {\\beta}_2(X_i - \\theta)& \\text{ } \\mbox{$X_i > \\theta.$}\\end{array} \\right.$$\n\n-   $\\theta \\in (\\min X_i, \\max X_i)$ represents a change point.\n\n## Change point model in Stan\n\n\n\n\n\n\n::: {.cell output.var='cp'}\n\n```{.stan .cell-code}\n// saved in change_points.stan\nfunctions {\n  vector compute_mean(vector X, real beta0, real beta1, real beta2, real theta) {\n    int n = size(X);\n    vector[n] mu;\n    for (t in 1:n) {\n      if (X[t] <= theta) mu[t] = beta0 + beta1 * X[t];\n      if (X[t] > theta) mu[t] = beta0 + beta1 * theta + beta2 * (X[t] - theta);\n  }\n  return mu;\n  }\n}\ndata {\n  int<lower=1> n;\n  vector[n] Y;\n  vector[n] X;\n  int n_pred;\n  vector[n_pred] X_pred;\n}\ntransformed data {\n  real min_X = min(X);\n  real max_X = max(X);\n}\nparameters {\n  real beta0;\n  real beta1;\n  real beta2;\n  real<lower = 0> sigma;\n  real<lower = min_X, upper = max_X> theta;\n}\nmodel {\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(sigma | 0, 1);\n  target += normal_lpdf(beta0 | 0, 1);\n  target += normal_lpdf(beta1 | 0, 1);\n  target += normal_lpdf(beta2 | 0, 1);\n}\ngenerated quantities {\n  vector[n_pred] mu_pred = compute_mean(X_pred, beta0, beta1, beta2, theta);\n  array[n_pred] real Y_pred_out = normal_rng(mu_pred, sigma);\n  vector[n] mu = compute_mean(X, beta0, beta1, beta2, theta);\n  array[n] real Y_pred_in = normal_rng(mu, sigma);\n}\n```\n:::\n\n\n\n\n\n\n## Change point regression {.smaller}\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_model <- stan_model(file = \"change_points.stan\")\nn_pred <- 1000\nstan_data <- list(Y = dat_pat$Y, \n                  X = dat_pat$X,\n                  n = nrow(dat_pat),\n                  n_pred = n_pred,\n                  X_pred = seq(0, max(dat_pat$X) + 2, length.out = n_pred))\nfit_cp <- sampling(stan_model, data = stan_data)\nprint(fit_cp, probs = c(0.025, 0.5, 0.0975))\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n       mean se_mean   sd   25%   50%   75% n_eff Rhat\nbeta0 -1.82    0.02 0.66 -2.29 -1.85 -1.40   749 1.00\nbeta1 -0.05    0.01 0.29 -0.19 -0.04  0.10   560 1.01\nbeta2 -0.84    0.02 0.45 -1.12 -0.87 -0.58   889 1.00\nsigma  1.27    0.01 0.26  1.08  1.23  1.41   792 1.00\ntheta  5.36    0.07 1.79  4.69  5.57  6.34   749 1.00\n\nSamples were drawn using NUTS(diag_e) at Wed Jan  1 17:29:30 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Diagnostics\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-27-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Posterior fit\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nâ„¹ Please use `linewidth` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Posterior fit\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-29-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Prediction\n\n\n\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_vline()`).\nRemoved 1 row containing missing values or values outside the scale range\n(`geom_vline()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09-nonlinear_files/figure-html/unnamed-chunk-30-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Prepare for next class\n\n-   Work on [HW 02](https://biostat725-sp25.netlify.app/hw/hw-02).\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Robust regression\n",
    "supporting": [
      "09-nonlinear_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}