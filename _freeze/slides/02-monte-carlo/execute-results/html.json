{
  "hash": "faf0d7ae032e7dcf1a95375ae5862fdb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Monte Carlo Sampling\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-01-14\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Agenda\n\n-   Monte Carlo (MC) sampling\n\n-   Bayesian inference using MC sampling\n\n# Monte Carlo Sampling\n\n## Simulating $\\pi$ using Monte Carlo\n\nSuppose we are interested in estimating $\\pi$.\n\n::::: columns\n::: {.column width=\"60%\"}\n-   We can formulate $\\pi$ as a function of the area of a square and circle.\n\n-   Area of a circle: $A_c = \\pi r^2$\n\n-   Area of a square: $A_s = 4 r^2$\n\n-   The ratio of the two areas is: $\\frac{A_c}{A_s} = \\frac{\\pi r^2}{4 r^2} \\implies \\pi = \\frac{4 A_c}{A_s}$\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n:::\n:::::\n\n- If we have an estimate for the ratio we can solve for $\\pi$. The challenge becomes estimating this ratio.\n\n## Monte Carlo sampling\n\n-   We can take advantage of how quickly a computer can generate pseudo-random numbers.\n\n-   There is a class of algorithms called Monte Carlo sampling that exploit randomness to estimate real world scenarios that would otherwise be difficult to explicitly calculate.\n\n-   The name comes from the Monte Carlo Casino in Monaco, where the primary developer of the method, mathematician Stanislaw Ulam, was inspired by his uncle's gambling habits. (This is who Stan was named after!)\n\n## Monte Carlo estimation of the ratio\n\n-   We can use a Monte Carlo simulation to estimate the area ratio of the circle to the square.\n\n-   Imagine you randomly drop grains of sand into the area of the square. By counting the total number of sand grains in the square (all of them since youâ€™re an accurate dropper) to the number of sand grains inside the circle we get this estimate.\n\n-   Multiply the estimated ratio by 4 and you get an estimate for $\\pi$.\n\n-   The more grains of sand that are used the more accurate your estimate of $\\pi$.\n\n## Algorithm for estimating $\\pi$\n\n1.  Generate a random point $(x, y)$ inside a square centered at the origin with length 2.\n\n::: question\nThis is equivalent to assuming:\n\n-   $f_{X,Y}(x,y) = f_X(x)f_Y(y)$\n\n-   $f_X(x) = Uniform(-1, 1)$, $f_Y(y) = Uniform(-1, 1)$\n:::\n\n. . .\n\n2.  Determine whether the point falls inside the unit circle inscribed in the square by checking whether $x^2 + y^2 \\leq 1$.\n\n. . .\n\n3.  Repeat steps 1 and 2 for a large number of points ($S$).\n\n## Algorithm for estimating $\\pi$\n\n1.  Generate a random point $(x, y)$ inside a square centered at the origin with length 2.\n\n2.  Determine whether the point falls inside the unit circle inscribed in the square by checking whether $x^2 + y^2 \\leq 1$.\n\n3.  Repeat steps 1 and 2 for a large number of points ($S$).\n\n4.  Calculate the ratio of the number of points that fell inside the circle to the total number of points generated.\n\n. . .\n\n5.  Multiply the ratio by 4 to estimate the value of $\\pi$.\n\n## How big of a difference does $S$ make\n\n\n\n::: {.cell layout-ncol=\"3\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=240}\n:::\n\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-3-2.png){fig-align='center' width=240}\n:::\n\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-3-3.png){fig-align='center' width=240}\n:::\n:::\n\n\n\n## Estimating $\\pi$ with increasing $S$\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-4-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Error in estimating $\\pi$\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Intuition behing Monte Carlo sampling\n\n-   Assume $X_i \\sim Uniform(-1,1)$ and $Y_i \\sim Uniform(-1,1)$ for $i = 1,\\ldots,S$.\n\n-   We can write our problem as, $\\pi = 4P(X^2 + Y^2 \\leq 1)$.\n\n-   How could we do this without Monte Carlo?\n\n-   Define, $Z = X^2 + Y^2$. We could then use change-of-variables to compute the density of $Z$ and then compute $P(Z \\leq 1)$.\n\n    -   This is generally difficult!\n\n## Intuition behing Monte Carlo sampling {.midi}\n\n-   Instead, we could write our problem as an expectation and use the law of large numbers,\n\n$$P(X^2 + Y^2 \\leq 1) = \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right].$$\n\n-   Recall that an expectation of an indicator is the probability of the event that it indicates, $\\mathbb{E}_X[1(A)] = \\int_A f_X(x)dx = P(X \\in A)$.\n\n-   We then have that, $$\\frac{1}{S}\\sum_{i = 1}^S 1\\left(X_i^2 + Y_i^2 \\leq 1\\right) \\rightarrow \\mathbb{E}_{X,Y}\\left[1\\left(X^2 + Y^2 \\leq 1\\right)\\right],$$\n\nwhere $(X_i,Y_i) \\sim f(X_i,Y_i)$.\n\n# Bayesian inference using MC sampling\n\n-   Computing a Bayesian posterior\n\n-   Posterior inference using MC sampling\n\n## A motivating example\n\n-   Suppose we are interested in estimating the prevalence of diabetes in Durham County. We aim to estimate this prevalence by taking a sample of $n$ individuals in Durham County and we record whether or not they have diabetes, $Y_i$.\n\n-   We assume that $Z = \\sum_{i=1}^n Y_i \\sim Binomial(n, \\pi)$ for $i = 1,\\ldots,n$.\n\n-   Our goal is to estimate $\\pi$ and perform statistical inference (e.g., point estimation, interval estimation, etc.).\n\n## Posterior inference\n\n-   In Bayesian statistics, inference is encoded through the posterior distribution,\n\n\\begin{align*}\nf(\\pi | Z) &= \\frac{f(Z | \\pi)f(\\pi)}{f(Z)},\\\\\n&= \\frac{f(Z | \\pi)f(\\pi)}{\\int f(Z | \\pi)f(\\pi)d\\pi}.\n\\end{align*}\n\n-   All we have to do is specify the likelihood and prior.\n\n## Likelihood specification\n\n$Z$ is a Binomial distribution with pmf,\n\n$$f(Z | \\pi) = P(Z = z) = {n \\choose z} \\pi^z(1-\\pi)^{n-z},$$\n\nwhere $z \\in \\{0, 1, \\ldots, n\\}$.\n\n-   ${n \\choose z} = \\frac{n!}{z!(n-z)!} = \\frac{\\Gamma(n+1)}{\\Gamma(z+1)\\Gamma(n-z+1)}$\n\n-   $\\Gamma(x) = (x-1)!$\n\n## Prior specification\n\nWhat do we know about $\\pi$?\n\n-   $\\pi$ is continuous.\n\n-   $\\pi \\in (0,1)$.\n\nWe should place a distribution on $\\pi$ that permits these properties.\n\nOne option is the Beta distribution, $$f(\\pi) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}.$$\n\nIf we assume that $\\pi \\sim Beta(\\alpha,\\beta)$, we can proceed to compute the posterior.\n\n## Computing the marginal likelihood {.midi}\n\nUnder our prior specification, we can compute the marginal likelihood, $f(Z)$:\n\n\\begin{align*}\nf(Z) &= \\int f(Z | \\pi) f(\\pi) d\\pi\\\\\n&= \\int {n \\choose z} \\pi^z(1-\\pi)^{n-z} \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\underbrace{\\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1}}_{Beta\\text{ }kernel} d\\pi\n\\end{align*}\n\n::: callout-note\n## Definition\n\nThe part of the pdf/pmf that depends on the variable is called the **kernel**.\n\nExample:\n\n-   The kernel of the Beta pdf is $\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}$\n:::\n\n## Computing the marginal likelihood {.midi}\n\nUnder our prior specification, we can compute the marginal likelihood, $f(Z)$:\n\n\\begin{align*}\nf(Z) &= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\int \\pi^{(\\alpha + z) - 1}(1-\\pi)^{(\\beta + n - z) - 1} d\\pi\\\\\n&= {n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta + n)}{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}.\n\\end{align*}\n\n## We can then compute the posterior {.midi}\n\n\\begin{align*}\nf(\\pi | Z) &= \\frac{f(Z | \\pi) f(\\pi)}{f(Z)}\\\\\n&= \\frac{{n \\choose z} \\pi^z(1-\\pi)^{n-z} \\times \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\pi^{\\alpha-1}(1-\\pi)^{\\beta-1}}{{n \\choose z}\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)} \\frac{\\Gamma(\\alpha+\\beta) }{\\Gamma(\\alpha + z)\\Gamma(\\beta + n - z)}}\\\\\n&=\\frac{\\Gamma(\\alpha+z)\\Gamma(\\beta+n-z)}{\\Gamma(\\alpha + \\beta + n)}\\pi^{(\\alpha + z) - 1} (1 - \\pi)^{(\\beta + n - z) - 1}\\\\\n&=Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{align*}\n\n-   A prior that is considered **conjugate** yields a posterior with the same distribution.\n\n-   The Beta distribution is conjugate for the Bernoulli/Binomial distributions.\n\n## Computing the posterior\n\n-   In general, computing the marginal likelihood, $f(Z)$, is extremely difficulty.\n\n-   An easier approach is to use the kernel trick.\n\n\\begin{align*}\nf(\\pi | Z) &\\propto f(Z | \\pi) f(\\pi)\\\\\n&\\propto \\pi^z (1-\\pi)^{n - z} \\pi^{\\alpha-1}(1-\\pi)^{\\beta-1} \\\\\n&= \\pi^{\\left(\\alpha + z\\right) - 1} (1-\\pi)^{\\left(\\beta + n - z\\right) - 1}\\\\\n&= Beta\\left(\\alpha + z, \\beta + n - z\\right).\n\\end{align*}\n\n-   This only works when a conjugate prior is used.\n\n## Let's inspect the posterior\n\n-   Suppose we conducted a simple random sample of 500 individuals in Durham County and 120 responsed that they had diabetes and 380 that they did not.\n\n-   The posterior becomes, $Beta\\left(\\alpha + 120, \\beta + 380\\right)$.\n\n-   We must choose our prior distribution wisely.\n\n-   Note that:\n\n    -   $\\mathbb{E}[\\pi] = \\alpha/(\\alpha + \\beta)$\n\n    -   $\\mathbb{V}(\\pi) = (\\alpha\\beta)/[(\\alpha + \\beta)^2(\\alpha + \\beta + 1)]$.\n\n-   Typically, $\\alpha = \\beta = 1$, which corresponds to a uniform prior on $\\pi$.\n\n## Let's inspect the posterior\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Suppose my prior changes\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-7-1.png){fig-align='center' width=480}\n:::\n\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-7-2.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Summarizing the posterior\n\n1.  Posterior means, medians, modes, and variances\n\n2.  Joint, conditional, and marginal probabilities, for example: $P(\\pi < c | \\mathbf{Y})$\n\n3.  $\\alpha$-quantiles: $\\{q_{\\alpha} : P(\\pi < q_{\\alpha} | \\mathbf{Y}) = \\alpha\\}, \\alpha \\in (0,1)$\n\n4.  $\\ldots$\n\nSummarizing a posterior (with known parametric form) is straightforward for basic quantities of interest.\n\n## Summarization can be complex\n\n-   Posteriors are often not available in closed form.\n\n-   Even when we have a closed form posterior, it can be difficult to compute summaries of interest.\n\n-   For example, consider $P(\\pi < c |\\mathbf{Y})$. What are our options to calculate this probability?\n\n    -   Direct integration (by hand)\n\n    -   Numerical integration/software packages (e.g., $\\texttt{pbeta}$)\n\nThese methods work well for standard posterior quantities and distributions.\n\n## Summarizing the posterior\n\nHowever, sometimes we will want to summarize other aspects of a posterior distribution.\n\n-   $P(\\pi \\in A|\\mathbf{Y})$ for some arbitrary set $A$\n\n-   Means and standard deviations of some function of $\\pi$, $g\\left(\\pi\\right)$\n\n-   The posterior distribution of functions of many parameters:\n\n    -   $|\\pi_1 - \\pi_2|$, $\\pi_1/\\pi_2$, $\\max\\left\\{\\pi_1,\\ldots,\\pi_p\\right\\}$, $\\dots$\n\n-   Obtaining exact values for these posterior quantities can be difficult or even impossible.\n\nWhat are our options?\n\n# Monte Carlo sampling\n\n## Monte Carlo (MC) sampling\n\n-   Integration method based on random sampling\n\n-   The general principles and procedures remain relatively constant across a broad class of problems\n\n-   Suppose we have $S$ iid samples from our posterior distribution: $\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right)$\n\n-   Then the empirical distribution of $\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}$ would approximate $f\\left(\\pi|\\mathbf{Y}\\right)$, with the approximation improving as $S$ increases\n\n-   The empirical distribution of $\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}$ is known as a MC approximation to $f\\left(\\pi|\\mathbf{Y}\\right)$\n\n## MC approximation\n\nLet $g\\left(\\pi\\right)$ be (just about) any function of $\\pi$. The law of large numbers says that if,\n\n$$\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\stackrel{\\text{iid}}{\\sim}f\\left(\\pi|\\mathbf{Y}\\right),$$ then, $$\\frac{1}{S}\\sum_{s=1}^S g\\left(\\pi^{\\left(s\\right)}\\right)\\rightarrow \\mathbb{E}\\left[g\\left(\\pi\\right)|\\mathbf{Y}\\right]=\\int g\\left(\\pi\\right)f\\left(\\pi|\\mathbf{Y}\\right)d\\pi,$$ as $S\\rightarrow \\infty$.\n\n## MC approximation\n\n-   Implications (as $S\\rightarrow \\infty$):\n\n    -   $\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)} \\rightarrow \\mathbb{E}\\left[\\pi|\\mathbf{Y}\\right]$\n\n    -   $\\frac{1}{S-1}\\sum_{s=1}^S \\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2 \\rightarrow \\mathbb{V}\\left(\\pi|\\mathbf{Y}\\right)$\n\n    -   $\\frac{1}{S}\\sum_{s=1}^S 1\\left(\\pi^{\\left(s\\right)}\\leq \\mathbf{c}\\right) \\rightarrow P\\left(\\pi\\leq \\mathbf{c} | \\mathbf{Y}\\right)$\n\n    -   $\\alpha$-quantile of $\\left\\{\\pi^{\\left(1\\right)},\\ldots,\\pi^{\\left(S\\right)}\\right\\}\\rightarrow q_{\\alpha}$\n\n-   Just about any aspect of the posterior distribution can be approximated arbitrarily exactly with a large enough MC sample\n\n## Posterior inference for arbitrary functions\n\nInterest in the posterior distribution of a function of $\\pi$, $g\\left(\\pi\\right)$\n\n-   MC sampling plan:\n\n    -   Draw $\\pi^{\\left(1\\right)}$ from $f\\left(\\pi|\\mathbf{Y}\\right)$, calculate $g\\left(\\pi^{\\left(1\\right)}\\right)$\n\n        $\\ldots$\n\n    -   Draw $\\pi^{\\left(S\\right)}$ from $f\\left(\\pi|\\mathbf{Y}\\right)$, calculate $g\\left(\\pi^{\\left(S\\right)}\\right)$\n\n    $\\Rightarrow g\\left(\\pi^{\\left(1\\right)}\\right),\\ldots,g\\left(\\pi^{\\left(S\\right)}\\right)\\stackrel{\\text{iid}}{\\sim}f\\left(g\\left(\\pi\\right)|\\mathbf{Y}\\right)$\n\n-   Therefore, similar quantities can be estimated (posterior mean, variance, quantiles, distribution, etc.)\n\n## How many samples to take? {.small}\n\nWe can use a central limit theorem: $\\sqrt{S}\\left(\\overline{\\pi}-\\mathbb{E}[\\pi | \\mathbf{Y}]\\right)/\\sigma \\stackrel{d}{\\rightarrow} \\text{N}\\left(0,1\\right)$,\n\n-   $\\overline{\\pi}=\\frac{1}{S}\\sum_{s=1}^S \\pi^{\\left(s\\right)}$\n\n-   $\\sigma^2 = \\mathbb{V}\\left(\\overline{\\pi}\\right) = \\frac{1}{S^2}\\sum_{s=1}^S\\mathbb{V}\\left(\\pi^{(s)}\\right) = \\frac{1}{S}\\mathbb{V}\\left(\\pi | \\mathbf{Y}\\right)$.\n\n$\\implies \\overline{\\pi}\\approx N\\left(\\mathbb{E}[\\pi | \\mathbf{Y}],\\sigma^2/S\\right)$\n\nMC standard error: $\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}$, $\\widehat{\\sigma}^2=\\frac{1}{S-1}\\sum_{s=1}^S\\left(\\pi^{\\left(s\\right)}-\\overline{\\pi}\\right)^2$\n\nApproximate 95% MC confidence interval for the posterior mean: $\\overline{\\pi} \\pm 2\\sqrt{\\frac{\\widehat{\\sigma}^2}{S}}$\n\n-   Choose $S$ large enough to report the posterior mean with your desired precision\n\n-   Reporting MC standard errors for the posterior mean is a good way to indicate that $S$ is large enough\n\n## Returning to our posterior\n\nLet's obtain $S = 1,000$ samples from our posterior.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npi_samples <- rbeta(1000, 1 + 120, 1 + 380)\n```\n:::\n\n\n\nWe can compute the posterior mean and variance.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(mean(pi_samples))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2412684\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(var(pi_samples))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0003574553\n```\n\n\n:::\n:::\n\n\n\n## Assessing accuracy\n\n\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](02-monte-carlo_files/figure-revealjs/unnamed-chunk-10-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Additional posterior summaries\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# median\nmedian(pi_samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2408114\n```\n\n\n:::\n\n```{.r .cell-code}\n# 95% credible intervals\nquantile(pi_samples, probs = c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     2.5%     97.5% \n0.2053340 0.2786704 \n```\n\n\n:::\n\n```{.r .cell-code}\n# evaluating probability\nmean(pi_samples < 0.25)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.687\n```\n\n\n:::\n\n```{.r .cell-code}\n# summarizing arbitrary functions of the parameters\npi_new <- pi_samples^3 - pi_samples\nc(mean(pi_new), quantile(pi_new, probs = c(0.025, 0.975)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                 2.5%      97.5% \n-0.2267887 -0.2570297 -0.1966767 \n```\n\n\n:::\n:::\n\n\n\n# Additional examples of MC sampling\n\n## Poisson random variable\n\nSuppose that $Y_i \\stackrel{iid}{\\sim} Poisson(\\lambda)$ for $i = 1,\\ldots,n$. Assume the following conjugate prior, $f(\\lambda) \\sim Gamma(a, b)$, so that $f(\\lambda|\\mathbf{Y}) \\sim Gamma(a + \\sum_{i=1}^nY_i,b+n)$. We would like to perform inference for the $\\lambda$. We take $S=10,000$ samples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(54) # set seed for replicability\nS <- 10000 # number of Monte Carlo samples\nlambda <- 4 # true value of lambda\nn <- 100 # sample size\nY <- rpois(n, lambda)\na <- 3 # hyperprior\nb <- 1 # hyperprior\nsamples <- rgamma(S, a + sum(Y), b + n) # sample from posterior\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|   min|    q1| median|    q3|   max|  mean|    sd|\n|-----:|-----:|------:|-----:|-----:|-----:|-----:|\n| 3.269| 3.904|  4.039| 4.173| 4.773| 4.041| 0.201|\n\n\n:::\n:::\n\n\n\n<!-- ## Binomial random variable -->\n\n<!-- Suppose that $Y_i \\stackrel{iid}{\\sim} Bernoulli(\\pi)$ for $i = 1,\\ldots,n$. Assume the following conjugate prior, $f(\\pi) \\sim Beta(a, b)$, so that $f(\\pi|\\mathbf{Y}) \\sim Beta(a + \\sum_{i=1}^nY_i,b+n - \\sum_{i=1}^n)$. We would like to perform inference for the $\\pi$. We take $S=10,000$ samples. -->\n\n<!-- ```{r} -->\n\n<!-- set.seed(54) # set seed for replicability -->\n\n<!-- S <- 10000 # number of Monte Carlo samples -->\n\n<!-- pi <- 0.35 # true value of lambda -->\n\n<!-- n <- 1000 # sample size -->\n\n<!-- Y <- rbinom(n, size = 1, prob = pi) -->\n\n<!-- a <- 1 # hyperprior -->\n\n<!-- b <- 1 # hyperprior -->\n\n<!-- samples <- rbeta(S, a + sum(Y), b + n - sum(Y)) # sample from posterior -->\n\n<!-- ``` -->\n\n<!-- ```{r, echo=FALSE} -->\n\n<!-- samples <- data.frame(pi = samples) -->\n\n<!-- samples |> -->\n\n<!--   summarise(min = min(pi), q1 = quantile(pi, 0.25),  -->\n\n<!--             median = median(pi), q3 = quantile(pi, 0.75),  -->\n\n<!--             max = max(pi), mean = mean(pi), sd = sd(pi)) |> -->\n\n<!--   kable(digits = 3) -->\n\n<!-- ``` -->\n\n## Sampling for any distribution\n\nMonte Carlo sampling does not have to be used solely for posterior inference. Suppose we are interested in computed summaries for $X_i \\stackrel{iid}{\\sim} N(3, 4)$ for $i = 1,\\ldots,n$. We take $S = 10,000$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nS <- 10000\nsamples <- rnorm(S, 3, 2)\nlibrary(moments)\nkurtosis(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.041546\n```\n\n\n:::\n\n```{.r .cell-code}\nskewness(samples)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.0109433\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(samples > 4) # P(X_i > 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.3087\n```\n\n\n:::\n:::\n\n\n\n## Combination of random variables\n\nSuppose $X_i \\stackrel{iid}{\\sim} N(3, 4)$ and $Y_i \\stackrel{iid}{\\sim} \\chi^2(df=3)$ for $i = 1,\\ldots,n$. $X_i$ and $Y_i$ are independent. We are interested in summaries of $Z_i = X_i / Y_i$. We take $S = 10,000$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nS <- 10000\nx <- rnorm(S, 3, 2)\ny <- rchisq(S, 3)\nz <- x / y\nmean(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.961057\n```\n\n\n:::\n\n```{.r .cell-code}\nmedian(z)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.146081\n```\n\n\n:::\n\n```{.r .cell-code}\nmean(z > 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5456\n```\n\n\n:::\n:::\n\n\n\n## Prepare for next class\n\n-   Complete [HW 00](https://biostat725-sp25.netlify.app/hw/hw-00) which is due before Thursday's class\n\n-   Complete reading to prepare for Thursday's lecture\n\n-   Thursday's lecture: Markov chain Monte Carlo\n",
    "supporting": [
      "02-monte-carlo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}