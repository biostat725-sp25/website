{
  "hash": "5d204df2b64ad11c2f589a9949310136",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Missing Data\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-25\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   Last, we learned about classification for binary and multiclass problems.\n\n## Missing data in research\n\n- In any real-world dataset, missing values are nearly always going to be present.\n\n  - Missing data can be totally innocuous or a source of bias.\n\n- Cannot determine which because there are no values to inspect.\n\n- Handling missing data is extremely difficult!\n\n## Example\n\n- Suppose an individual's depression scores are missing in dataset of patients with colon cancer.\n\n- It could be missing because:\n\n  -   A data entry error where some values did not make it into the dataset.\n\n  -   The patient is a man, and men are less likely to complete the depression score in general (i.e., it is not related to the unobserved depression).\n\n  -   The patient has depression and as a result did not complete the depression survey. \n\n## Classifactions of missing data\n\n- Missing completely at random (MCAR)\n\n  - This is the ideal case but rarely seen in practice. Usually a data entry problem.\n\n- Missing at random (MAR)\n\n  - The missing value is related to some other variable that has been collected.\n\n- Missing not at random (MNAR)\n\n  - The missing value is related to a variable that was\nnot collected or not observed.\n\n## Missing data\n\n- Missing data can appear in the outcome and/or predictors.\n\n- Today, we will write down some math for missing data occuring in the outcome space, however this is generalizable to missingness in the predictor space.\n\n## Missing data framework {.midi}\n\n- We are interested in modeling a random variable $Y_{i}$, for $i \\in \\{1,\\ldots,n\\}$.\n\n- In a missing data setting, we only observe the outcome in subset of observations, $\\mathbf{Y}_{obs} = \\{Y_{i}:i \\in \\mathcal N_{obs}\\}$. \n\n  - $\\mathcal N_{obs}$ is the set of indeces in the observed set, such that $|\\mathcal N_{obs}|= n_{obs}$ is the number of observed data points.\n\n- The remaining observations are assumed to be missing and are contained in $\\mathbf{Y}_{mis} = \\{Y_{i}:i \\in \\mathcal N_{mis}\\}$. \n\n  - $\\mathcal N_{mis}$ is the set of indeces of the missing data and $|\\mathcal N_{mis}|= n_{mis}$ is the number of missing data points. \n\n- The full set of data is given by $\\mathbf{Y}=(\\mathbf{Y}_{obs},\\mathbf{Y}_{mis})$.\n\n## Missing data notation\n\n- Define $O_{i}$ as a binary indicator of observation $Y_{i}$ being present, where $O_{i} = 1$ indicates that $Y_{i}$ was observed. \n\n- The collection of missingness indicators is given by $\\mathbf{O} = \\{O_{i}:i = 1,\\ldots,n\\}$. \n\n- Our observed data then consists of $(\\mathbf{Y}_{obs}, \\mathbf{O})$.\n\n## Complete data likelihood\n\n- The joint distribution of $(\\mathbf{Y}, \\mathbf{O})$ can be written as,\n\n$$f(\\mathbf{Y}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\underbrace{f(\\mathbf{Y} | \\mathbf{X}, \\boldsymbol{\\theta})}_{\\text{likelihood}} \\times  \\underbrace{f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X}, \\boldsymbol{\\phi})}_{\\text{missing model}}.$$\n\n  - The parameter block, $(\\boldsymbol{\\theta},\\boldsymbol{\\phi})$, consists of: \n  \n    - $\\boldsymbol{\\theta}$, the **target parameters** of interest (e.g., feature effects on outcome), and \n    \n    - $\\boldsymbol{\\phi}$, the **nuisance parameters**.\n    \n. . .\n\n**Can we perform inference using this likelihood?**\n\n## Observed data likelihood \n\n- The likelihood for the observed data must be written by marginalizing over the unobserved outcome variables.\n\n\\begin{align*}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y}_{obs}, \\mathbf{Y}_{mis}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int f(\\mathbf{Y}, \\mathbf{O} |\\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int \\underbrace{f(\\mathbf{Y} |\\mathbf{X}, \\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{X},\\mathbf{Y}, \\boldsymbol{\\phi})}_{\\text{complete data likelihood}} d\\mathbf{Y}_{mis}\n\\end{align*}\n\n## Missing data models: MCAR\n\nThe data are missing completely at random (MCAR) if the missing mechanism is defined as,\n\\begin{align*}\nf(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi}) &= f(\\mathbf{O} | \\mathbf{Y}_{obs}, \\mathbf{Y}_{mis},\\mathbf{X},\\boldsymbol{\\phi})\\\\\n&= f(\\mathbf{O} | \\boldsymbol{\\phi}).\n\\end{align*}\n\n- The missingness does not depend on any data.\n\n## Implications of the missing model: MCAR {.midi}\n\n\\begin{align*}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} |\\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{O} | \\boldsymbol{\\phi}) \\int f(\\mathbf{Y}_{obs} |\\mathbf{X}_{obs}, \\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} |\\mathbf{X}_{mis}, \\boldsymbol{\\theta})d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta})f(\\mathbf{O} | \\boldsymbol{\\phi})\\int  f(\\mathbf{Y}_{mis} |\\mathbf{X}_{mis}, \\boldsymbol{\\theta}) d\\mathbf{Y}_{mis}\\\\\n&=  f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta})f(\\mathbf{O} | \\boldsymbol{\\phi}).\n\\end{align*}\n\n## Key points about MCAR assumption\n\n- **No bias:** The analysis based on the observed data will not be biased, as the missingness does not systematically favor any particular pattern in the data. \n\n- **Reduced power:** While unbiased, MCAR still reduces the statistical power of the analysis due to the smaller sample size resulting from missing data.\n  \n- **Simple handling methods:** Because of its random nature, MCAR allows for straightforward handling methods like ***listwise deletion*** (i.e., complete-case analysis) or simple imputation techniques (e.g., mean imputation) without introducing bias. \n\n## Missing data models: MAR\n\nThe data are missing at random (MAR) if the missing mechanism is defined as,\n\\begin{align*}\nf(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi}) &= f(\\mathbf{O} | \\mathbf{Y}_{obs}, \\mathbf{Y}_{mis},\\mathbf{X},\\boldsymbol{\\phi})\\\\\n&= f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{X},\\boldsymbol{\\phi}).\n\\end{align*}\n\n- The missingness depends on the observed data only.\n\n## Implications of the missing model: MAR {.midi}\n\n\\begin{align*}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} |\\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} |\\mathbf{X}, \\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{X}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}=  f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{X},\\boldsymbol{\\phi}) \\int f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta})d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}=  f(\\mathbf{Y}_{obs} |\\mathbf{X}_{obs}, \\boldsymbol{\\theta})f(\\mathbf{O} |\\mathbf{Y}_{obs},\\mathbf{X}, \\boldsymbol{\\phi})\\int  f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}=  f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta})f(\\mathbf{O} |\\mathbf{Y}_{obs}, \\mathbf{X},\\boldsymbol{\\phi}).\n\\end{align*}\n\n- **Unbiased Parameter Estimates:** Similar to MCAR, we can perform a complete case analysis and can ignore the missing data model! This is never done, however, because it leads to incorrect inference.\n\n## Key points about MAR assumption\n\n- **Complete-case analysis is not acceptable: **\n\n  - Parameter estimation remains unbiased, but, in general, estimation of variances and intervals is biased.\n  \n  - Also, smaller sample size leads to less power and worse prediction.\n  \n  - Under certain missingness settings, parameter estimation may not be unbiased. \n\n- **Simple handling approaches fail:** Methods like mean imputation will also result in small estimated standard errors.\n\n- **More advanced methods are needed:** Multiple imputation, Bayes.\n\n## Missing data models: MNAR\n\nThe data are missing not at random (MNAR) if the missing mechanism is defined as,\n\\begin{align*}\nf(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi}) &= f(\\mathbf{O} | \\mathbf{Y}_{obs}, \\mathbf{Y}_{mis},\\mathbf{X},\\boldsymbol{\\phi}).\n\\end{align*}\n\n- The missingness depends on the observed and missing data.\n\n## Implications of the missing model: MNAR {.midi}\n\n\\begin{align*}\nf(\\mathbf{Y}_{obs}, \\mathbf{O} | \\mathbf{X},\\boldsymbol{\\theta},\\boldsymbol{\\phi}) &= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X}, \\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= \\int f(\\mathbf{Y} | \\mathbf{X},\\boldsymbol{\\theta}) f(\\mathbf{O} | \\mathbf{Y}_{obs},\\mathbf{Y}_{mis}, \\mathbf{X},\\boldsymbol{\\phi}) d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= \\int f(\\mathbf{Y}_{obs} |\\mathbf{X}_{obs}, \\boldsymbol{\\theta}) f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi})d\\mathbf{Y}_{mis}\\\\\n&\\hspace{-2in}= f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs},\\boldsymbol{\\theta}) \\int f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis},\\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y},\\mathbf{X},\\boldsymbol{\\phi})d\\mathbf{Y}_{mis}\n\\end{align*}\n\n- Under the MNAR assumption, we are NOT allowed to ignore the missing data. We must specify a model for the missing data.\n\n- This is really hard! We can ignore this in our class.\n\n## Summary of missing mechanisms\n\n- Under MCAR and MAR, we are allowed to fit our model to the observed data (i.e., a complete case analysis/listwise deletion). Under these settings the missingness is considered **ignorable**.\n\n- Under MAR, fitting the complete case analysis is not efficient and advanced techniques are needed to guarentee proper statistical inference.\n\n- Under MNAR, we must model the missing data mechanism. This data is considered **non-ignorable**.\n\n# Bayesian approches to missing data\n\n-   Full Bayesian joint model\n-   Multiple imputation\n\n## Let's motivate with some data\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-ncol=\"1\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-missing_files/figure-revealjs/unnamed-chunk-3-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Simulate missing data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(54)\nn <- nrow(fulldata)\nexpit <- function(x) exp(x) / (1 + exp(x))\nfulldata$o <- rbinom(n, 1, expit(1.5 * scale(fulldata$x1) - 3 * fulldata$x2))\nhead(fulldata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     y    x1 x2  Sex o\n1 65.6 174.0  1 Male 1\n2 71.8 175.3  1 Male 0\n3 80.7 193.5  1 Male 0\n4 72.6 186.5  1 Male 0\n5 78.8 187.2  1 Male 0\n6 74.8 181.5  1 Male 0\n```\n\n\n:::\n:::\n\n\n\n## Visualize data\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-missing_files/figure-revealjs/unnamed-chunk-5-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](14-missing_files/figure-revealjs/unnamed-chunk-5-2.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Model fits\n\n\n\n::: {.cell layout-ncol=\"2\" layout-align=\"center\"}\n::: {.cell-output-display}\n![](14-missing_files/figure-revealjs/unnamed-chunk-6-1.png){fig-align='center' width=384}\n:::\n\n::: {.cell-output-display}\n![](14-missing_files/figure-revealjs/unnamed-chunk-6-2.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n## Full Bayesian model\n\n- Since we are Bayesians, we can treat the unobserved $\\mathbf{Y}_{mis}$ as parameters and work with the complete data likelihood.\n\n\\begin{align*}\nf(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}, \\mathbf{Y}_{mis} | \\mathbf{Y}_{obs},\\mathbf{O},\\mathbf{X}) &\\propto f(\\mathbf{Y}, \\mathbf{O}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\mathbf{X})\\\\\n&\\hspace{-4in}=f(\\mathbf{Y}, \\mathbf{O} | \\mathbf{X}, \\boldsymbol{\\theta},\\boldsymbol{\\phi})f(\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\\\\n&\\hspace{-4in}= f(\\mathbf{Y} | \\mathbf{X}, \\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X}, \\boldsymbol{\\phi})f(\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\\\\n&\\hspace{-4in}=f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs}, \\boldsymbol{\\theta})f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis}, \\boldsymbol{\\theta})f(\\mathbf{O} | \\mathbf{Y}, \\mathbf{X}, \\boldsymbol{\\phi})f(\\boldsymbol{\\theta},\\boldsymbol{\\phi})\\\\\n\\end{align*}\n\n- We can simplify this posterior by dropping the missing data mechanism.\n\n## Full Bayesian model\n\n- Assuming that $f(\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = f(\\boldsymbol{\\theta})f(\\boldsymbol{\\phi})$, the missingness process does not need to be explicitly modeled when we are interested in inference for $\\boldsymbol{\\theta}$.\n\n\\begin{align*}\n&f(\\boldsymbol{\\theta}, \\mathbf{Y}_{mis} | \\mathbf{Y}_{obs},\\mathbf{O},\\mathbf{X})\\\\ &\\hspace{2in}\\propto f(\\mathbf{Y}_{obs} | \\mathbf{X}_{obs}, \\boldsymbol{\\theta})f(\\mathbf{Y}_{mis} | \\mathbf{X}_{mis}, \\boldsymbol{\\theta})f(\\boldsymbol{\\theta})\n\\end{align*}\n\n## Full Bayesian model for linear regression\n\n\\begin{align*}\nY_i | \\alpha, \\beta, \\sigma^2 &\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_i\\boldsymbol{\\beta}, \\sigma^2), \\quad i \\in \\mathcal N_{obs}\\\\\nY_i | \\alpha, \\beta, \\sigma^2&\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_i \\boldsymbol{\\beta}, \\sigma^2), \\quad i \\in \\mathcal N_{mis}\\\\\n\\alpha &\\sim f(\\alpha)\\\\\n\\beta &\\sim f(\\boldsymbol{\\beta})\\\\\n\\sigma &\\sim f(\\sigma),\n\\end{align*}\n\n- $\\mathbf{x}_i = (height_i, 1(sex_i = male), height_i \\times 1(sex_i = male))$.\n\n## Full Bayesian missing data model in Stan\n\n\n\n::: {.cell output.var='missing'}\n\n```{.stan .cell-code}\n// saved in missing-full-bayes.stan\ndata {\n  int<lower = 1> n_obs;\n  int<lower = 1> n_mis;\n  int<lower = 1> p;\n  vector[n_obs] Y_obs;\n  matrix[n_obs, p] X_obs;\n  matrix[n_mis, p] X_mis;\n}\ntransformed data {\n  vector[n_obs] Y_obs_centered;\n  real Y_bar;\n  matrix[n_obs, p] X_obs_centered;\n  matrix[n_mis, p] X_mis_centered;\n  row_vector[p] X_bar;\n  Y_bar = mean(Y_obs);\n  Y_obs_centered = Y_obs - Y_bar;\n  matrix[n_obs + n_mis, p] X = append_row(X_obs, X_mis);\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_obs_centered[, i] = X_obs[, i] - X_bar[i];\n    X_mis_centered[, i] = X_mis[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha_centered;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  vector[n_mis] Y_mis_centered;\n}\nmodel {\n  target += normal_lpdf(Y_obs_centered | alpha_centered + X_obs_centered * beta, sigma);\n  target += normal_lpdf(Y_mis_centered | alpha_centered + X_mis_centered * beta, sigma);\n  target += normal_lpdf(alpha_centered | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}\ngenerated quantities {\n  real alpha;\n  vector[n_mis] Y_mis;\n  alpha = Y_bar + alpha_centered - X_bar * beta;\n  Y_mis = Y_mis_centered + Y_bar;\n}\n```\n:::\n\n\n\n## Fit missing data model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_missing_data_model <- stan_model(file = \"missing-full-bayes.stan\")\nX <- model.matrix(~ x1 * x2, data = fulldata)[, -1]\nstan_data_missing_data <- list(\n  n_obs = sum(fulldata$o == 1),\n  n_mis = sum(fulldata$o == 0),\n  p = ncol(X),\n  Y_obs = array(fulldata$y[fulldata$o == 1]),\n  X_obs = X[fulldata$o == 1, ],\n  X_mis = X[fulldata$o == 0, ]\n)\nfit_full_bayes_joint <- sampling(stan_missing_data_model, stan_data_missing_data)\nprint(fit_full_bayes_joint, pars = c(\"alpha\", \"beta\", \"sigma\"), probs = c(0.025, 0.975))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n## Explore model fit\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(fit_full_bayes_joint, pars = c(\"alpha\", \"beta\", \"sigma\"), probs = c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nInference for Stan model: anon_model.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean    sd   2.5% 97.5% n_eff Rhat\nalpha   -48.38    0.83 21.10 -90.15 -7.03   653 1.00\nbeta[1]   0.65    0.00  0.12   0.41  0.90   673 1.00\nbeta[2]  -3.19    0.22  9.75 -22.23 16.56  2047 1.00\nbeta[3]   0.08    0.00  0.06  -0.03  0.19  2487 1.00\nsigma     8.21    0.02  0.58   7.17  9.45   600 1.01\n\nSamples were drawn using NUTS(diag_e) at Wed Jan 22 19:43:45 2025.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n## Explore latent missing variable\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(bayesplot)\nY_mis <- rstan::extract(fit_full_bayes_joint, pars = \"Y_mis\")$Y_mis\ncolnames(Y_mis) <- paste0(\"Y_mis[\", 1:ncol(Y_mis), \"]\")\nmcmc_areas_ridges(Y_mis[, 1:10])\n```\n\n::: {.cell-output-display}\n![](14-missing_files/figure-revealjs/unnamed-chunk-11-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n\n## Summary of Bayesian joint model\n\n- The joint model treats the missing data as parameters (i.e., latent variables in the model).\n\n- Placing a prior on the missing data allows us to jointly learn the model parameters and the missing data. \n\n- Equivalent to multiple imputation at every step of the HMC. Can be slow!\n\n- In Stan, we can only treat continuous missing data as parameters, so this method is somewhat limited (what do we do if the missing data is a binary outcome?)\n\n## Multiple imputation\n\n- As an alternative to fitting a joint model, there are many approaches that allow us to impute missing data before the actual model fitting takes place.\n\n- From a statistical perspective, multiple imputation is one of the best solutions. \n\n- Each missing value is not imputed once but $m$ times leading to a total of $m$ fully imputed data sets. \n\n- The model can then be fitted to each of those data sets separately and results are pooled across models, afterwards. \n\n- One widely applied package for multiple imputation is `mice` (Buuren & Groothuis-Oudshoorn, 2010) and we will use it in the following in combination with Stan. \n\n## Mice\n\n- Here, we apply the default settings of mice, which means that all variables will be used to impute missing values in all other variables and imputation functions automatically chosen based on the variablesâ€™ characteristics.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mice)\nm <- 100\nmardata <- data.frame(\n  y = fulldata$y,\n  x1 = fulldata$x1,\n  x2 = fulldata$x2\n)\nmardata$y[fulldata$o == 0] <- NA\nimp <- mice(mardata, m = m, print = FALSE)\n```\n:::\n\n\n\n## Mice\n\nNow, we have m = 5 imputed data sets stored within the imp object. In practice, we will likely need more than 5 of those to accurately account for the uncertainty induced by the missingness, perhaps even in the area of 100 imputed data sets (Zhou & Reiter, 2010).\n\nWe can extract the first imputed dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- complete(imp, 1)\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     y    x1 x2\n1 65.6 174.0  1\n2 77.7 175.3  1\n3 98.2 193.5  1\n4 84.8 186.5  1\n5 85.9 187.2  1\n6 75.0 181.5  1\n```\n\n\n:::\n:::\n\n\n\n- We can now fit our model $m$ times for each imputed data sets and combine the posterior samples from all chains for inference.\n\n## Model for the imputed datasets\n\n\n\n::: {.cell output.var='cc'}\n\n```{.stan .cell-code}\n// bayesian-mi.stan\ndata {\n  int<lower = 1> n;\n  int<lower = 1> p;\n  vector[n] Y;\n  matrix[n, p] X;\n}\ntransformed data {\n  vector[n] Y_centered;\n  real Y_bar;\n  matrix[n, p] X_centered;\n  row_vector[p] X_bar;\n  Y_bar = mean(Y);\n  Y_centered = Y - Y_bar;\n  for (i in 1:p) {\n    X_bar[i] = mean(X[, i]);\n    X_centered[, i] = X[, i] - X_bar[i];\n  }\n}\nparameters {\n  real alpha_centered;\n  vector[p] beta;\n  real<lower = 0> sigma;\n}\nmodel {\n  target += normal_lpdf(Y_centered | alpha_centered + X_centered * beta, sigma);\n  target += normal_lpdf(alpha_centered | 0, 10);\n  target += normal_lpdf(beta | 0, 10);\n  target += normal_lpdf(sigma | 0, 10);\n}\ngenerated quantities {\n  real alpha;\n  alpha = Y_bar + alpha_centered - X_bar * beta;\n}\n```\n:::\n\n\n\n## Fit the model to the imputed data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_bayesian_mi <- stan_model(file = \"bayesian-mi.stan\")\nalpha <- beta <- sigma <- r_hat <- n_eff <- NULL\nn_chains <- 2\nfor (i in 1:m) {\n  \n  ###Load each imputed dataset and fit the Stan complete case model\n  data <- complete(imp, i)\n  X <- model.matrix(~ x1 * x2, data = data)[, -1, drop = FALSE]\n  stan_data_bayesian_mi <- list(\n    n = nrow(data),\n    p = ncol(X),\n    Y = data$y,\n    X = X\n  )\n  fit_mi <- sampling(stan_bayesian_mi, stan_data_bayesian_mi, chains = n_chains)\n  \n  ###Save convergence diagnostics from each imputed dataset\n  r_hat <- cbind(r_hat, summary(fit_mi)$summary[, \"Rhat\"])\n  n_eff <- cbind(n_eff, summary(fit_mi)$summary[, \"n_eff\"])\n  pars <- rstan::extract(fit_mi, pars = c(\"alpha\", \"beta\", \"sigma\"))\n  \n  ### Save the parameters from each imputed dataset\n  n_sims_chain <- length(pars$alpha) / n_chains\n  alpha <- rbind(alpha, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$alpha))\n  beta <- rbind(beta, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$beta))\n  sigma <- rbind(sigma, cbind(i, rep(1:n_chains, each = n_sims_chain), pars$sigma))\n}\n```\n:::\n\n\n\n## Inspect traceplots\n\n![](images/14/alpha.png){fig-align=\"center\" height=300}\n\n## Inspect traceplots\n\n![](images/14/beta[1].png){fig-align=\"center\" height=300}\n\n## Inspect traceplots\n\n![](images/14/beta[2].png){fig-align=\"center\" height=300}\n\n## Inspect traceplots\n\n![](images/14/beta[3].png){fig-align=\"center\" height=300}\n\n## Inspect traceplots\n\n![](images/14/sigma.png){fig-align=\"center\" height=300}\n\n## Comparison of methods: $\\alpha$ {.small}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Data |Model       |   Mean|    SD|   2.5%|  97.5%|     n_eff| Rhat|\n|:----|:-----------|------:|-----:|------:|------:|---------:|----:|\n|Full |OLS         | -43.82| 13.78| -70.89| -16.75|        NA|   NA|\n|MAR  |Bayes Joint | -48.38| 21.10| -90.15|  -7.03|    652.73|    1|\n|MAR  |Bayes MI    | -43.45| 23.72| -85.03|   3.97| 119974.91|    1|\n|MAR  |OLS         | -30.56| 23.70| -77.55|  16.43|        NA|   NA|\n\n\n:::\n:::\n\n\n\n## Comparison of methods: $\\beta_1$ {.small}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Data |Model       | Mean|   SD| 2.5%| 97.5%|     n_eff| Rhat|\n|:----|:-----------|----:|----:|----:|-----:|---------:|----:|\n|Full |OLS         | 0.63| 0.08| 0.47|  0.80|        NA|   NA|\n|MAR  |Bayes Joint | 0.65| 0.12| 0.41|  0.90|    672.77|    1|\n|MAR  |Bayes MI    | 0.63| 0.14| 0.35|  0.87| 119778.47|    1|\n|MAR  |OLS         | 0.55| 0.14| 0.27|  0.83|        NA|   NA|\n\n\n:::\n:::\n\n\n\n## Comparison of methods: $\\beta_2$ {.small}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Data |Model       |   Mean|    SD|    2.5%| 97.5%|    n_eff| Rhat|\n|:----|:-----------|------:|-----:|-------:|-----:|--------:|----:|\n|Full |OLS         | -17.13| 19.56|  -55.57| 21.30|       NA|   NA|\n|MAR  |Bayes Joint |  -3.19|  9.75|  -22.23| 16.56|  2047.43|    1|\n|MAR  |Bayes MI    |  -5.52| 10.24|  -25.29| 14.89| 85499.19|    1|\n|MAR  |OLS         | -82.60| 49.54| -180.82| 15.62|       NA|   NA|\n\n\n:::\n:::\n\n\n\n## Comparison of methods: $\\beta_3$ {.small}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Data |Model       | Mean|   SD|  2.5%| 97.5%|    n_eff| Rhat|\n|:----|:-----------|----:|----:|-----:|-----:|--------:|----:|\n|Full |OLS         | 0.15| 0.11| -0.08|  0.37|       NA|   NA|\n|MAR  |Bayes Joint | 0.08| 0.06| -0.03|  0.19|  2487.34|    1|\n|MAR  |Bayes MI    | 0.09| 0.06| -0.03|  0.20| 83444.33|    1|\n|MAR  |OLS         | 0.52| 0.27| -0.02|  1.06|       NA|   NA|\n\n\n:::\n:::\n\n\n\n## Comparison of methods: $\\sigma$ {.small}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|Data |Model       | Mean|   SD| 2.5%| 97.5%|     n_eff| Rhat|\n|:----|:-----------|----:|----:|----:|-----:|---------:|----:|\n|Full |OLS         | 8.73| 0.39| 8.07|  9.49|        NA|   NA|\n|MAR  |Bayes Joint | 8.21| 0.58| 7.17|  9.45|    599.95| 1.01|\n|MAR  |Bayes MI    | 8.36| 0.35| 7.71|  9.09| 151152.94| 1.00|\n|MAR  |OLS         | 7.87| 0.52| 6.84|  8.90|        NA|   NA|\n\n\n:::\n:::\n\n\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Missing data\n",
    "supporting": [
      "14-missing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}