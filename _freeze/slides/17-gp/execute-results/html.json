{
  "hash": "7a663b60a225db75030a87c849cd397c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Gaussian Processes\"\nauthor: \"Dr. Youngsoo Baek\"\ndate: \"2025-03-18\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Recalling what went before the break\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/plot_intro-1.png){width=960}\n:::\n:::\n\n\n\n## Is Linear Fit the Right Way? {.small}\n\nThere are various ways to fit a model to time-varying behavior of the curves.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/plot_smooth-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/plot_smooth-2.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n## A brief agenda\n\nBy the end of this lecture you should:\n\n1.  Understand the basic concepts of Gaussian processes (GP) and their usefulness in statistical modeling.\n\n2.  Compute posterior and predictive inference for a simple GP model.\n\n3.  Use GP as a building block in modeling time-correlated data.\n\n## Motivation {.midi}\n\nLet $i$ index distinct eyes ($i=1,2,\\ldots,n$) and $t$ index within-eye data in time ($t=1,2,\\ldots,n_i$).\n\nSuppose we are interested in effects of time-invariant predictors (`age` and `iop`):\n\n$$Y_{it} = \\underbrace{\\mathbf{x}_i\\boldsymbol{\\beta}}_{\\text{Fixed in time}} + \\underbrace{\\eta_{it}}_{\\text{Varying in time}} + \\epsilon_{it}$$\n\nIn a previous lecture, we included only `time` ($X_{it}$) and fit a random slope regression.\n\n$$\\eta_{it} = \\theta_{0i} + \\underbrace{X_{it}}_{=Time}\\theta_{1i}.$$\n\nWhat if we want a **nonlinear model** for time effects?\n\n## Exploratory Analysis of Population Predictors\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/eda_vis-1.png){width=960}\n:::\n:::\n\n\n\n## Exploratory Analysis of Population Predictors\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n\n## A review of computation / linear algebra needed to sample {.small}\n\nHow can we sample a 3D normal random vector?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- 3\nmu <- c(3, -1, 5)\nSigma <- matrix(\n  c(\n    1.0, 0.6, 0.3,\n    0.6, 1.0, 0.6,\n    0.3, 0.6, 1.0\n  ),\n  nrow = 3, ncol = 3\n)\nz <- rnorm(p)\nR <- t(chol(Sigma)) ## R %*% t(R) == Sigma\nx <- R %*% z\n\n# This is the same as\nx <- rmvnorm(1, mu, Sigma)\n```\n:::\n\n\n\nFrom previous lecture: `R` decomposes into `D` (diagonal) and `L` (Cholesky factor of a correlation matrix).\n\n## Constructing a Gaussian Random Function in Time {.midi}\n\nWhat about \"random functions\"? Can we extend this idea to arbitrarily many time points? What does that even mean?\n\n1.  We start at time $t=0$ from $X_0 = 0$.\n\n2.  When time passes by amount of $h > 0$, we want\n\n$$X_{t+h} - X_t \\sim N(0,h).$$ Note: $\\mathbb{E}[X_t^2] = \\mathbb{V}(X_t) = t$ (Why?)\n\n3.  Each increment will be independent from each other.\n\nNow think of *passing to the limit*: Somehow it works (!) and we have a process defined **at every** $t$.\n\n## Pointwise Observations from a Brownian Motion {.small}\n\nSuppose we observe this process $X_t$ at time points $(t_1,t_2,\\ldots,t_N)$. Then the distribution of the following vector is a multivariate normal:\n\n$$\n\\begin{bmatrix}\nX_1\\\\ \\vdots \\\\ X_N\n\\end{bmatrix}\\sim N\\left(\\mathbf{0},\\boldsymbol{\\Sigma}\\right),\\; \\boldsymbol{\\Sigma} = \\begin{bmatrix}\nt_1 & t_1 & \\cdots & t_1 \\\\\nt_1 & t_2 & \\cdots & t_2 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nt_1 & t_2 & \\cdots & t_N\n\\end{bmatrix}\n$$\n\nThe covariance structure comes from solving the equation:\n\n$$\nh = \\mathbb{V}(X_{t+h} - X_t) = \\underbrace{\\mathbb{E}[X_{t+h}^2]}_{=t+h} + \\underbrace{\\mathbb{E}[X_t^2]}_{=t} - 2\\mathbb{C}(X_{t+h},X_t).\n$$\n\n## Visualizing the Brownian Motion\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/brownian_motion-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## What if I want different correlation structures? {.midi}\n\nFor longitudinal data,\n\n-   We are not interested in an indefinitely long time span.\n\n-   Within the time window, the data can exhibit stable behavior.\n\ni.  *Hypothesis 1* : The marginal variability of $X_t$ should stay the same.\n\nii. *Hypothesis 2* : Time correlation should decay in, and **only** depend on, the amount of time elapsed.\n\niii. *Hypothesis 3* : We have expectations about the span of correlation and smoothness of the process.\n\nThese are all **a priori** hypotheses. The data may come from something very different!\n\n## Stationary Gaussian processes\n\nA different process results from a **stationary kernel**:\n\n$$\n\\mathbb{C}(X_t, X_s) = C(|t-s|).\n$$\n\nWe want $C(0) = \\sigma^2 > 0$ and $C\\to 0$ as $h = |t-s|\\to\\infty$. Some common choices:\n\n-   Exponential kernel: $C(h) = \\sigma^2\\exp(-h/\\rho)$\n\n-   Square exponential kernel: $C(h) = \\sigma^2\\exp\\{-h^2/(2\\rho^2)\\}$\n\n-   MatÃ©rn kernels\n\n## Visualizing the process\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/exp_kernel-1.png){fig-align='center' width=768}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/sqexp_kernel-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Interpreting Hyperparameters and Kernels\n\nOften, we fix the overall **kernel function** while learning its \"bandwidth\" $\\rho$ due to poor identifiability from the data.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/kernel_vis-1.png){fig-align='center' width=1152}\n:::\n:::\n\n\n\n## Default Prior Choices {.midi}\n\n-   Stan team often recommends $\\rho^{-1} \\sim Gamma(5,5)$\n\n-   *A priori* the prior is concentrated around 1. For a prior mean, distance increase of 1 corrresponds to a multiplicative decay of correlation by $e^{-1}\\sim 37\\%$.\n\n-   Beware of the units! A **year** correlation is very different from that over a **day**.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/prior_vis-1.png){fig-align='center' width=768}\n:::\n:::\n\n\n\n## Gaussian Process Covariance Functions in Stan\n\nA list of available kernels is available [here](https://mc-stan.org/docs/functions-reference/matrix_operations.html#gaussian-process-covariance-functions).\n\nThe squared exponential kernel (exponentiated quadratic kernel) is given by:\n\n$$C(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma^2 \\exp\\left(-\\frac{|\\mathbf{x}_i - \\mathbf{x}_j|^2}{2l^2}\\right)$$\n\n\n\n::: {.cell output.var='kernel'}\n\n```{.stan .cell-code}\nmatrix gp_exp_quad_cov(array[] real x, real sigma, real length_scale)\n```\n:::\n\n\n\nFor us, $\\mathbf{x}$ is 1D (time)...but it does not have to be!\n\n## Prior Sampling in Stan {.midi}\n\nOnce we have a GP model, sampling from the prior is equivalent to sampling from a multivariate normal.\n\n\n\n::: {.cell output.var='prior_sampling'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=1> N;\n  array[N] real x;\n  real<lower=0> sigma;\n  real<lower=0> l;\n}\ntransformed data {\n  // 0. A \"small nugget\" to stabilize matrix root computation\n  // This is for.better numerical stability in taking large matrix roots\n  real delta = 1e-9;\n\n  // 1. Compute the squared exponential kernel matrix\n  // x is the time variable\n  vector[N] mu = rep_vector(0, N);\n  matrix[N, N] R_C;\n  matrix[N, N] C = gp_exp_quad_cov(x, sigma, l);\n  for (i in 1:N) {\n    C[i, i] = C[i, i] + delta;\n  }\n\n  // 2. Compute the root of C by Cholesky decomposition\n  R_C = cholesky_decompose(C);\n}\ngenerated quantities {\n  // 3. Sample from the prior: multivariate_normal(0, C)\n  f ~ multi_normal_cholesky(mu, R_C)\n}\n```\n:::\n\n\n\n## Posterior inference {.midi}\n\nFor each $i$-th eye: time effect vector is given an independent prior based on the GP model.\n\n$$\\boldsymbol{\\eta}_i = (\\eta_{i1},\\ldots,\\eta_{in_i})^\\top \\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0},\\mathbf{C}_i)$$\n\nEach $\\eta_{it}$ is a pointwise value of the process $\\eta_i(t) = \\eta_{it}$.\n\n$$\\mathbf{C}_i = \\begin{bmatrix}\nC(0) & C(|t_{i1}-t_{i2}|) & \\cdots & C(|t_{i1} - t_{in_i}|)\\\\\nC(|t_{i1} - t_{i2}|) & C(0) & \\cdots & C(|t_{i,2} - t_{in_i}|)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\n\\end{bmatrix}$$\n\n**Remember**: Different eyes need not have the same number of encounters, nor need they have measurements at the same time.\n\n## The full model {.midi}\n\nFor $i$ ($i = 1,\\ldots,n$) and $t$ ($t = 1,\\ldots,n_i$),\n\n\\begin{align*}\nY_{it} &= \\mathbf{x}_{i}\\boldsymbol{\\beta} + \\eta_{i}(t) + \\epsilon_{it}, \\quad \\epsilon_{it} \\stackrel{iid}{\\sim} N(0,\\sigma^2),\\\\\n\\boldsymbol{\\eta}_i &\\stackrel{ind}{\\sim} N_{n_i}(\\mathbf{0},\\mathbf{C}_i),\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}).\n\\end{align*}\n\n-   $\\boldsymbol{\\Omega}$ represents the population parameters: $\\boldsymbol{\\beta}$, $\\sigma^2$, and any kernel parameters determining $\\mathbf{C}_i$\n\n-   For concreteness: we use a squared exponential kernel $C$ and model $\\mathbb{C}(\\eta_i(t),\\eta_i(t')) = \\alpha^2\\exp(-|t-t'|^2/2\\rho^2).$ Thus $\\boldsymbol{\\Omega} = (\\boldsymbol{\\beta},\\sigma^2,\\alpha,\\rho)$.\n\n## First Look at the Data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(dataset)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  pat_id eye_id mean_deviation      time      age      iop\n1      1      1          -7.69 0.0000000 51.55616 10.87303\n2      1      1          -9.95 0.5753425 51.55616 10.87303\n3      1      1          -9.58 1.0547945 51.55616 10.87303\n4      1      1          -9.53 1.5726027 51.55616 10.87303\n5      1      1          -9.18 2.0136986 51.55616 10.87303\n6      1      1          -9.63 2.5671233 51.55616 10.87303\n```\n\n\n:::\n:::\n\n\n\n## Model Specification {.midi}\n\nWe want to fit a model estimating the effects of age and iop on mean deviation, adjusting for nonlinear time effects:\n\n\\begin{align*}\nY_{it} &= \\underbrace{\\beta_0 + \\text{age}_i\\beta_1 + \\text{iop}_i\\beta_2}_{=\\mathbf{x}_{i}\\boldsymbol{\\beta}} + \\eta_{it} + \\epsilon_{it}\\\\\n\\boldsymbol{\\eta}_i &= \\begin{bmatrix} \\eta_{i1}\\\\ \\vdots\\\\ \\eta_{in_i} \\end{bmatrix} \\stackrel{iid}{\\sim} N(0,\\mathbf{C}_i)\\\\\n\\epsilon_{it} &\\stackrel{iid}{\\sim} N(0,\\sigma^2)\n\\end{align*}\n\nOur model for $\\eta_{it}$ is *nonlinear* in time: `time` enters the covariance structure of $\\mathbf{C}_i$ producing a function $\\eta_i(t) = \\eta_{it}$.\n\n## Preparing the Data\n\nA few data processing steps are needed to handle the data structure with more ease.\n\n-   $N$ will denote the number of **all** observations: $N = \\sum_{i=1}^{n} n_i$.\n\n-   $n$ will denote the total number of **eyes**.\n\n-   $p$ will denote the number of predictors fixed across time ($p=3$: intercept, slopes for `age` and `iop`).\n\n-   A separate vector of the number of observations ($n_i$) for each eye will be stored as `s`.\n\n## Preparing the Data {.midi}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\n# Group predictors fixed across time\nfixed_df <- dataset %>% \n  group_by(eye_id) %>% \n  reframe(age = unique(age), iop = unique(iop))\nXmat <- model.matrix(~age + iop, data = fixed_df)\n\n# Number of measurements for each eye\ngroupsizes <- dataset %>% \n  group_by(eye_id) %>% \n  summarise(n = n()) %>% \n  pull(n)\n\nstan_data <- list(\n  N = dim(dataset)[1],\n  n = max(dataset$eye_id),\n  p = dim(Xmat)[2],\n  t = dataset$time,\n  Y = dataset$mean_deviation,\n  s = groupsizes,\n  X = Xmat\n)\n```\n:::\n\n\n\n## Conditional GP Model {.midi}\n\n\n\n::: {.cell output.var='conditional_fit'}\n\n```{.stan .cell-code}\ndata {\n  int<lower=0> N;       // total number of observations\n  int<lower=1> n;       // number of eyes\n  int<lower=1> p;       // fixed effects dimension\n  vector[N] Y;          // observation\n  matrix[n, p] X;        // fixed effects predictors\n  array[N] real t;      // obs time points\n  array[n] int s;       // sizes of within-pt obs\n}\ntransformed data {\n  real delta = 1e-9;\n}\nparameters {\n  // Fixed effects model\n  vector[p] beta;\n  real<lower=0> sigma;\n  // GP parameters\n  vector[N] eta;\n  real<lower=0> alpha;\n  real<lower=0> rho;\n}\ntransformed parameters {\n  vector[n] mu = X * beta;\n}\nmodel {\n  beta ~ normal(0,3);\n  z ~ std_normal();\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n  rho ~ inv_gamma(5,5);\n\n  vector[N] mu_rep;\n  vector[N] eta;\n  \n  int pos;\n  pos = 1;\n  // Ragged loop computing the mean for each time obs\n  for (i in 1:n) {\n    // GP covariance for the k-th eye\n    int n_i = s[i];\n    int pos_end = pos + n_i - 1;\n    matrix[n_i, n_i] R_C;\n    matrix[n_i, n_i] C = gp_exp_quad_cov(segment(t, pos, n_i), alpha, rho);\n    for (j in 1:n_i) {\n      // adding a small term to the diagonal entries\n      C[j, j] = C[j, j] + delta;\n    }\n    R_C = cholesky_decompose(C);\n    \n    // Mean of data at each time\n    mu_rep[pos:pos_end] = rep_vector(mu[i], n_i);\n    // GP for the i-th eye\n    eta[pos:pos_end] = R_C * segment(z, pos, n_i);\n    pos = pos_end + 1;\n  }\n  // Normal observation model centered at mu_rep + eta\n  Y ~ normal(mu_rep + eta, sigma);\n}\n```\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(gp_condl_fit, pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/traceplot1-1.png){width=960}\n:::\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_acf(gp_condl_fit, regex_pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/acf1-1.png){width=960}\n:::\n:::\n\n\n\n## Marginal Model Specification {.midi}\n\nThe conditional model is pretty slow (*can be improved*) and results in poor mixing (*can be run longer*). Marginalizing the random effect can stabilize the computation.\n\nIn vector notation, our observation model for the $i$-th eye is\n\n$$\n\\mathbf{Y}_i = \\begin{bmatrix} Y_{i1}\\\\ \\vdots \\\\ Y_{in_i} \\end{bmatrix}\n= \\mathbf{x}_i\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_i + \\boldsymbol{\\epsilon}_i\n$$ By marginalizing out $\\boldsymbol{\\eta}_i$, we obtain\n\n$$\n\\mathbf{Y}_i \\sim N_{n_i}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}_i + \\sigma^2\\mathbf{I}_{n_i})\n$$\n\n## Fitting a Marginal Model {.midi}\n\nOnly the `model` segment has to be changed.\n\n\n\n::: {.cell output.var='marginal'}\n\n```{.stan .cell-code}\nmodel {\n  beta ~ normal(0,3);\n  alpha ~ std_normal();\n  sigma ~ std_normal();\n  rho ~ inv_gamma(5,5);\n\n  int pos;\n  pos = 1;\n  // Ragged loop computing joint likelihood for each eye\n  for (i in 1:n) {\n    // GP covariance for the k-th eye\n    int n_i = s[i];\n    vector[n_i] mu_rep;\n    \n    matrix[n_i, n_i] R_C;\n    matrix[n_i, n_i] C = gp_exp_quad_cov(segment(t, pos, n_i), alpha, rho);\n    for (j in 1:n_i) {\n      // Add noise variance to the diagonal entries\n      C[j, j] = C[j, j] + square(sigma);\n    }\n    R_C = cholesky_decompose(C);\n    // Marginal model for the i-th eye\n    mu_rep = rep_vector(mu[i], n_i);\n    target += multi_normal_cholesky_lpdf(to_vector(segment(Y, pos, n_i)) | mu_rep, R_C);\n    pos = pos + n_i;\n  }\n}\n```\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraceplot(gp_marginal_fit, pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/traceplot2-1.png){width=960}\n:::\n:::\n\n\n\n## Assessing Convergence\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbayesplot::mcmc_acf(gp_marginal_fit, regex_pars = c(\"beta\", \"sigma\", \"alpha\", \"rho\"))\n```\n\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/acf2-1.png){width=960}\n:::\n:::\n\n\n\n## Estimates of eye-level Predictors\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/ols_comparison-1.png){width=960}\n:::\n:::\n\n\n\n## Learned GP hyperparameter {.midi}\n\n-   The posterior mean of $\\rho$ (in year) is very large: the pattern of mean deviation change is very gradual over time.\n\n-   *A posteriori* we believe mean deviations of an eye, 10 years apart and adjusted for age and iop, are still strongly correlated ($e^{-5/6}\\sim 43\\%$).\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/post_vis-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Random effects covariance structure for the first 5 years\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/cov_heatmap-1.png){width=960}\n:::\n:::\n\n\n\n## Predictive inference\n\nThe model is *continuous* in nature and defined, in principle, for all times. This is very convenient for visualiztion, interpolation, and forecasting.\n\n$$\n\\mathbf{Y}_i = \\underbrace{\\mathbf{f}_{i}}_{ = \\mathbf{x}_{i}\\boldsymbol{\\beta} + \\boldsymbol{\\eta}_{i} } + \\boldsymbol{\\epsilon}_i\n$$\n\nThe vector $\\mathbf{f}_{i}$ may be interpreted as the **denoised latent process** of an eye-specific mean deviation over time: $(f_{i1},\\ldots,f_{in_i})^\\top$.\n\n$$\n\\mathbb{E}[f_{it}] = \\mathbf{x}_i\\boldsymbol{\\beta},\\; \\mathbb{C}(f_{it}, f_{it'}) = \\mathbb{C}(\\eta_{it},\\eta_{it'})\n$$\n\n## Predictive Inference using Normal Conditioning {.small}\n\nSay $\\mathbf{f}^{pred}_i$ is the values of $f_i$ at $m_i$ **new time points** which we want to predict, based on our GP model:\n\n\\begin{align*}\n\\mathbf{f}^{pred}_i &\\sim N_{m_i}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}^{pred}_i)\\\\\n\\mathbf{Y}_i = \\mathbf{f}_i + \\boldsymbol{\\epsilon}_i &\\sim N_{n_1}(\\mathbf{x}_i\\boldsymbol{\\beta},\\mathbf{C}_i + \\sigma^2\\mathbf{I}_{n_i})\n\\end{align*}\n\nAn analytical formula exists for the **conditional distribution** of $\\mathbf{f}^{pred}$ given observed $Y_{it}$ at $n_i$ time points.\n\n$$\n\\mathbf{f}^{pred}_i | \\mathbf{Y}_i \\sim N\\left(\\mathbf{x}_i\\boldsymbol{\\beta} + \\mathbf{k}_i(\\mathbf{C}_i + \\sigma^2\\mathbf{I})^{-1}(\\mathbf{Y}_i - \\mathbf{x}_i\\boldsymbol{\\beta}), \\mathbf{C}_i^{pred}-\\mathbf{k}_i(\\mathbf{C}_i + \\sigma^2\\mathbf{I})^{-1}\\mathbf{k}_i^\\top\\right)\n$$ where $\\mathbf{k}_i = \\mathbb{C}(\\mathbf{f}^{pred}_i, \\mathbf{f}_i)$ (\"cross-covariances\").\n\nThis stems from a more general fact about conditional distributions of jointly normal vectors: e.g., $\\mathbf{f}_i^{pred}$ and $\\mathbf{f}_i$ need not have the same mean.\n\n## New Data for Predictive Inference\n\nSuppose we are interested in understanding the mean deviation trend for the first 5 years from baseline ($t^{pred}\\in [0,5]$).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstan_data <- list(\n  N = dim(dataset)[1],\n  n = max(dataset$eye_id),\n  p = dim(Xmat)[2],\n  Y = dataset$mean_deviation,\n  t = dataset$time,\n  s = groupsizes,\n  X = Xmat,\n  # Time window for which we want predictive\n  t_pred = seq(0, 5, by = 0.25),\n  # Total length of this window\n  N_pred = 21\n)\n```\n:::\n\n\n\n## Stan Implementation {.small}\n\nSee the [Stan Help page](https://mc-stan.org/docs/stan-users-guide/gaussian-processes.html) for details.\n\n\n\n::: {.cell output.var='predictive_inference'}\n\n```{.stan .cell-code}\nfunctions {\n  // Analytical formula for latent GP conditional on Gaussian observations\n  vector gp_pred_rng(array[] real x_pred,\n                     vector Y,\n                     array[] real x,\n                     real mu,\n                     real alpha,\n                     real rho,\n                     real sigma,\n                     real delta) {\n    int N1 = rows(Y);\n    int N2 = size(x_pred);\n    vector[N2] f_pred;\n    {\n      matrix[N1, N1] L_Sigma;\n      vector[N1] Sigma_div_y;\n      matrix[N1, N2] C_x_xpred;\n      matrix[N1, N2] v_pred;\n      vector[N2] fpred_mu;\n      matrix[N2, N2] cov_fpred;\n      matrix[N2, N2] diag_delta;\n      matrix[N1, N1] Sigma;\n      Sigma = gp_exp_quad_cov(x, alpha, rho);\n      for (n in 1:N1) {\n        Sigma[n, n] = Sigma[n, n] + square(sigma);\n      }\n      L_Sigma = cholesky_decompose(Sigma);\n      Sigma_div_y = mdivide_left_tri_low(L_Sigma, Y - mu);\n      Sigma_div_y = mdivide_right_tri_low(Sigma_div_y', L_Sigma)';\n      C_x_xpred = gp_exp_quad_cov(x, x_pred, alpha, rho);\n      fpred_mu = (C_x_xpred' * Sigma_div_y);\n      v_pred = mdivide_left_tri_low(L_Sigma, C_x_xpred);\n      cov_fpred = gp_exp_quad_cov(x_pred, alpha, rho) - v_pred' * v_pred;\n      diag_delta = diag_matrix(rep_vector(delta, N2));\n\n      f_pred = multi_normal_rng(fpred_mu, cov_fpred + diag_delta);\n    }\n    return f_pred;\n  }\n}\n//...\ngenerated quantities {\n  matrix[n,Np] f_pred;\n  matrix[Np,Np] Cp;\n  Cp = gp_exp_quad_cov(t_pred, alpha, rho);\n  // Posterior predictive on fixed time grid for all eyes\n  int pos;\n  pos = 1;\n  for (i in 1:n) {\n    int n_i = s[i];\n    f_pred[i,] = mu[i] + \n      gp_pred_rng(\n          t_pred,\n          segment(Y, pos, n_i),\n          segment(t, pos, n_i),\n          mu[i], \n          alpha,\n          rho,\n          sigma,\n          delta\n        )';\n    pos = pos + n_i;\n  }\n}\n```\n:::\n\n\n\n## Visualizing latent effects using predictive formulae\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](17-gp_files/figure-revealjs/predictive-1.png){width=960}\n:::\n:::\n\n\n\n## Stan speed concerns\n\n-   GP is notorious for not being scalable. Tl;dr is the need for matrix root / inverse computation.\n\n-   For datasets covered in this class, Stan works well. For research purposes, worth exploring other specialized toolkits.\n\ni.  Code MCMC yourself to make it faster (e.g., using `Rcpp`)\n\nii. Do a bit of software research on [Wikipedia](https://en.wikipedia.org/wiki/Comparison_of_Gaussian_process_software)\n\niii. Scalable Approximations (some of them will be coming soon!)\n\n## A summary\n\n-   GP is a flexible, high-dimensional model for handling correlated measurements over time.\n\n-   With some basic knowledge about conditioning Gaussian random variables, we can implement posterior computation and prediction / interpolation.\n\n-   Programming in Stan is straightforward but can be expensive with large number of observations.\n\n## Prepare for next class\n\n-   Work on [HW 04](https://biostat725-sp25.netlify.app/hw/hw-04) which is due March 25\n\n-   Complete reading to prepare for next Thursday's lecture\n\n-   Thursday's lecture: Using GP to model spatial data\n",
    "supporting": [
      "17-gp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}