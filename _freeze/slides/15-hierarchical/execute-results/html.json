{
  "hash": "cd0dedc0e821bf7084bc26ed5a56c99a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Hierarchical Models\"\nauthor: \"Prof. Sam Berchuck\"\ndate: \"2025-02-27\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— BIOSTAT 725 - Spring 2025](https://biostat725-sp25.netlify.app/)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: true\n    incremental: false \n    chalkboard: true\n    html-math-method: mathjax\nfilters:\n  - parse-latex\nexecute:\n  freeze: auto\n  echo: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n::: {.cell}\n\n:::\n\n\n\n## Review of last lecture\n\n-   Last week, we learned about classification for binary and multiclass problems.\n\n-   Up until today, we have dealt with independent data. Today, we will look at our first example of dependent data!\n\n## Linear regression assumptions\n\n\\begin{align*}\nY_i &= \\alpha + \\mathbf{x}_i \\boldsymbol{\\beta} + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2)\\\\\n&= \\mu_i + \\epsilon_i.\n\\end{align*}\n\nAssumptions:\n\n1.  $Y_i$ are independent observations (independence).\n\n2.  $Y_i$ is linearly related to $\\mathbf{x}_i$ (linearity).\n\n3.  $\\epsilon_i = Y_i - \\mu_i$ is normally distributed (normality).\n\n4.  $\\epsilon_i$ has constant variance across $\\mathbf{x}_i$ (homoskedasticity).\n\n## Independence Assumption in Linear Regression\n\nWe assume that the residuals $\\epsilon_i$ are independent:\n\n$$\\mathbb{C}(\\epsilon_i, \\epsilon_j) = 0, \\quad \\text{for} \\quad i \\neq j,$$\nwhere $\\mathbb{C}(X, Y)$ is the covariance between two random variables $X$ and $Y$. As a note: $\\mathbb{C}(X, X) = \\mathbb{V}(X)$.\n\n- This implies that the observations $Y_i$ and $Y_j$ are independent, and their correlation is zero.\n\n  - Correlation: $\\rho(X,Y) = \\frac{\\mathbb{C}(X, Y)}{\\sqrt{\\mathbb{V}(X)\\mathbb{V}(Y)}}$.\n\n## Real-World: Dependent Observations\n\nHowever, in real-world data, the independence assumption often does not hold:\n\n- **Repeated measures data** (e.g., same individual over time).\n\n- **Clustered data** (e.g., patients within a hospital).\n\n- **Longitudinal data** (e.g., disease severity measures over time).\n\n- **Spatial data** (e.g., disease counts observed across zip codes).\n\n## The Challenge\n\n- If we assume independence in the presence of correlation:\n\n  1. **Biased parameter estimates**: Parameter estimation will be biased due to group-level dependencies that effect the outcome.\n  \n  2. **Underestimated uncertainty**: The model will not account for the true variability, leading to narrower confidence intervals.\n\n  3. **Inaccurate Predictions**: Predictions for new groups may be biased because the model doesn't properly account for group-level variability.\n\n- Thus, we need a way to account for dependencies between observations, especially when data are grouped or clustered.\n\n## Example of Hierarchical Data\n\n- Hierarchical data refers to data that is organized into groups or clusters, where each group contains multiple observations.\n\n- Consider data from patients within hospitals. Each patient is being treated in a hospital, with multiple patients belonging to each hospital.\n\n- In this case, the observation for a patient is indexed by two variables:\n\n    - $i$: hospital index.\n\n    - $j$: patient index, nested within hospital.\n\n- So, for patient $j$ within hospital $i$, we write the response as $Y_{ij}$.\n\n## Observations with Two Indices: $Y_{ij}$\n\n- $Y_{ij}$ represents the response for patient $j$ in hospital $i$.\n\n- The first index $i$ represents group-level effects (e.g., hospital-level).\n\n- The second index $j$ represents individual-level observations (e.g., patient).\n\n- We typically say that $i = 1,\\ldots,n.$ and $j = 1,\\ldots,n_i$.\n\n- The total number of observations is $N = \\sum_{i = 1}^{n}n_i$.\n\n## Why Two Indices?\n\nHaving two indices allows us to model both:\n\n- **Within-group variation** (differences between patients within the same hospital).\n\n- **Between-group variation** (differences between hospitals).\n\nThe hierarchical structure captures both types of variation.\n\n## Conceptualizing Hierarchical Data\n\nConsider the example of patients within hospitals:\n\n- Each data point (i.e., observed data $Y_{ij}$) represents an outcome measured on a patient.\n\n- The data points are grouped by hospital, indicating that patients from the same hospital are likely to have similar outcomes due to shared hospital-level factors.\n\nThis is a typical example of hierarchical data.\n\n## Hierarchical Model\n\nNow, we can see how hierarchical data appears in the model:\n\n$$Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).$$\n\n- $Y_{ij}$: response for patient $j$ in hospital $i$.\n\n- $\\mathbf{x}_{ij}$ are the predictors for patient $j$ in hospital $i$.\n\n- $\\epsilon_{ij}$: residual error for patient $j$ in hospital $i$.\n\nThis model allows us to account for the correlation within hospital.\n\n## Random Intercept Model\n\n$$Y_{ij} = \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij},\\quad \\epsilon_{ij} \\stackrel{iid}{\\sim}N(0,\\sigma^2).$$\n\n- From a frequentist perspective, this model may be called a **random intercept** model, but in the Bayesian framework all parameters are random variables, so the terms fixed and random effects don't apply. \n\n- $\\theta_i$: hospital-specific parameter for hospital $i$, accounting for hospital-level variation (group-specific, random effect).\n\n- $\\alpha, \\boldsymbol{\\beta}, \\sigma$ are population parameters (common across all groups, $\\boldsymbol{\\beta}$ are the fixed effects).\n\n## Understanding the Random Intercept\n\n- $\\theta_i$: hospital-specific parameter captures group-level differences.\n\n- The intercept $\\theta_i$ allows for each group to have its own baseline value.\n\n- This model introduces **dependence** within groups because observations from the same group share the same intercept $\\theta_i$.\n\n\\begin{align*}\n\\mathbb{E}[Y_{ij} | \\alpha, \\boldsymbol{\\beta},\\sigma,\\theta_i] &= \\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i\\\\\n&= \\alpha_i^* + \\mathbf{x}_{ij} \\boldsymbol{\\beta},\n\\end{align*}\n\nwhere $\\alpha_i^* = \\alpha + \\theta_i$.\n\n## Why Does This Model Work?\n\n1. **Within-group correlation**: Observations within the same hospital are more similar due to the shared intercept.\n\n2. **Between-group differences**: Groups have different intercepts $\\alpha_i^*$, reflecting different baseline effects.\n\n## Identifiability Issues\n\n- **Population Intercept ($\\alpha$)**: This is the average intercept for the entire population, i.e., the baseline outcome across all hospitals.\n\n- **Hospital-Specific Intercept ($\\alpha + \\theta_i$)**: The hospital-specific intercept, where $\\theta_i$ represents the deviation from the population intercept for hospital $i$.\n\nWe face an **identifiability issue** when estimating the population intercept and hospital-specific intercepts. We could add the same constant to all $\\theta_i$'s and subtract that constant from $\\alpha$.\n\n  - This is solved by setting $\\theta_i$ to be mean zero. \n\n## Hospital-Specific Parameters: $\\theta_i$ {.midi}\n\nEach hospital $i$ has a **hospital-specific parameter** $\\theta_i$, which represents how that hospital's baseline (e.g., health outcomes) deviates from the population average.\n\nWe model $\\theta_i$ as a parameter drawn from a **normal distribution** centered at zero, with some variance $\\tau^2$:\n\n$$\\theta_i \\stackrel{iid}{\\sim} N(0, \\tau^2).$$\n\n- **Mean at zero**: This assumption reflects that, on average, hospitals don't deviate from the population mean (helps with identifiability).\n\n- **Variance $\\tau^2$**: This represents the variability in hospital-level intercepts. A larger $\\tau^2$ implies greater variability between hospitals.\n\n## Understanding the Correlation Structure {.midi}\n\n- The random intercept $\\theta_i$ introduces correlation between observations within the same group.\n\n- For two observations $Y_{ij}$ and $Y_{ik}$ from the same group $i$, we have:\n\n\\begin{align*}\n\\mathbb{C}(Y_{ij}, Y_{ik} | \\alpha,\\boldsymbol{\\beta},\\sigma) &=  \\mathbb{C}(\\alpha + \\mathbf{x}_{ij} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ij}, \\alpha + \\mathbf{x}_{ik} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{ik})\\\\\n&= \\mathbb{C}(\\theta_i, \\theta_i)\\\\\n&= \\mathbb{V}(\\theta_i).\n\\end{align*}\n\n- This non-zero covariance reflects the correlation between observations in the same group.\n\n- Note: $\\mathbb{C}(Y_{ij}, Y_{i'k}) = 0$ for $i \\neq i'$.\n\n- To model the correlation, we need to specify the moments of $\\theta_i$.\n\n<!-- ## Why Normal Distribution? -->\n\n<!-- - **Natural Assumption**: We assume that the hospital-specific parameters (e.g., hospital intercepts) are normally distributed with a mean of zero because there's no reason to expect systematic deviations from the population average. -->\n\n<!-- - **Flexibility**: The normal distribution allows us to model a wide range of variation in hospital effects, with the variance $\\tau^2$ capturing how much hospitals differ from each other. -->\n\n## Group-Specific Intercept Model: Conditional Specification\n\nFor $i = 1,\\ldots,n$ and $j = 1,\\ldots,n_i$,\n\\begin{align*}\nY_{ij} | \\boldsymbol{\\Omega},\\theta_i &\\stackrel{ind}{\\sim} N(\\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\theta_i,\\sigma^2)\\\\\n\\theta_i | \\tau^2 &\\stackrel{iid}{\\sim} N(0,\\tau^2)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{align*}\nwhere $\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)$ are the population parameters.\n\n## Group-Specific Intercept Model: Conditional Specification\n\n- Moments for the Conditional Model:\n\n\\begin{align*}\n\\mathbb{E}[Y_{ij} | \\boldsymbol{\\Omega},\\theta_i] &= \\alpha + \\mathbf{x}_i\\boldsymbol{\\beta} + \\theta_i\\\\\n\\mathbb{V}(Y_{ij} | \\boldsymbol{\\Omega},\\theta_i) &= \\sigma^2\\\\\n\\mathbb{C}(Y_{ij}, Y_{lk} | \\boldsymbol{\\Omega},\\theta_i) &= 0,\\quad \\forall i,j,l,k.\n\\end{align*}\n\n## Group-Specific Intercept Model: Conditional Specification\n\n- Define $\\mathbf{Y}_i = (Y_{i1},\\ldots,Y_{in_i})$ and $\\mathbf{Y} = (\\mathbf{Y}_1,\\ldots,\\mathbf{Y}_n)$.\n\n- The posterior for the conditional model can be written as:\n\n\\begin{align*}\nf(\\boldsymbol{\\Omega}, \\boldsymbol{\\theta} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta}) f(\\boldsymbol{\\theta} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n \\prod_{j = 1}^{n_i} f(Y_{ij} | \\boldsymbol{\\Omega}, \\boldsymbol{\\theta})  \\prod_{i=1}^n f(\\theta_i | \\tau^2) f(\\boldsymbol{\\Omega}),\n\\end{align*}\nwhere $\\boldsymbol{\\theta} = (\\theta_1,\\ldots,\\theta_n)$.\n\n## Group-Specific Intercept Model: Marginal Specification {.midi}\n\nTo derive a marginal model it is useful to write the model at the level of the independent observations, $\\mathbf{Y}_i$.\n\n$$\\mathbf{Y}_i = \\begin{bmatrix}\n    Y_{i1}\\\\\n    Y_{i2}\\\\\n    \\vdots\\\\\n    Y_{in_i}\n  \\end{bmatrix} = \n  \\begin{bmatrix}\n    \\alpha + \\mathbf{x}_{i1} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i1}\\\\\n    \\alpha + \\mathbf{x}_{i2} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{i2}\\\\\n    \\vdots \\\\\n    \\alpha + \\mathbf{x}_{in_i} \\boldsymbol{\\beta} + \\theta_i + \\epsilon_{in_i}\n  \\end{bmatrix} = \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i,$$\nwhere $\\mathbf{1}_{n_i}$ is an $n_i \\times 1$ dimensional vector of ones, $\\mathbf{X}_i$ is an $n_i \\times p$ dimensional matrix with rows $\\mathbf{x}_{ij}$. \n\n  - $\\boldsymbol{\\epsilon}_i = (\\epsilon_{i1},\\ldots,\\epsilon_{in_i}) \\stackrel{ind}{\\sim} N(\\mathbf{0}_{n_i}, \\sigma^2 \\mathbf{I}_{n_i})$, with $\\mathbf{0}_{n_i}$ an $n_i \\times 1$ dimensional vector of zeros.\n\n## Group-Specific Intercept Model: Marginal Specification {.midi}\n\n- Moments for the Marginal Model:\n\n\\begin{align*}\n\\mathbb{E}[\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}] &= \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i\\boldsymbol{\\beta}\\\\\n\\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) &= \\tau^2 \\mathbf{1}_{n_i} \\mathbf{1}_{n_i}^\\top + \\sigma^2 \\mathbf{I}_{n_i} = \\boldsymbol{\\Upsilon}_i\\\\\n\\mathbb{C}(\\mathbf{Y}_{i}, \\mathbf{Y}_{i'} | \\boldsymbol{\\Omega}) &= \\mathbf{0}_{n_i \\times n_i},\\quad i \\neq i'.\n\\end{align*}\n\n$$\\implies \\boldsymbol{\\Upsilon}_i = \\mathbb{V}(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) = \\begin{bmatrix}\n    \\tau^2 + \\sigma^2 & \\tau^2 & \\cdots & \\tau^2\\\\\n    \\tau^2 & \\tau^2 + \\sigma^2 & \\cdots & \\tau^2\\\\\n    \\vdots & \\vdots & \\ddots & \\vdots\\\\\n    \\tau^2 & \\tau^2 & \\cdots &\\tau^2 + \\sigma^2\n  \\end{bmatrix}.$$\n\n## Group-Specific Intercept Model: Marginal Specification\n\nFor $i = 1,\\ldots,n$, \n\\begin{align*}\n\\mathbf{Y}_{i} | \\boldsymbol{\\Omega} &\\stackrel{ind}{\\sim} N(\\alpha \\mathbf{1}_{n_i}+ \\mathbf{X}_i\\boldsymbol{\\beta} + \\theta_i,\\boldsymbol{\\Upsilon}_i)\\\\\n\\boldsymbol{\\Omega} &\\sim f(\\boldsymbol{\\Omega}),\n\\end{align*}\nwhere $\\boldsymbol{\\Omega} = (\\alpha, \\boldsymbol{\\beta},\\sigma,\\tau)$ are the population parameters.\n\n## Group-Specific Intercept Model: Marginal Specification\n\n- The posterior for the conditional model can be written as:\n\n\\begin{align*}\nf(\\boldsymbol{\\Omega} | \\mathbf{Y}) &\\propto f(\\mathbf{Y}, \\boldsymbol{\\Omega})\\\\\n&= f(\\mathbf{Y} | \\boldsymbol{\\Omega})f(\\boldsymbol{\\Omega})\\\\\n&=  \\prod_{i=1}^n f(\\mathbf{Y}_{i} | \\boldsymbol{\\Omega}) f(\\boldsymbol{\\Omega}).\n\\end{align*}\n\n**Why might we be interested in fitting the marginal model?**\n\n## Recovering the Group-Specific Parameters {.midi}\n\n- We can still recover the $\\theta_i$ when we fit the marginal model, we only need to compute $f(\\theta_i | \\mathbf{Y}_i,\\boldsymbol{\\Omega})$ for all $i$.\n\n- We can obtain this full conditional by specifying the joint distribution,\n\n$$f\\left(\\begin{bmatrix}\n    \\mathbf{Y}_i\\\\\n    \\theta_i\n  \\end{bmatrix} \\Bigg| \\boldsymbol{\\Omega}\\right) = N\\left(\\begin{bmatrix}\n    \\alpha \\mathbf{1}_{n_i} + \\mathbf{X}_i \\boldsymbol{\\beta} + \\theta_i \\mathbf{1}_{n_i} + \\boldsymbol{\\epsilon}_i\\\\\n    \\mathbf{0}_{n_i}\n  \\end{bmatrix}, \\begin{bmatrix}\n    \\Upsilon_i & \\tau^2 \\mathbf{1}_{n_i}\\\\\n    \\tau^2 \\mathbf{1}_{n_i}^\\top & \\tau^2\n  \\end{bmatrix}\\right).$$\n\nWe can then use the [conditional specification of a multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions) to find, $f(\\theta_i | \\mathbf{Y_i}, \\boldsymbol{\\Omega}) = N(\\mathbb{E}_{\\theta_i},\\mathbb{V}_{\\theta_i})$, where \n\n\\begin{align*}\n\\mathbb{E}_{\\theta_i} &= \\mathbf{0}_{n_i} + \\tau^2 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} (\\mathbf{Y}_i - \\alpha \\mathbf{1}_{n_i} - \\mathbf{X}_i \\boldsymbol{\\beta})\\\\\n\\mathbb{V}_{\\theta_i} &= \\tau^2 - \\tau^4 \\mathbf{1}_{n_i}^\\top \\boldsymbol{\\Upsilon}_i^{-1} \\mathbf{1}_{n_i}.\n\\end{align*}\n\n\n\n## Covariance Structure\n\n- The variance $\\tau^2$ for $\\theta_i$ can be interpreted as the **covariance** between two observations from the same hospital. \n\n- This reflects how much two students from the same school are expected to be similar in terms of their outcomes.\n  \n\\begin{align*}\n\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\mathbb{V}(\\theta_i)\\\\\n&= \\tau^2.\n\\end{align*}\n\n- Thus, $\\tau^2$ dictates the **within-group correlation** in our model.\n\n## Induced Within Correlation\n\n\\begin{align*}\n\\rho (Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega}) &= \\frac{\\mathbb{C}(Y_{ij}, Y_{ik} | \\boldsymbol{\\Omega})}{\\sqrt{\\mathbb{V}(Y_{ij} |  \\boldsymbol{\\Omega}) \\mathbb{V}(Y_{ik} |  \\boldsymbol{\\Omega})}}\\\\\n&=\\frac{\\tau^2}{\\tau^2 + \\sigma^2}\\\\\n&= \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}.\n\\end{align*}\n\nThis model induces positive correlation within group observations.\n\n## Induced Within Correlation\n\n$$\\rho (Y_{ij}, Y_{ik} | \\alpha,\\boldsymbol{\\beta},\\sigma) = \\frac{1}{1 + \\frac{\\sigma^2}{\\tau^2}}$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](15-hierarchical_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n\n## Fitting the Conditional Model in Stan\n\n\n\n::: {.cell output.var='intercept'}\n\n```{.stan .cell-code}\ndata {\n  int<lower = 1> n;\n  int<lower = 1> N;\n  int<lower = 1> p;\n  matrix[N, p] X;\n  vector[N] Y;\n  int<lower = 1, upper = n> Ids[N];\n}\nparameters {\n  real alpha;\n  vector[p] beta;\n  real<lower = 0> sigma;\n  real<lower = 0> tau;\n  vector[n] theta;\n}\nmodel {\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n  }\n  target += normal_lpdf(Y | mu, sigma);\n  target += normal_lpdf(theta | 0, tau);\n  target += normal_lpdf(beta | 0, 3);\n  target += normal_lpdf(sigma | 0, 3);\n  target += normal_lpdf(tau | 0, 3);\n}\ngenerated quantities {\n  real rho = 1 / (1 + ((sigma * sigma) / (tau * tau)));\n  vector[N] Y_pred;\n  vector[N] log_lik;\n  vector[N] mu = rep_vector(0.0, N);\n  mu += alpha;\n  for (i in 1:N) {\n    mu[i] += X[i, ] * beta + theta[Ids[i]];\n    log_lik[i] = normal_lpdf(Y[i] | mu[i], sigma);\n    Y_pred[i] = normal_rng(mu[i], sigma);\n  }\n}\n```\n:::\n\n\n\n## Summary of Key Points\n\n- School-specific intercepts $u_j$ are modeled as random effects, assumed to come from a normal distribution centered at zero with variance $\\sigma_u^2$.\n- The variance $\\sigma_u^2$ captures the variability in school-level effects and the within-group correlation.\n- The identifiability issue arises from the redundancy between the population intercept and the school-specific intercepts.\n- **Centering** the school-specific intercepts ensures that the population intercept $\\beta_0$ is well-defined and identifiable.\n\n## Conclusion\n\nBy introducing a random intercept, we allow for dependencies between observations within groups, making the model more realistic for real-world clustered or repeated measures data.\n\nThis is just one way to think about hierarchical models in Bayesian analysis, where we model not only fixed effects but also group-level variability.\n\n## Prepare for next class\n\n-   Work on [HW 03](https://biostat725-sp25.netlify.app/hw/hw-03).\n\n-   Complete reading to prepare for next Tuesday's lecture\n\n-   Tuesday's lecture: Missing data\n",
    "supporting": [
      "15-hierarchical_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}